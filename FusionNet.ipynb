{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import scipy.io as sio\n",
    "from densenet121 import DenseNet\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from keras.optimizers import SGD, Adagrad, Adadelta, RMSprop, Adam, Nadam\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2, l1\n",
    "# from densenet121 import DenseNet\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Concatenate, LeakyReLU, UpSampling2D\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, Conv2DTranspose\n",
    "from keras.optimizers import RMSprop, Adam, Adadelta\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "import keras as k\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten,Conv2D, MaxPooling2D,Activation,Input, Reshape, Permute, GlobalAveragePooling2D\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "from scipy.misc import imresize, imsave\n",
    "import csv\n",
    "import scipy.io as sio\n",
    "from PIL import Image\n",
    "import scipy\n",
    "import pickle\n",
    "from scipy.misc import imresize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dirName):\n",
    "    if not os.path.exists(dirName):\n",
    "        os.mkdir(dirName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set root path \n",
    "Network = 'deepCount/'\n",
    "root_pth ='/media/akhtar/6D2C8F896B2F79E0/naseer_houseCounting/fusionNet/' +  Network\n",
    "#set path of dataset\n",
    "dataset_path = '/media/akhtar/6D2C8F896B2F79E0/naseer_houseCounting/all_data_jittered_13425/'\n",
    "#set path of feature map where you want to store\n",
    "feature_map_save_path = '/home/akhtar/Naseer2/feature_maps/'\n",
    "create_dir(feature_map_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssnet_maps_path = feature_map_save_path + 'dmaps/'\n",
    "create_dir(ssnet_maps_path)\n",
    "densenet_maps_path = feature_map_save_path + 'relu5_blk/'\n",
    "create_dir(densenet_maps_path)\n",
    "densenet_gap_maps_path = feature_map_save_path + 'pool5/'\n",
    "create_dir(densenet_gap_maps_path)\n",
    "gwap_maps_path = feature_map_save_path + 'gap_maps/'\n",
    "create_dir(gwap_maps_path)\n",
    "product_maps_path = feature_map_save_path + 'pf_maps/'\n",
    "create_dir(product_maps_path)\n",
    "overlays_path = \"{}/overlays/\".format(feature_map_save_path)\n",
    "create_dir(overlays_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Extract feature maps of relu5_blk and pool5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiiii\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "densenet121.py:49: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (7, 7), strides=(2, 2), use_bias=False, name=\"conv1\")`\n",
      "  x = Convolution2D(nb_filter, 7, 7, subsample=(2, 2), name='conv1', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv2_1_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv2_1_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:169: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  concat_feat = merge([concat_feat, x], mode='concat', concat_axis=concat_axis, name='concat_'+str(stage)+'_'+str(branch))\n",
      "/home/akhtar/Naseer/local/lib/python2.7/site-packages/keras/legacy/layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv2_2_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv2_2_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv2_3_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv2_3_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv2_4_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv2_4_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv2_5_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv2_5_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv2_6_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv2_6_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:140: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv2_blk\")`\n",
      "  x = Convolution2D(int(nb_filter * compression), 1, 1, name=conv_name_base, bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv3_1_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv3_1_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv3_2_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv3_2_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv3_3_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv3_3_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv3_4_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv3_4_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv3_5_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv3_5_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv3_6_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv3_6_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv3_7_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv3_7_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv3_8_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv3_8_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv3_9_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv3_9_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv3_10_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv3_10_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv3_11_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv3_11_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv3_12_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv3_12_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "densenet121.py:140: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), use_bias=False, name=\"conv3_blk\")`\n",
      "  x = Convolution2D(int(nb_filter * compression), 1, 1, name=conv_name_base, bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_1_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_1_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_2_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_2_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_3_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_3_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_4_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_4_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_5_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_5_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_6_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_6_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_7_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_7_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_8_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_8_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_9_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_9_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_10_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_10_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_11_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_11_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_12_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_12_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_13_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_13_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_14_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_14_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_15_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_15_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_16_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_16_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_17_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_17_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_18_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_18_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_19_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_19_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_20_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_20_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_21_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_21_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_22_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_22_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_23_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_23_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv4_24_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv4_24_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:140: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (1, 1), use_bias=False, name=\"conv4_blk\")`\n",
      "  x = Convolution2D(int(nb_filter * compression), 1, 1, name=conv_name_base, bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv5_1_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv5_1_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv5_2_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv5_2_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv5_3_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv5_3_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv5_4_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv5_4_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv5_5_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv5_5_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv5_6_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv5_6_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv5_7_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv5_7_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv5_8_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv5_8_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv5_9_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv5_9_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv5_10_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv5_10_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv5_11_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv5_11_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv5_12_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv5_12_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv5_13_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv5_13_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv5_14_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv5_14_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n",
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv5_15_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv5_15_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/akhtar/Naseer/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1297: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "densenet121.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, name=\"conv5_16_x1\")`\n",
      "  x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x)\n",
      "densenet121.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, name=\"conv5_16_x2\")`\n",
      "  x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "data (InputLayer)                (None, 336, 336, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1_zeropadding (ZeroPadding2D (None, 342, 342, 3)   0           data[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                   (None, 168, 168, 64)  9408        conv1_zeropadding[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)    (None, 168, 168, 64)  256         conv1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv1_scale (Scale)              (None, 168, 168, 64)  128         conv1_bn[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "relu1 (Activation)               (None, 168, 168, 64)  0           conv1_scale[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "pool1_zeropadding (ZeroPadding2D (None, 170, 170, 64)  0           relu1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)             (None, 84, 84, 64)    0           pool1_zeropadding[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv2_1_x1_bn (BatchNormalizatio (None, 84, 84, 64)    256         pool1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv2_1_x1_scale (Scale)         (None, 84, 84, 64)    128         conv2_1_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu2_1_x1 (Activation)          (None, 84, 84, 64)    0           conv2_1_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2_1_x1 (Conv2D)              (None, 84, 84, 128)   8192        relu2_1_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_1_x2_bn (BatchNormalizatio (None, 84, 84, 128)   512         conv2_1_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_1_x2_scale (Scale)         (None, 84, 84, 128)   256         conv2_1_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu2_1_x2 (Activation)          (None, 84, 84, 128)   0           conv2_1_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2_1_x2_zeropadding (ZeroPadd (None, 86, 86, 128)   0           relu2_1_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_1_x2 (Conv2D)              (None, 84, 84, 32)    36864       conv2_1_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_2_1 (Merge)               (None, 84, 84, 96)    0           pool1[0][0]                      \n",
      "                                                                   conv2_1_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_2_x1_bn (BatchNormalizatio (None, 84, 84, 96)    384         concat_2_1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_2_x1_scale (Scale)         (None, 84, 84, 96)    192         conv2_2_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu2_2_x1 (Activation)          (None, 84, 84, 96)    0           conv2_2_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2_2_x1 (Conv2D)              (None, 84, 84, 128)   12288       relu2_2_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_2_x2_bn (BatchNormalizatio (None, 84, 84, 128)   512         conv2_2_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_2_x2_scale (Scale)         (None, 84, 84, 128)   256         conv2_2_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu2_2_x2 (Activation)          (None, 84, 84, 128)   0           conv2_2_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2_2_x2_zeropadding (ZeroPadd (None, 86, 86, 128)   0           relu2_2_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_2_x2 (Conv2D)              (None, 84, 84, 32)    36864       conv2_2_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_2_2 (Merge)               (None, 84, 84, 128)   0           concat_2_1[0][0]                 \n",
      "                                                                   conv2_2_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_3_x1_bn (BatchNormalizatio (None, 84, 84, 128)   512         concat_2_2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_3_x1_scale (Scale)         (None, 84, 84, 128)   256         conv2_3_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu2_3_x1 (Activation)          (None, 84, 84, 128)   0           conv2_3_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2_3_x1 (Conv2D)              (None, 84, 84, 128)   16384       relu2_3_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_3_x2_bn (BatchNormalizatio (None, 84, 84, 128)   512         conv2_3_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_3_x2_scale (Scale)         (None, 84, 84, 128)   256         conv2_3_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu2_3_x2 (Activation)          (None, 84, 84, 128)   0           conv2_3_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2_3_x2_zeropadding (ZeroPadd (None, 86, 86, 128)   0           relu2_3_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_3_x2 (Conv2D)              (None, 84, 84, 32)    36864       conv2_3_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_2_3 (Merge)               (None, 84, 84, 160)   0           concat_2_2[0][0]                 \n",
      "                                                                   conv2_3_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_4_x1_bn (BatchNormalizatio (None, 84, 84, 160)   640         concat_2_3[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_4_x1_scale (Scale)         (None, 84, 84, 160)   320         conv2_4_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu2_4_x1 (Activation)          (None, 84, 84, 160)   0           conv2_4_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2_4_x1 (Conv2D)              (None, 84, 84, 128)   20480       relu2_4_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_4_x2_bn (BatchNormalizatio (None, 84, 84, 128)   512         conv2_4_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_4_x2_scale (Scale)         (None, 84, 84, 128)   256         conv2_4_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu2_4_x2 (Activation)          (None, 84, 84, 128)   0           conv2_4_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2_4_x2_zeropadding (ZeroPadd (None, 86, 86, 128)   0           relu2_4_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_4_x2 (Conv2D)              (None, 84, 84, 32)    36864       conv2_4_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_2_4 (Merge)               (None, 84, 84, 192)   0           concat_2_3[0][0]                 \n",
      "                                                                   conv2_4_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_5_x1_bn (BatchNormalizatio (None, 84, 84, 192)   768         concat_2_4[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_5_x1_scale (Scale)         (None, 84, 84, 192)   384         conv2_5_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu2_5_x1 (Activation)          (None, 84, 84, 192)   0           conv2_5_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2_5_x1 (Conv2D)              (None, 84, 84, 128)   24576       relu2_5_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_5_x2_bn (BatchNormalizatio (None, 84, 84, 128)   512         conv2_5_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_5_x2_scale (Scale)         (None, 84, 84, 128)   256         conv2_5_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu2_5_x2 (Activation)          (None, 84, 84, 128)   0           conv2_5_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2_5_x2_zeropadding (ZeroPadd (None, 86, 86, 128)   0           relu2_5_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_5_x2 (Conv2D)              (None, 84, 84, 32)    36864       conv2_5_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_2_5 (Merge)               (None, 84, 84, 224)   0           concat_2_4[0][0]                 \n",
      "                                                                   conv2_5_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_6_x1_bn (BatchNormalizatio (None, 84, 84, 224)   896         concat_2_5[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_6_x1_scale (Scale)         (None, 84, 84, 224)   448         conv2_6_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu2_6_x1 (Activation)          (None, 84, 84, 224)   0           conv2_6_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2_6_x1 (Conv2D)              (None, 84, 84, 128)   28672       relu2_6_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_6_x2_bn (BatchNormalizatio (None, 84, 84, 128)   512         conv2_6_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_6_x2_scale (Scale)         (None, 84, 84, 128)   256         conv2_6_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu2_6_x2 (Activation)          (None, 84, 84, 128)   0           conv2_6_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2_6_x2_zeropadding (ZeroPadd (None, 86, 86, 128)   0           relu2_6_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_6_x2 (Conv2D)              (None, 84, 84, 32)    36864       conv2_6_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_2_6 (Merge)               (None, 84, 84, 256)   0           concat_2_5[0][0]                 \n",
      "                                                                   conv2_6_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_blk_bn (BatchNormalization (None, 84, 84, 256)   1024        concat_2_6[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2_blk_scale (Scale)          (None, 84, 84, 256)   512         conv2_blk_bn[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "relu2_blk (Activation)           (None, 84, 84, 256)   0           conv2_blk_scale[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2_blk (Conv2D)               (None, 84, 84, 128)   32768       relu2_blk[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "pool2 (AveragePooling2D)         (None, 42, 42, 128)   0           conv2_blk[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv3_1_x1_bn (BatchNormalizatio (None, 42, 42, 128)   512         pool2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv3_1_x1_scale (Scale)         (None, 42, 42, 128)   256         conv3_1_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu3_1_x1 (Activation)          (None, 42, 42, 128)   0           conv3_1_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv3_1_x1 (Conv2D)              (None, 42, 42, 128)   16384       relu3_1_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_1_x2_bn (BatchNormalizatio (None, 42, 42, 128)   512         conv3_1_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_1_x2_scale (Scale)         (None, 42, 42, 128)   256         conv3_1_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu3_1_x2 (Activation)          (None, 42, 42, 128)   0           conv3_1_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv3_1_x2_zeropadding (ZeroPadd (None, 44, 44, 128)   0           relu3_1_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_1_x2 (Conv2D)              (None, 42, 42, 32)    36864       conv3_1_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_3_1 (Merge)               (None, 42, 42, 160)   0           pool2[0][0]                      \n",
      "                                                                   conv3_1_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_2_x1_bn (BatchNormalizatio (None, 42, 42, 160)   640         concat_3_1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_2_x1_scale (Scale)         (None, 42, 42, 160)   320         conv3_2_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu3_2_x1 (Activation)          (None, 42, 42, 160)   0           conv3_2_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv3_2_x1 (Conv2D)              (None, 42, 42, 128)   20480       relu3_2_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_2_x2_bn (BatchNormalizatio (None, 42, 42, 128)   512         conv3_2_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_2_x2_scale (Scale)         (None, 42, 42, 128)   256         conv3_2_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu3_2_x2 (Activation)          (None, 42, 42, 128)   0           conv3_2_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv3_2_x2_zeropadding (ZeroPadd (None, 44, 44, 128)   0           relu3_2_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_2_x2 (Conv2D)              (None, 42, 42, 32)    36864       conv3_2_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_3_2 (Merge)               (None, 42, 42, 192)   0           concat_3_1[0][0]                 \n",
      "                                                                   conv3_2_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_3_x1_bn (BatchNormalizatio (None, 42, 42, 192)   768         concat_3_2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_3_x1_scale (Scale)         (None, 42, 42, 192)   384         conv3_3_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu3_3_x1 (Activation)          (None, 42, 42, 192)   0           conv3_3_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv3_3_x1 (Conv2D)              (None, 42, 42, 128)   24576       relu3_3_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_3_x2_bn (BatchNormalizatio (None, 42, 42, 128)   512         conv3_3_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_3_x2_scale (Scale)         (None, 42, 42, 128)   256         conv3_3_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu3_3_x2 (Activation)          (None, 42, 42, 128)   0           conv3_3_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv3_3_x2_zeropadding (ZeroPadd (None, 44, 44, 128)   0           relu3_3_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_3_x2 (Conv2D)              (None, 42, 42, 32)    36864       conv3_3_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_3_3 (Merge)               (None, 42, 42, 224)   0           concat_3_2[0][0]                 \n",
      "                                                                   conv3_3_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_4_x1_bn (BatchNormalizatio (None, 42, 42, 224)   896         concat_3_3[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_4_x1_scale (Scale)         (None, 42, 42, 224)   448         conv3_4_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu3_4_x1 (Activation)          (None, 42, 42, 224)   0           conv3_4_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv3_4_x1 (Conv2D)              (None, 42, 42, 128)   28672       relu3_4_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_4_x2_bn (BatchNormalizatio (None, 42, 42, 128)   512         conv3_4_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_4_x2_scale (Scale)         (None, 42, 42, 128)   256         conv3_4_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu3_4_x2 (Activation)          (None, 42, 42, 128)   0           conv3_4_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv3_4_x2_zeropadding (ZeroPadd (None, 44, 44, 128)   0           relu3_4_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_4_x2 (Conv2D)              (None, 42, 42, 32)    36864       conv3_4_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_3_4 (Merge)               (None, 42, 42, 256)   0           concat_3_3[0][0]                 \n",
      "                                                                   conv3_4_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_5_x1_bn (BatchNormalizatio (None, 42, 42, 256)   1024        concat_3_4[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_5_x1_scale (Scale)         (None, 42, 42, 256)   512         conv3_5_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu3_5_x1 (Activation)          (None, 42, 42, 256)   0           conv3_5_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv3_5_x1 (Conv2D)              (None, 42, 42, 128)   32768       relu3_5_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_5_x2_bn (BatchNormalizatio (None, 42, 42, 128)   512         conv3_5_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_5_x2_scale (Scale)         (None, 42, 42, 128)   256         conv3_5_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu3_5_x2 (Activation)          (None, 42, 42, 128)   0           conv3_5_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv3_5_x2_zeropadding (ZeroPadd (None, 44, 44, 128)   0           relu3_5_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_5_x2 (Conv2D)              (None, 42, 42, 32)    36864       conv3_5_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_3_5 (Merge)               (None, 42, 42, 288)   0           concat_3_4[0][0]                 \n",
      "                                                                   conv3_5_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_6_x1_bn (BatchNormalizatio (None, 42, 42, 288)   1152        concat_3_5[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_6_x1_scale (Scale)         (None, 42, 42, 288)   576         conv3_6_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu3_6_x1 (Activation)          (None, 42, 42, 288)   0           conv3_6_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv3_6_x1 (Conv2D)              (None, 42, 42, 128)   36864       relu3_6_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_6_x2_bn (BatchNormalizatio (None, 42, 42, 128)   512         conv3_6_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_6_x2_scale (Scale)         (None, 42, 42, 128)   256         conv3_6_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu3_6_x2 (Activation)          (None, 42, 42, 128)   0           conv3_6_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv3_6_x2_zeropadding (ZeroPadd (None, 44, 44, 128)   0           relu3_6_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_6_x2 (Conv2D)              (None, 42, 42, 32)    36864       conv3_6_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_3_6 (Merge)               (None, 42, 42, 320)   0           concat_3_5[0][0]                 \n",
      "                                                                   conv3_6_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_7_x1_bn (BatchNormalizatio (None, 42, 42, 320)   1280        concat_3_6[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_7_x1_scale (Scale)         (None, 42, 42, 320)   640         conv3_7_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu3_7_x1 (Activation)          (None, 42, 42, 320)   0           conv3_7_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv3_7_x1 (Conv2D)              (None, 42, 42, 128)   40960       relu3_7_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_7_x2_bn (BatchNormalizatio (None, 42, 42, 128)   512         conv3_7_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_7_x2_scale (Scale)         (None, 42, 42, 128)   256         conv3_7_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu3_7_x2 (Activation)          (None, 42, 42, 128)   0           conv3_7_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv3_7_x2_zeropadding (ZeroPadd (None, 44, 44, 128)   0           relu3_7_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_7_x2 (Conv2D)              (None, 42, 42, 32)    36864       conv3_7_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_3_7 (Merge)               (None, 42, 42, 352)   0           concat_3_6[0][0]                 \n",
      "                                                                   conv3_7_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_8_x1_bn (BatchNormalizatio (None, 42, 42, 352)   1408        concat_3_7[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_8_x1_scale (Scale)         (None, 42, 42, 352)   704         conv3_8_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu3_8_x1 (Activation)          (None, 42, 42, 352)   0           conv3_8_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv3_8_x1 (Conv2D)              (None, 42, 42, 128)   45056       relu3_8_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_8_x2_bn (BatchNormalizatio (None, 42, 42, 128)   512         conv3_8_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_8_x2_scale (Scale)         (None, 42, 42, 128)   256         conv3_8_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu3_8_x2 (Activation)          (None, 42, 42, 128)   0           conv3_8_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv3_8_x2_zeropadding (ZeroPadd (None, 44, 44, 128)   0           relu3_8_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_8_x2 (Conv2D)              (None, 42, 42, 32)    36864       conv3_8_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_3_8 (Merge)               (None, 42, 42, 384)   0           concat_3_7[0][0]                 \n",
      "                                                                   conv3_8_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_9_x1_bn (BatchNormalizatio (None, 42, 42, 384)   1536        concat_3_8[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_9_x1_scale (Scale)         (None, 42, 42, 384)   768         conv3_9_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu3_9_x1 (Activation)          (None, 42, 42, 384)   0           conv3_9_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv3_9_x1 (Conv2D)              (None, 42, 42, 128)   49152       relu3_9_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_9_x2_bn (BatchNormalizatio (None, 42, 42, 128)   512         conv3_9_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_9_x2_scale (Scale)         (None, 42, 42, 128)   256         conv3_9_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu3_9_x2 (Activation)          (None, 42, 42, 128)   0           conv3_9_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv3_9_x2_zeropadding (ZeroPadd (None, 44, 44, 128)   0           relu3_9_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_9_x2 (Conv2D)              (None, 42, 42, 32)    36864       conv3_9_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_3_9 (Merge)               (None, 42, 42, 416)   0           concat_3_8[0][0]                 \n",
      "                                                                   conv3_9_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_10_x1_bn (BatchNormalizati (None, 42, 42, 416)   1664        concat_3_9[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv3_10_x1_scale (Scale)        (None, 42, 42, 416)   832         conv3_10_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu3_10_x1 (Activation)         (None, 42, 42, 416)   0           conv3_10_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_10_x1 (Conv2D)             (None, 42, 42, 128)   53248       relu3_10_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv3_10_x2_bn (BatchNormalizati (None, 42, 42, 128)   512         conv3_10_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv3_10_x2_scale (Scale)        (None, 42, 42, 128)   256         conv3_10_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu3_10_x2 (Activation)         (None, 42, 42, 128)   0           conv3_10_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_10_x2_zeropadding (ZeroPad (None, 44, 44, 128)   0           relu3_10_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv3_10_x2 (Conv2D)             (None, 42, 42, 32)    36864       conv3_10_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_3_10 (Merge)              (None, 42, 42, 448)   0           concat_3_9[0][0]                 \n",
      "                                                                   conv3_10_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv3_11_x1_bn (BatchNormalizati (None, 42, 42, 448)   1792        concat_3_10[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv3_11_x1_scale (Scale)        (None, 42, 42, 448)   896         conv3_11_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu3_11_x1 (Activation)         (None, 42, 42, 448)   0           conv3_11_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_11_x1 (Conv2D)             (None, 42, 42, 128)   57344       relu3_11_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv3_11_x2_bn (BatchNormalizati (None, 42, 42, 128)   512         conv3_11_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv3_11_x2_scale (Scale)        (None, 42, 42, 128)   256         conv3_11_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu3_11_x2 (Activation)         (None, 42, 42, 128)   0           conv3_11_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_11_x2_zeropadding (ZeroPad (None, 44, 44, 128)   0           relu3_11_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv3_11_x2 (Conv2D)             (None, 42, 42, 32)    36864       conv3_11_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_3_11 (Merge)              (None, 42, 42, 480)   0           concat_3_10[0][0]                \n",
      "                                                                   conv3_11_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv3_12_x1_bn (BatchNormalizati (None, 42, 42, 480)   1920        concat_3_11[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv3_12_x1_scale (Scale)        (None, 42, 42, 480)   960         conv3_12_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu3_12_x1 (Activation)         (None, 42, 42, 480)   0           conv3_12_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_12_x1 (Conv2D)             (None, 42, 42, 128)   61440       relu3_12_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv3_12_x2_bn (BatchNormalizati (None, 42, 42, 128)   512         conv3_12_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv3_12_x2_scale (Scale)        (None, 42, 42, 128)   256         conv3_12_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu3_12_x2 (Activation)         (None, 42, 42, 128)   0           conv3_12_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv3_12_x2_zeropadding (ZeroPad (None, 44, 44, 128)   0           relu3_12_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv3_12_x2 (Conv2D)             (None, 42, 42, 32)    36864       conv3_12_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_3_12 (Merge)              (None, 42, 42, 512)   0           concat_3_11[0][0]                \n",
      "                                                                   conv3_12_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv3_blk_bn (BatchNormalization (None, 42, 42, 512)   2048        concat_3_12[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv3_blk_scale (Scale)          (None, 42, 42, 512)   1024        conv3_blk_bn[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "relu3_blk (Activation)           (None, 42, 42, 512)   0           conv3_blk_scale[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv3_blk (Conv2D)               (None, 42, 42, 256)   131072      relu3_blk[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "pool3 (AveragePooling2D)         (None, 21, 21, 256)   0           conv3_blk[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv4_1_x1_bn (BatchNormalizatio (None, 21, 21, 256)   1024        pool3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv4_1_x1_scale (Scale)         (None, 21, 21, 256)   512         conv4_1_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu4_1_x1 (Activation)          (None, 21, 21, 256)   0           conv4_1_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv4_1_x1 (Conv2D)              (None, 21, 21, 128)   32768       relu4_1_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_1_x2_bn (BatchNormalizatio (None, 21, 21, 128)   512         conv4_1_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_1_x2_scale (Scale)         (None, 21, 21, 128)   256         conv4_1_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu4_1_x2 (Activation)          (None, 21, 21, 128)   0           conv4_1_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv4_1_x2_zeropadding (ZeroPadd (None, 23, 23, 128)   0           relu4_1_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_1_x2 (Conv2D)              (None, 21, 21, 32)    36864       conv4_1_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_1 (Merge)               (None, 21, 21, 288)   0           pool3[0][0]                      \n",
      "                                                                   conv4_1_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_2_x1_bn (BatchNormalizatio (None, 21, 21, 288)   1152        concat_4_1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_2_x1_scale (Scale)         (None, 21, 21, 288)   576         conv4_2_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu4_2_x1 (Activation)          (None, 21, 21, 288)   0           conv4_2_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv4_2_x1 (Conv2D)              (None, 21, 21, 128)   36864       relu4_2_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_2_x2_bn (BatchNormalizatio (None, 21, 21, 128)   512         conv4_2_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_2_x2_scale (Scale)         (None, 21, 21, 128)   256         conv4_2_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu4_2_x2 (Activation)          (None, 21, 21, 128)   0           conv4_2_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv4_2_x2_zeropadding (ZeroPadd (None, 23, 23, 128)   0           relu4_2_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_2_x2 (Conv2D)              (None, 21, 21, 32)    36864       conv4_2_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_2 (Merge)               (None, 21, 21, 320)   0           concat_4_1[0][0]                 \n",
      "                                                                   conv4_2_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_x1_bn (BatchNormalizatio (None, 21, 21, 320)   1280        concat_4_2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_x1_scale (Scale)         (None, 21, 21, 320)   640         conv4_3_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu4_3_x1 (Activation)          (None, 21, 21, 320)   0           conv4_3_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_x1 (Conv2D)              (None, 21, 21, 128)   40960       relu4_3_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_x2_bn (BatchNormalizatio (None, 21, 21, 128)   512         conv4_3_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_x2_scale (Scale)         (None, 21, 21, 128)   256         conv4_3_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu4_3_x2 (Activation)          (None, 21, 21, 128)   0           conv4_3_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_x2_zeropadding (ZeroPadd (None, 23, 23, 128)   0           relu4_3_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_3_x2 (Conv2D)              (None, 21, 21, 32)    36864       conv4_3_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_3 (Merge)               (None, 21, 21, 352)   0           concat_4_2[0][0]                 \n",
      "                                                                   conv4_3_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_4_x1_bn (BatchNormalizatio (None, 21, 21, 352)   1408        concat_4_3[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_4_x1_scale (Scale)         (None, 21, 21, 352)   704         conv4_4_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu4_4_x1 (Activation)          (None, 21, 21, 352)   0           conv4_4_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv4_4_x1 (Conv2D)              (None, 21, 21, 128)   45056       relu4_4_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_4_x2_bn (BatchNormalizatio (None, 21, 21, 128)   512         conv4_4_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_4_x2_scale (Scale)         (None, 21, 21, 128)   256         conv4_4_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu4_4_x2 (Activation)          (None, 21, 21, 128)   0           conv4_4_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv4_4_x2_zeropadding (ZeroPadd (None, 23, 23, 128)   0           relu4_4_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_4_x2 (Conv2D)              (None, 21, 21, 32)    36864       conv4_4_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_4 (Merge)               (None, 21, 21, 384)   0           concat_4_3[0][0]                 \n",
      "                                                                   conv4_4_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_5_x1_bn (BatchNormalizatio (None, 21, 21, 384)   1536        concat_4_4[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_5_x1_scale (Scale)         (None, 21, 21, 384)   768         conv4_5_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu4_5_x1 (Activation)          (None, 21, 21, 384)   0           conv4_5_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv4_5_x1 (Conv2D)              (None, 21, 21, 128)   49152       relu4_5_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_5_x2_bn (BatchNormalizatio (None, 21, 21, 128)   512         conv4_5_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_5_x2_scale (Scale)         (None, 21, 21, 128)   256         conv4_5_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu4_5_x2 (Activation)          (None, 21, 21, 128)   0           conv4_5_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv4_5_x2_zeropadding (ZeroPadd (None, 23, 23, 128)   0           relu4_5_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_5_x2 (Conv2D)              (None, 21, 21, 32)    36864       conv4_5_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_5 (Merge)               (None, 21, 21, 416)   0           concat_4_4[0][0]                 \n",
      "                                                                   conv4_5_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_6_x1_bn (BatchNormalizatio (None, 21, 21, 416)   1664        concat_4_5[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_6_x1_scale (Scale)         (None, 21, 21, 416)   832         conv4_6_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu4_6_x1 (Activation)          (None, 21, 21, 416)   0           conv4_6_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv4_6_x1 (Conv2D)              (None, 21, 21, 128)   53248       relu4_6_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_6_x2_bn (BatchNormalizatio (None, 21, 21, 128)   512         conv4_6_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_6_x2_scale (Scale)         (None, 21, 21, 128)   256         conv4_6_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu4_6_x2 (Activation)          (None, 21, 21, 128)   0           conv4_6_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv4_6_x2_zeropadding (ZeroPadd (None, 23, 23, 128)   0           relu4_6_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_6_x2 (Conv2D)              (None, 21, 21, 32)    36864       conv4_6_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_6 (Merge)               (None, 21, 21, 448)   0           concat_4_5[0][0]                 \n",
      "                                                                   conv4_6_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_7_x1_bn (BatchNormalizatio (None, 21, 21, 448)   1792        concat_4_6[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_7_x1_scale (Scale)         (None, 21, 21, 448)   896         conv4_7_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu4_7_x1 (Activation)          (None, 21, 21, 448)   0           conv4_7_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv4_7_x1 (Conv2D)              (None, 21, 21, 128)   57344       relu4_7_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_7_x2_bn (BatchNormalizatio (None, 21, 21, 128)   512         conv4_7_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_7_x2_scale (Scale)         (None, 21, 21, 128)   256         conv4_7_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu4_7_x2 (Activation)          (None, 21, 21, 128)   0           conv4_7_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv4_7_x2_zeropadding (ZeroPadd (None, 23, 23, 128)   0           relu4_7_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_7_x2 (Conv2D)              (None, 21, 21, 32)    36864       conv4_7_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_7 (Merge)               (None, 21, 21, 480)   0           concat_4_6[0][0]                 \n",
      "                                                                   conv4_7_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_8_x1_bn (BatchNormalizatio (None, 21, 21, 480)   1920        concat_4_7[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_8_x1_scale (Scale)         (None, 21, 21, 480)   960         conv4_8_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu4_8_x1 (Activation)          (None, 21, 21, 480)   0           conv4_8_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv4_8_x1 (Conv2D)              (None, 21, 21, 128)   61440       relu4_8_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_8_x2_bn (BatchNormalizatio (None, 21, 21, 128)   512         conv4_8_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_8_x2_scale (Scale)         (None, 21, 21, 128)   256         conv4_8_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu4_8_x2 (Activation)          (None, 21, 21, 128)   0           conv4_8_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv4_8_x2_zeropadding (ZeroPadd (None, 23, 23, 128)   0           relu4_8_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_8_x2 (Conv2D)              (None, 21, 21, 32)    36864       conv4_8_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_8 (Merge)               (None, 21, 21, 512)   0           concat_4_7[0][0]                 \n",
      "                                                                   conv4_8_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_9_x1_bn (BatchNormalizatio (None, 21, 21, 512)   2048        concat_4_8[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_9_x1_scale (Scale)         (None, 21, 21, 512)   1024        conv4_9_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu4_9_x1 (Activation)          (None, 21, 21, 512)   0           conv4_9_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv4_9_x1 (Conv2D)              (None, 21, 21, 128)   65536       relu4_9_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_9_x2_bn (BatchNormalizatio (None, 21, 21, 128)   512         conv4_9_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_9_x2_scale (Scale)         (None, 21, 21, 128)   256         conv4_9_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu4_9_x2 (Activation)          (None, 21, 21, 128)   0           conv4_9_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv4_9_x2_zeropadding (ZeroPadd (None, 23, 23, 128)   0           relu4_9_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_9_x2 (Conv2D)              (None, 21, 21, 32)    36864       conv4_9_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_9 (Merge)               (None, 21, 21, 544)   0           concat_4_8[0][0]                 \n",
      "                                                                   conv4_9_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_10_x1_bn (BatchNormalizati (None, 21, 21, 544)   2176        concat_4_9[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv4_10_x1_scale (Scale)        (None, 21, 21, 544)   1088        conv4_10_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_10_x1 (Activation)         (None, 21, 21, 544)   0           conv4_10_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_10_x1 (Conv2D)             (None, 21, 21, 128)   69632       relu4_10_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_10_x2_bn (BatchNormalizati (None, 21, 21, 128)   512         conv4_10_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_10_x2_scale (Scale)        (None, 21, 21, 128)   256         conv4_10_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_10_x2 (Activation)         (None, 21, 21, 128)   0           conv4_10_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_10_x2_zeropadding (ZeroPad (None, 23, 23, 128)   0           relu4_10_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_10_x2 (Conv2D)             (None, 21, 21, 32)    36864       conv4_10_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_10 (Merge)              (None, 21, 21, 576)   0           concat_4_9[0][0]                 \n",
      "                                                                   conv4_10_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_11_x1_bn (BatchNormalizati (None, 21, 21, 576)   2304        concat_4_10[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_11_x1_scale (Scale)        (None, 21, 21, 576)   1152        conv4_11_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_11_x1 (Activation)         (None, 21, 21, 576)   0           conv4_11_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_11_x1 (Conv2D)             (None, 21, 21, 128)   73728       relu4_11_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_11_x2_bn (BatchNormalizati (None, 21, 21, 128)   512         conv4_11_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_11_x2_scale (Scale)        (None, 21, 21, 128)   256         conv4_11_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_11_x2 (Activation)         (None, 21, 21, 128)   0           conv4_11_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_11_x2_zeropadding (ZeroPad (None, 23, 23, 128)   0           relu4_11_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_11_x2 (Conv2D)             (None, 21, 21, 32)    36864       conv4_11_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_11 (Merge)              (None, 21, 21, 608)   0           concat_4_10[0][0]                \n",
      "                                                                   conv4_11_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_12_x1_bn (BatchNormalizati (None, 21, 21, 608)   2432        concat_4_11[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_12_x1_scale (Scale)        (None, 21, 21, 608)   1216        conv4_12_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_12_x1 (Activation)         (None, 21, 21, 608)   0           conv4_12_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_12_x1 (Conv2D)             (None, 21, 21, 128)   77824       relu4_12_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_12_x2_bn (BatchNormalizati (None, 21, 21, 128)   512         conv4_12_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_12_x2_scale (Scale)        (None, 21, 21, 128)   256         conv4_12_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_12_x2 (Activation)         (None, 21, 21, 128)   0           conv4_12_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_12_x2_zeropadding (ZeroPad (None, 23, 23, 128)   0           relu4_12_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_12_x2 (Conv2D)             (None, 21, 21, 32)    36864       conv4_12_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_12 (Merge)              (None, 21, 21, 640)   0           concat_4_11[0][0]                \n",
      "                                                                   conv4_12_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_13_x1_bn (BatchNormalizati (None, 21, 21, 640)   2560        concat_4_12[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_13_x1_scale (Scale)        (None, 21, 21, 640)   1280        conv4_13_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_13_x1 (Activation)         (None, 21, 21, 640)   0           conv4_13_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_13_x1 (Conv2D)             (None, 21, 21, 128)   81920       relu4_13_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_13_x2_bn (BatchNormalizati (None, 21, 21, 128)   512         conv4_13_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_13_x2_scale (Scale)        (None, 21, 21, 128)   256         conv4_13_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_13_x2 (Activation)         (None, 21, 21, 128)   0           conv4_13_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_13_x2_zeropadding (ZeroPad (None, 23, 23, 128)   0           relu4_13_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_13_x2 (Conv2D)             (None, 21, 21, 32)    36864       conv4_13_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_13 (Merge)              (None, 21, 21, 672)   0           concat_4_12[0][0]                \n",
      "                                                                   conv4_13_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_14_x1_bn (BatchNormalizati (None, 21, 21, 672)   2688        concat_4_13[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_14_x1_scale (Scale)        (None, 21, 21, 672)   1344        conv4_14_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_14_x1 (Activation)         (None, 21, 21, 672)   0           conv4_14_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_14_x1 (Conv2D)             (None, 21, 21, 128)   86016       relu4_14_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_14_x2_bn (BatchNormalizati (None, 21, 21, 128)   512         conv4_14_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_14_x2_scale (Scale)        (None, 21, 21, 128)   256         conv4_14_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_14_x2 (Activation)         (None, 21, 21, 128)   0           conv4_14_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_14_x2_zeropadding (ZeroPad (None, 23, 23, 128)   0           relu4_14_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_14_x2 (Conv2D)             (None, 21, 21, 32)    36864       conv4_14_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_14 (Merge)              (None, 21, 21, 704)   0           concat_4_13[0][0]                \n",
      "                                                                   conv4_14_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_15_x1_bn (BatchNormalizati (None, 21, 21, 704)   2816        concat_4_14[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_15_x1_scale (Scale)        (None, 21, 21, 704)   1408        conv4_15_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_15_x1 (Activation)         (None, 21, 21, 704)   0           conv4_15_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_15_x1 (Conv2D)             (None, 21, 21, 128)   90112       relu4_15_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_15_x2_bn (BatchNormalizati (None, 21, 21, 128)   512         conv4_15_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_15_x2_scale (Scale)        (None, 21, 21, 128)   256         conv4_15_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_15_x2 (Activation)         (None, 21, 21, 128)   0           conv4_15_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_15_x2_zeropadding (ZeroPad (None, 23, 23, 128)   0           relu4_15_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_15_x2 (Conv2D)             (None, 21, 21, 32)    36864       conv4_15_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_15 (Merge)              (None, 21, 21, 736)   0           concat_4_14[0][0]                \n",
      "                                                                   conv4_15_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_16_x1_bn (BatchNormalizati (None, 21, 21, 736)   2944        concat_4_15[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_16_x1_scale (Scale)        (None, 21, 21, 736)   1472        conv4_16_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_16_x1 (Activation)         (None, 21, 21, 736)   0           conv4_16_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_16_x1 (Conv2D)             (None, 21, 21, 128)   94208       relu4_16_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_16_x2_bn (BatchNormalizati (None, 21, 21, 128)   512         conv4_16_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_16_x2_scale (Scale)        (None, 21, 21, 128)   256         conv4_16_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_16_x2 (Activation)         (None, 21, 21, 128)   0           conv4_16_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_16_x2_zeropadding (ZeroPad (None, 23, 23, 128)   0           relu4_16_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_16_x2 (Conv2D)             (None, 21, 21, 32)    36864       conv4_16_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_16 (Merge)              (None, 21, 21, 768)   0           concat_4_15[0][0]                \n",
      "                                                                   conv4_16_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_17_x1_bn (BatchNormalizati (None, 21, 21, 768)   3072        concat_4_16[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_17_x1_scale (Scale)        (None, 21, 21, 768)   1536        conv4_17_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_17_x1 (Activation)         (None, 21, 21, 768)   0           conv4_17_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_17_x1 (Conv2D)             (None, 21, 21, 128)   98304       relu4_17_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_17_x2_bn (BatchNormalizati (None, 21, 21, 128)   512         conv4_17_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_17_x2_scale (Scale)        (None, 21, 21, 128)   256         conv4_17_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_17_x2 (Activation)         (None, 21, 21, 128)   0           conv4_17_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_17_x2_zeropadding (ZeroPad (None, 23, 23, 128)   0           relu4_17_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_17_x2 (Conv2D)             (None, 21, 21, 32)    36864       conv4_17_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_17 (Merge)              (None, 21, 21, 800)   0           concat_4_16[0][0]                \n",
      "                                                                   conv4_17_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_18_x1_bn (BatchNormalizati (None, 21, 21, 800)   3200        concat_4_17[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_18_x1_scale (Scale)        (None, 21, 21, 800)   1600        conv4_18_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_18_x1 (Activation)         (None, 21, 21, 800)   0           conv4_18_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_18_x1 (Conv2D)             (None, 21, 21, 128)   102400      relu4_18_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_18_x2_bn (BatchNormalizati (None, 21, 21, 128)   512         conv4_18_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_18_x2_scale (Scale)        (None, 21, 21, 128)   256         conv4_18_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_18_x2 (Activation)         (None, 21, 21, 128)   0           conv4_18_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_18_x2_zeropadding (ZeroPad (None, 23, 23, 128)   0           relu4_18_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_18_x2 (Conv2D)             (None, 21, 21, 32)    36864       conv4_18_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_18 (Merge)              (None, 21, 21, 832)   0           concat_4_17[0][0]                \n",
      "                                                                   conv4_18_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_19_x1_bn (BatchNormalizati (None, 21, 21, 832)   3328        concat_4_18[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_19_x1_scale (Scale)        (None, 21, 21, 832)   1664        conv4_19_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_19_x1 (Activation)         (None, 21, 21, 832)   0           conv4_19_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_19_x1 (Conv2D)             (None, 21, 21, 128)   106496      relu4_19_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_19_x2_bn (BatchNormalizati (None, 21, 21, 128)   512         conv4_19_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_19_x2_scale (Scale)        (None, 21, 21, 128)   256         conv4_19_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_19_x2 (Activation)         (None, 21, 21, 128)   0           conv4_19_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_19_x2_zeropadding (ZeroPad (None, 23, 23, 128)   0           relu4_19_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_19_x2 (Conv2D)             (None, 21, 21, 32)    36864       conv4_19_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_19 (Merge)              (None, 21, 21, 864)   0           concat_4_18[0][0]                \n",
      "                                                                   conv4_19_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_20_x1_bn (BatchNormalizati (None, 21, 21, 864)   3456        concat_4_19[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_20_x1_scale (Scale)        (None, 21, 21, 864)   1728        conv4_20_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_20_x1 (Activation)         (None, 21, 21, 864)   0           conv4_20_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_20_x1 (Conv2D)             (None, 21, 21, 128)   110592      relu4_20_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_20_x2_bn (BatchNormalizati (None, 21, 21, 128)   512         conv4_20_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_20_x2_scale (Scale)        (None, 21, 21, 128)   256         conv4_20_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_20_x2 (Activation)         (None, 21, 21, 128)   0           conv4_20_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_20_x2_zeropadding (ZeroPad (None, 23, 23, 128)   0           relu4_20_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_20_x2 (Conv2D)             (None, 21, 21, 32)    36864       conv4_20_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_20 (Merge)              (None, 21, 21, 896)   0           concat_4_19[0][0]                \n",
      "                                                                   conv4_20_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_21_x1_bn (BatchNormalizati (None, 21, 21, 896)   3584        concat_4_20[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_21_x1_scale (Scale)        (None, 21, 21, 896)   1792        conv4_21_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_21_x1 (Activation)         (None, 21, 21, 896)   0           conv4_21_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_21_x1 (Conv2D)             (None, 21, 21, 128)   114688      relu4_21_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_21_x2_bn (BatchNormalizati (None, 21, 21, 128)   512         conv4_21_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_21_x2_scale (Scale)        (None, 21, 21, 128)   256         conv4_21_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_21_x2 (Activation)         (None, 21, 21, 128)   0           conv4_21_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_21_x2_zeropadding (ZeroPad (None, 23, 23, 128)   0           relu4_21_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_21_x2 (Conv2D)             (None, 21, 21, 32)    36864       conv4_21_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_21 (Merge)              (None, 21, 21, 928)   0           concat_4_20[0][0]                \n",
      "                                                                   conv4_21_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_22_x1_bn (BatchNormalizati (None, 21, 21, 928)   3712        concat_4_21[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_22_x1_scale (Scale)        (None, 21, 21, 928)   1856        conv4_22_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_22_x1 (Activation)         (None, 21, 21, 928)   0           conv4_22_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_22_x1 (Conv2D)             (None, 21, 21, 128)   118784      relu4_22_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_22_x2_bn (BatchNormalizati (None, 21, 21, 128)   512         conv4_22_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_22_x2_scale (Scale)        (None, 21, 21, 128)   256         conv4_22_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_22_x2 (Activation)         (None, 21, 21, 128)   0           conv4_22_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_22_x2_zeropadding (ZeroPad (None, 23, 23, 128)   0           relu4_22_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_22_x2 (Conv2D)             (None, 21, 21, 32)    36864       conv4_22_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_22 (Merge)              (None, 21, 21, 960)   0           concat_4_21[0][0]                \n",
      "                                                                   conv4_22_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_23_x1_bn (BatchNormalizati (None, 21, 21, 960)   3840        concat_4_22[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_23_x1_scale (Scale)        (None, 21, 21, 960)   1920        conv4_23_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_23_x1 (Activation)         (None, 21, 21, 960)   0           conv4_23_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_23_x1 (Conv2D)             (None, 21, 21, 128)   122880      relu4_23_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_23_x2_bn (BatchNormalizati (None, 21, 21, 128)   512         conv4_23_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_23_x2_scale (Scale)        (None, 21, 21, 128)   256         conv4_23_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_23_x2 (Activation)         (None, 21, 21, 128)   0           conv4_23_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_23_x2_zeropadding (ZeroPad (None, 23, 23, 128)   0           relu4_23_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_23_x2 (Conv2D)             (None, 21, 21, 32)    36864       conv4_23_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_23 (Merge)              (None, 21, 21, 992)   0           concat_4_22[0][0]                \n",
      "                                                                   conv4_23_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_24_x1_bn (BatchNormalizati (None, 21, 21, 992)   3968        concat_4_23[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_24_x1_scale (Scale)        (None, 21, 21, 992)   1984        conv4_24_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_24_x1 (Activation)         (None, 21, 21, 992)   0           conv4_24_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_24_x1 (Conv2D)             (None, 21, 21, 128)   126976      relu4_24_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_24_x2_bn (BatchNormalizati (None, 21, 21, 128)   512         conv4_24_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_24_x2_scale (Scale)        (None, 21, 21, 128)   256         conv4_24_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu4_24_x2 (Activation)         (None, 21, 21, 128)   0           conv4_24_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv4_24_x2_zeropadding (ZeroPad (None, 23, 23, 128)   0           relu4_24_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_24_x2 (Conv2D)             (None, 21, 21, 32)    36864       conv4_24_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_4_24 (Merge)              (None, 21, 21, 1024)  0           concat_4_23[0][0]                \n",
      "                                                                   conv4_24_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_blk_bn (BatchNormalization (None, 21, 21, 1024)  4096        concat_4_24[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv4_blk_scale (Scale)          (None, 21, 21, 1024)  2048        conv4_blk_bn[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "relu4_blk (Activation)           (None, 21, 21, 1024)  0           conv4_blk_scale[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv4_blk (Conv2D)               (None, 21, 21, 512)   524288      relu4_blk[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "pool4 (AveragePooling2D)         (None, 10, 10, 512)   0           conv4_blk[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv5_1_x1_bn (BatchNormalizatio (None, 10, 10, 512)   2048        pool4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv5_1_x1_scale (Scale)         (None, 10, 10, 512)   1024        conv5_1_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu5_1_x1 (Activation)          (None, 10, 10, 512)   0           conv5_1_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv5_1_x1 (Conv2D)              (None, 10, 10, 128)   65536       relu5_1_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_1_x2_bn (BatchNormalizatio (None, 10, 10, 128)   512         conv5_1_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_1_x2_scale (Scale)         (None, 10, 10, 128)   256         conv5_1_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu5_1_x2 (Activation)          (None, 10, 10, 128)   0           conv5_1_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv5_1_x2_zeropadding (ZeroPadd (None, 12, 12, 128)   0           relu5_1_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_1_x2 (Conv2D)              (None, 10, 10, 32)    36864       conv5_1_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_5_1 (Merge)               (None, 10, 10, 544)   0           pool4[0][0]                      \n",
      "                                                                   conv5_1_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_2_x1_bn (BatchNormalizatio (None, 10, 10, 544)   2176        concat_5_1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_2_x1_scale (Scale)         (None, 10, 10, 544)   1088        conv5_2_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu5_2_x1 (Activation)          (None, 10, 10, 544)   0           conv5_2_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv5_2_x1 (Conv2D)              (None, 10, 10, 128)   69632       relu5_2_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_2_x2_bn (BatchNormalizatio (None, 10, 10, 128)   512         conv5_2_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_2_x2_scale (Scale)         (None, 10, 10, 128)   256         conv5_2_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu5_2_x2 (Activation)          (None, 10, 10, 128)   0           conv5_2_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv5_2_x2_zeropadding (ZeroPadd (None, 12, 12, 128)   0           relu5_2_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_2_x2 (Conv2D)              (None, 10, 10, 32)    36864       conv5_2_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_5_2 (Merge)               (None, 10, 10, 576)   0           concat_5_1[0][0]                 \n",
      "                                                                   conv5_2_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_3_x1_bn (BatchNormalizatio (None, 10, 10, 576)   2304        concat_5_2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_3_x1_scale (Scale)         (None, 10, 10, 576)   1152        conv5_3_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu5_3_x1 (Activation)          (None, 10, 10, 576)   0           conv5_3_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv5_3_x1 (Conv2D)              (None, 10, 10, 128)   73728       relu5_3_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_3_x2_bn (BatchNormalizatio (None, 10, 10, 128)   512         conv5_3_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_3_x2_scale (Scale)         (None, 10, 10, 128)   256         conv5_3_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu5_3_x2 (Activation)          (None, 10, 10, 128)   0           conv5_3_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv5_3_x2_zeropadding (ZeroPadd (None, 12, 12, 128)   0           relu5_3_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_3_x2 (Conv2D)              (None, 10, 10, 32)    36864       conv5_3_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_5_3 (Merge)               (None, 10, 10, 608)   0           concat_5_2[0][0]                 \n",
      "                                                                   conv5_3_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_4_x1_bn (BatchNormalizatio (None, 10, 10, 608)   2432        concat_5_3[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_4_x1_scale (Scale)         (None, 10, 10, 608)   1216        conv5_4_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu5_4_x1 (Activation)          (None, 10, 10, 608)   0           conv5_4_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv5_4_x1 (Conv2D)              (None, 10, 10, 128)   77824       relu5_4_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_4_x2_bn (BatchNormalizatio (None, 10, 10, 128)   512         conv5_4_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_4_x2_scale (Scale)         (None, 10, 10, 128)   256         conv5_4_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu5_4_x2 (Activation)          (None, 10, 10, 128)   0           conv5_4_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv5_4_x2_zeropadding (ZeroPadd (None, 12, 12, 128)   0           relu5_4_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_4_x2 (Conv2D)              (None, 10, 10, 32)    36864       conv5_4_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_5_4 (Merge)               (None, 10, 10, 640)   0           concat_5_3[0][0]                 \n",
      "                                                                   conv5_4_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_5_x1_bn (BatchNormalizatio (None, 10, 10, 640)   2560        concat_5_4[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_5_x1_scale (Scale)         (None, 10, 10, 640)   1280        conv5_5_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu5_5_x1 (Activation)          (None, 10, 10, 640)   0           conv5_5_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv5_5_x1 (Conv2D)              (None, 10, 10, 128)   81920       relu5_5_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_5_x2_bn (BatchNormalizatio (None, 10, 10, 128)   512         conv5_5_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_5_x2_scale (Scale)         (None, 10, 10, 128)   256         conv5_5_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu5_5_x2 (Activation)          (None, 10, 10, 128)   0           conv5_5_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv5_5_x2_zeropadding (ZeroPadd (None, 12, 12, 128)   0           relu5_5_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_5_x2 (Conv2D)              (None, 10, 10, 32)    36864       conv5_5_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_5_5 (Merge)               (None, 10, 10, 672)   0           concat_5_4[0][0]                 \n",
      "                                                                   conv5_5_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_6_x1_bn (BatchNormalizatio (None, 10, 10, 672)   2688        concat_5_5[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_6_x1_scale (Scale)         (None, 10, 10, 672)   1344        conv5_6_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu5_6_x1 (Activation)          (None, 10, 10, 672)   0           conv5_6_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv5_6_x1 (Conv2D)              (None, 10, 10, 128)   86016       relu5_6_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_6_x2_bn (BatchNormalizatio (None, 10, 10, 128)   512         conv5_6_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_6_x2_scale (Scale)         (None, 10, 10, 128)   256         conv5_6_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu5_6_x2 (Activation)          (None, 10, 10, 128)   0           conv5_6_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv5_6_x2_zeropadding (ZeroPadd (None, 12, 12, 128)   0           relu5_6_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_6_x2 (Conv2D)              (None, 10, 10, 32)    36864       conv5_6_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_5_6 (Merge)               (None, 10, 10, 704)   0           concat_5_5[0][0]                 \n",
      "                                                                   conv5_6_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_7_x1_bn (BatchNormalizatio (None, 10, 10, 704)   2816        concat_5_6[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_7_x1_scale (Scale)         (None, 10, 10, 704)   1408        conv5_7_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu5_7_x1 (Activation)          (None, 10, 10, 704)   0           conv5_7_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv5_7_x1 (Conv2D)              (None, 10, 10, 128)   90112       relu5_7_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_7_x2_bn (BatchNormalizatio (None, 10, 10, 128)   512         conv5_7_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_7_x2_scale (Scale)         (None, 10, 10, 128)   256         conv5_7_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu5_7_x2 (Activation)          (None, 10, 10, 128)   0           conv5_7_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv5_7_x2_zeropadding (ZeroPadd (None, 12, 12, 128)   0           relu5_7_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_7_x2 (Conv2D)              (None, 10, 10, 32)    36864       conv5_7_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_5_7 (Merge)               (None, 10, 10, 736)   0           concat_5_6[0][0]                 \n",
      "                                                                   conv5_7_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_8_x1_bn (BatchNormalizatio (None, 10, 10, 736)   2944        concat_5_7[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_8_x1_scale (Scale)         (None, 10, 10, 736)   1472        conv5_8_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu5_8_x1 (Activation)          (None, 10, 10, 736)   0           conv5_8_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv5_8_x1 (Conv2D)              (None, 10, 10, 128)   94208       relu5_8_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_8_x2_bn (BatchNormalizatio (None, 10, 10, 128)   512         conv5_8_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_8_x2_scale (Scale)         (None, 10, 10, 128)   256         conv5_8_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu5_8_x2 (Activation)          (None, 10, 10, 128)   0           conv5_8_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv5_8_x2_zeropadding (ZeroPadd (None, 12, 12, 128)   0           relu5_8_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_8_x2 (Conv2D)              (None, 10, 10, 32)    36864       conv5_8_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_5_8 (Merge)               (None, 10, 10, 768)   0           concat_5_7[0][0]                 \n",
      "                                                                   conv5_8_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_9_x1_bn (BatchNormalizatio (None, 10, 10, 768)   3072        concat_5_8[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_9_x1_scale (Scale)         (None, 10, 10, 768)   1536        conv5_9_x1_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu5_9_x1 (Activation)          (None, 10, 10, 768)   0           conv5_9_x1_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv5_9_x1 (Conv2D)              (None, 10, 10, 128)   98304       relu5_9_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_9_x2_bn (BatchNormalizatio (None, 10, 10, 128)   512         conv5_9_x1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_9_x2_scale (Scale)         (None, 10, 10, 128)   256         conv5_9_x2_bn[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "relu5_9_x2 (Activation)          (None, 10, 10, 128)   0           conv5_9_x2_scale[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv5_9_x2_zeropadding (ZeroPadd (None, 12, 12, 128)   0           relu5_9_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_9_x2 (Conv2D)              (None, 10, 10, 32)    36864       conv5_9_x2_zeropadding[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "concat_5_9 (Merge)               (None, 10, 10, 800)   0           concat_5_8[0][0]                 \n",
      "                                                                   conv5_9_x2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_10_x1_bn (BatchNormalizati (None, 10, 10, 800)   3200        concat_5_9[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv5_10_x1_scale (Scale)        (None, 10, 10, 800)   1600        conv5_10_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu5_10_x1 (Activation)         (None, 10, 10, 800)   0           conv5_10_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_10_x1 (Conv2D)             (None, 10, 10, 128)   102400      relu5_10_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_10_x2_bn (BatchNormalizati (None, 10, 10, 128)   512         conv5_10_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_10_x2_scale (Scale)        (None, 10, 10, 128)   256         conv5_10_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu5_10_x2 (Activation)         (None, 10, 10, 128)   0           conv5_10_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_10_x2_zeropadding (ZeroPad (None, 12, 12, 128)   0           relu5_10_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_10_x2 (Conv2D)             (None, 10, 10, 32)    36864       conv5_10_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_5_10 (Merge)              (None, 10, 10, 832)   0           concat_5_9[0][0]                 \n",
      "                                                                   conv5_10_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_11_x1_bn (BatchNormalizati (None, 10, 10, 832)   3328        concat_5_10[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_11_x1_scale (Scale)        (None, 10, 10, 832)   1664        conv5_11_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu5_11_x1 (Activation)         (None, 10, 10, 832)   0           conv5_11_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_11_x1 (Conv2D)             (None, 10, 10, 128)   106496      relu5_11_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_11_x2_bn (BatchNormalizati (None, 10, 10, 128)   512         conv5_11_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_11_x2_scale (Scale)        (None, 10, 10, 128)   256         conv5_11_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu5_11_x2 (Activation)         (None, 10, 10, 128)   0           conv5_11_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_11_x2_zeropadding (ZeroPad (None, 12, 12, 128)   0           relu5_11_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_11_x2 (Conv2D)             (None, 10, 10, 32)    36864       conv5_11_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_5_11 (Merge)              (None, 10, 10, 864)   0           concat_5_10[0][0]                \n",
      "                                                                   conv5_11_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_12_x1_bn (BatchNormalizati (None, 10, 10, 864)   3456        concat_5_11[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_12_x1_scale (Scale)        (None, 10, 10, 864)   1728        conv5_12_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu5_12_x1 (Activation)         (None, 10, 10, 864)   0           conv5_12_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_12_x1 (Conv2D)             (None, 10, 10, 128)   110592      relu5_12_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_12_x2_bn (BatchNormalizati (None, 10, 10, 128)   512         conv5_12_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_12_x2_scale (Scale)        (None, 10, 10, 128)   256         conv5_12_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu5_12_x2 (Activation)         (None, 10, 10, 128)   0           conv5_12_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_12_x2_zeropadding (ZeroPad (None, 12, 12, 128)   0           relu5_12_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_12_x2 (Conv2D)             (None, 10, 10, 32)    36864       conv5_12_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_5_12 (Merge)              (None, 10, 10, 896)   0           concat_5_11[0][0]                \n",
      "                                                                   conv5_12_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_13_x1_bn (BatchNormalizati (None, 10, 10, 896)   3584        concat_5_12[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_13_x1_scale (Scale)        (None, 10, 10, 896)   1792        conv5_13_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu5_13_x1 (Activation)         (None, 10, 10, 896)   0           conv5_13_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_13_x1 (Conv2D)             (None, 10, 10, 128)   114688      relu5_13_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_13_x2_bn (BatchNormalizati (None, 10, 10, 128)   512         conv5_13_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_13_x2_scale (Scale)        (None, 10, 10, 128)   256         conv5_13_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu5_13_x2 (Activation)         (None, 10, 10, 128)   0           conv5_13_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_13_x2_zeropadding (ZeroPad (None, 12, 12, 128)   0           relu5_13_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_13_x2 (Conv2D)             (None, 10, 10, 32)    36864       conv5_13_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_5_13 (Merge)              (None, 10, 10, 928)   0           concat_5_12[0][0]                \n",
      "                                                                   conv5_13_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_14_x1_bn (BatchNormalizati (None, 10, 10, 928)   3712        concat_5_13[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_14_x1_scale (Scale)        (None, 10, 10, 928)   1856        conv5_14_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu5_14_x1 (Activation)         (None, 10, 10, 928)   0           conv5_14_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_14_x1 (Conv2D)             (None, 10, 10, 128)   118784      relu5_14_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_14_x2_bn (BatchNormalizati (None, 10, 10, 128)   512         conv5_14_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_14_x2_scale (Scale)        (None, 10, 10, 128)   256         conv5_14_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu5_14_x2 (Activation)         (None, 10, 10, 128)   0           conv5_14_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_14_x2_zeropadding (ZeroPad (None, 12, 12, 128)   0           relu5_14_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_14_x2 (Conv2D)             (None, 10, 10, 32)    36864       conv5_14_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_5_14 (Merge)              (None, 10, 10, 960)   0           concat_5_13[0][0]                \n",
      "                                                                   conv5_14_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_15_x1_bn (BatchNormalizati (None, 10, 10, 960)   3840        concat_5_14[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_15_x1_scale (Scale)        (None, 10, 10, 960)   1920        conv5_15_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu5_15_x1 (Activation)         (None, 10, 10, 960)   0           conv5_15_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_15_x1 (Conv2D)             (None, 10, 10, 128)   122880      relu5_15_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_15_x2_bn (BatchNormalizati (None, 10, 10, 128)   512         conv5_15_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_15_x2_scale (Scale)        (None, 10, 10, 128)   256         conv5_15_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu5_15_x2 (Activation)         (None, 10, 10, 128)   0           conv5_15_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_15_x2_zeropadding (ZeroPad (None, 12, 12, 128)   0           relu5_15_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_15_x2 (Conv2D)             (None, 10, 10, 32)    36864       conv5_15_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_5_15 (Merge)              (None, 10, 10, 992)   0           concat_5_14[0][0]                \n",
      "                                                                   conv5_15_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_16_x1_bn (BatchNormalizati (None, 10, 10, 992)   3968        concat_5_15[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_16_x1_scale (Scale)        (None, 10, 10, 992)   1984        conv5_16_x1_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu5_16_x1 (Activation)         (None, 10, 10, 992)   0           conv5_16_x1_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_16_x1 (Conv2D)             (None, 10, 10, 128)   126976      relu5_16_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_16_x2_bn (BatchNormalizati (None, 10, 10, 128)   512         conv5_16_x1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_16_x2_scale (Scale)        (None, 10, 10, 128)   256         conv5_16_x2_bn[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "relu5_16_x2 (Activation)         (None, 10, 10, 128)   0           conv5_16_x2_scale[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv5_16_x2_zeropadding (ZeroPad (None, 12, 12, 128)   0           relu5_16_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_16_x2 (Conv2D)             (None, 10, 10, 32)    36864       conv5_16_x2_zeropadding[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "concat_5_16 (Merge)              (None, 10, 10, 1024)  0           concat_5_15[0][0]                \n",
      "                                                                   conv5_16_x2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_blk_bn (BatchNormalization (None, 10, 10, 1024)  4096        concat_5_16[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv5_blk_scale (Scale)          (None, 10, 10, 1024)  2048        conv5_blk_bn[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "relu5_blk (Activation)           (None, 10, 10, 1024)  0           conv5_blk_scale[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "pool5 (GlobalAveragePooling2D)   (None, 1024)          0           relu5_blk[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "fc6 (Dense)                      (None, 1000)          1025000     pool5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "prob (Activation)                (None, 1000)          0           fc6[0][0]                        \n",
      "====================================================================================================\n",
      "Total params: 8,146,152\n",
      "Trainable params: 8,062,504\n",
      "Non-trainable params: 83,648\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "weights_path = 'densenet121_weights_tf.h5'\n",
    "model = DenseNet(reduction=0.5, classes=1000, weights_path=weights_path)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size_r = 336\n",
    "image_size_c = 336\n",
    "def Feature_extract(model,layer_name,feature_name):\n",
    "    part_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "    sgd = SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    part_model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    lines = [line.rstrip('\\n') for line in open(root_pth + 'filenames.txt')]\n",
    "\n",
    "    for i in range(0,len(lines)):\n",
    "        print(i)\n",
    "        line = lines[i].split(' ')\n",
    "        image_path = os.path.join(dataset_path,line[0] + '.tiff')\n",
    "        img = cv2.imread(image_path)\n",
    "        test_image = np.zeros((1,image_size_r,image_size_c,3))\n",
    "        test_image[0,:,:,:] = img[:,:,:]\n",
    "\n",
    "\n",
    "        pth = feature_map_save_path + layer_name\n",
    "        create_dir(pth)\n",
    "        save_path = os.path.join(pth, line[0] + '.mat')\n",
    "        feature_map = part_model.predict(test_image)     \n",
    "        sio.savemat(save_path, {feature_name : feature_map})\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'relu5_blk'\n",
    "feature_name = 'fmap'\n",
    "Feature_extract(model,layer_name,feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'pool5'\n",
    "feature_name = 'fmap'\n",
    "Feature_extract(model,layer_name,feature_name)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Extract dmaps from SSNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    model=Sequential()\n",
    "\n",
    "    #conv1_1\n",
    "    model.add(Conv2D(64,kernel_size=3, strides=1,\n",
    "                 padding='SAME', use_bias=False, \n",
    "                 activation='relu',name='conv1_1',batch_input_shape=(None,None,None,3)))\n",
    "    #conv1_2\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=1,\n",
    "                 padding='SAME', use_bias=False, \n",
    "                 activation='relu',name='conv1_2'))\n",
    "    model.add(MaxPooling2D(pool_size=2,strides=2))\n",
    "\n",
    "    #conv2_1\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=1,\n",
    "                 padding='SAME', use_bias=False, \n",
    "                 activation='relu',name=\"conv2_1\"))\n",
    "\n",
    "    #conv2_2\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=1,\n",
    "                 padding='SAME', use_bias=False, \n",
    "                 activation='relu',name='conv2_2'))\n",
    "    model.add(MaxPooling2D(pool_size=2,strides=2))\n",
    "\n",
    "    #conv3_1\n",
    "    model.add(Conv2D(256, kernel_size=3, strides=1,\n",
    "                 padding='SAME', use_bias=False,\n",
    "                 activation='relu',name='conv3_1'))\n",
    "\n",
    "    #conv3_2\n",
    "    model.add(Conv2D(256, kernel_size=3, strides=1,\n",
    "                 padding='SAME', use_bias=False, \n",
    "                 activation='relu',name='conv3_2'))\n",
    "\n",
    "    #conv3_3\n",
    "    model.add(Conv2D(256, kernel_size=3, strides=1,\n",
    "                 padding='SAME', use_bias=False, \n",
    "                 activation='relu',name='conv3_3'))\n",
    "    model.add(MaxPooling2D(pool_size=2,strides=2))\n",
    "\n",
    "    #conv6\n",
    "    model.add(Conv2D(2048, kernel_size=8, strides=1, use_bias=False,\n",
    "                 activation='relu',name='conv6'))\n",
    "\n",
    "    #conv7\n",
    "    model.add(Conv2D(256, kernel_size=1, strides=1,\n",
    "                 use_bias=False,\n",
    "                 activation='relu',name='conv7'))\n",
    "\n",
    "    #conv8\n",
    "    model.add(Conv2D(2, kernel_size=1, strides=1,\n",
    "                 use_bias=False, \n",
    "                 name='conv8'))\n",
    "    \n",
    "    \n",
    "    model.add(Activation('softmax'))\n",
    "#     model.add(GlobalAveragePooling2D())\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = open(root_pth + 'sh_testing_531_housecounting_data_tagged.txt', 'r').read().split('\\n')\n",
    "locations = [row.split(' ') for row in locations]\n",
    "\n",
    "save_overlays = True\n",
    "\n",
    "\n",
    "image_extension = '.tiff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.load_weights('Models/base_weights.hdf5')\n",
    "\n",
    "img_h = 256\n",
    "img_w = 256\n",
    "\n",
    "img_h = np.int(img_h)\n",
    "img_w = np.int(img_w)\n",
    "\n",
    "M = [108,104,94] # mean values\n",
    "\n",
    "for idx,row in enumerate(locations):\n",
    "    print(idx)\n",
    "    f = Image.open(dataset_path + row[0] + image_extension)    \n",
    "    image = np.asarray(f, dtype=np.float32)\n",
    "    image = cv2.resize(image, (256,256))\n",
    "    if len(image.shape) == 3:\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        \n",
    "    test_img = np.zeros((1, img_h, img_w, 3))\n",
    "    overlay = np.zeros((img_h, img_w, 3))\n",
    "    \n",
    "    test_img[0,:,:,0] = image[0,:,:] - M[0]\n",
    "    test_img[0,:,:,1] = image[1,:,:] - M[1] \n",
    "    test_img[0,:,:,2] = image[2,:,:] - M[2]\n",
    "    test_img = np.float32(test_img)\n",
    "    \n",
    "    output = model.predict(test_img)\n",
    "    sample0 = imresize(output[0,:,:,0], [ img_h,  img_w], interp='bilinear', mode='F')\n",
    "    sample1 = imresize(output[0,:,:,1], [ img_h,  img_w], interp='bilinear', mode='F')         \n",
    "    \n",
    "    overlay[:,:,0] = image[0,:,:]\n",
    "    overlay[:,:,1] = image[1,:,:]\n",
    "    temp = image[2,:,:]\n",
    "\n",
    "    vis = (np.double(temp) * np.double(sample0)) + np.double(255*(sample1))\n",
    "    overlay[:,:,2] =  vis\n",
    "\n",
    "    filename = \"{}/{}.npy\".format(feature_map_save_path + 'dmaps', row[0])\n",
    "    sio.savemat(filename , {'dmap':sample1})\n",
    "    \n",
    "    if save_overlays:\n",
    "        overlay_path = \"{}/{}.png\".format(overlays_path, row[0])\n",
    "        imsave(overlay_path, overlay)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract gap_maps and pf_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = open(root_pth +'filenames.txt', 'r').read().split('\\n')\n",
    "locations = [row.split(' ') for row in locations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GAP(feature_map):\n",
    "    return np.average(feature_map, axis = (0, 1))\n",
    "mm=0\n",
    "for row in locations:\n",
    "    name = row[0]\n",
    "    #ssnet_map = np.load(open(ssnet_maps_path + name + '.npy', 'rb'))\n",
    "\n",
    "    ssnet_map = sio.loadmat(ssnet_maps_path + name + '_1.mat')\n",
    "    ssnet_map = ssnet_map['Prob']\n",
    "\n",
    "\n",
    "    #densenet_map = np.load(open(densenet_maps_path + name + '.npy', 'rb'))\n",
    "    densenet_map =  sio.loadmat(densenet_maps_path + name + '.mat')\n",
    "    densenet_map = densenet_map['fmap']\n",
    "    densenet_map = densenet_map[0, :, :, :]\n",
    "    densenet_gap_map = GAP(densenet_map)\n",
    "    \n",
    "    \n",
    "    resized_densenet_map = np.zeros((42, 42, 1024))\n",
    "    for i in range(1024):\n",
    "        resized_densenet_map[:, :, i] = imresize(densenet_map[:, :, i], (42, 42), interp = \"bilinear\", mode = 'F')\n",
    "    \n",
    "    resized_ssnet_map = np.zeros((42, 42))\n",
    "    #resized_ssnet_map = imresize(ssnet_map, (42, 42), interp = \"bilinear\", mode = 'F')\n",
    "    resized_ssnet_map = cv2.resize(ssnet_map, dsize=(42, 42), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    product_map = np.zeros((42, 42, 1024))\n",
    "    \n",
    "    for i in range(1024):\n",
    "        product_map[:, :, i] = np.multiply(resized_densenet_map[:, :, i], resized_ssnet_map)\n",
    "    product_map_gap = GAP(product_map)\n",
    "    \n",
    "    #sio.savemat(densenet_gap_maps_path + name, {'pgap':densenet_gap_map})\n",
    "    sio.savemat(product_maps_path + name, {'pfmap':product_map})\n",
    "    sio.savemat(gwap_maps_path + name, {'pgap':product_map_gap})\n",
    "    print(mm)\n",
    "    mm = mm+1\n",
    "    \n",
    "    #np.save(open(densenet_gap_maps_path + name + '.npy', 'wb'), densenet_gap_map)\n",
    "    #np.save(open(product_maps_path + name + '.npy', 'wb'), product_map)\n",
    "    #np.save(open(gwap_maps_path + name + '.npy', 'wb'), product_map_gap)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FusionNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(train_indices, batch_size, itr, lines, image_size=336):\n",
    "    # Create empty arrays to contain batch of features and labels#\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    batch_pmap = []\n",
    "    batch_product = []\n",
    "    startInd = itr*batch_size\n",
    "    endInd = startInd + batch_size\n",
    "    p_mapsize = 32\n",
    "    fmap_size = 42\n",
    "    for i in range(startInd, endInd):\n",
    "        line = lines[i].split(' ')\n",
    "#         image_path = os.path.join('/media/itu/CV_Lab2/Dr_waqas', 'DenseNetFeatures_pool5', line[0] + '.mat')\n",
    "        image_path = os.path.join(feature_map_save_path+ 'pool5', line[0] + '.mat')\n",
    "        #print(image_path)\n",
    "        fmap1 = sio.loadmat(image_path)\n",
    "        fmap1 = fmap1['fmap']\n",
    "        fmap = fmap1.ravel()\n",
    "        \n",
    "#         image_path1 = os.path.join('/media/itu/CV_Lab2/Dr_waqas', 'DenseNetFeatures_Pmaps1only_GAP', line[0] + '.mat')\n",
    "        image_path1 = os.path.join(feature_map_save_path+'gap_maps', line[0] + '.mat')\n",
    "        #print(image_path)\n",
    "        fmap11 = sio.loadmat(image_path1)\n",
    "        fmap11 = fmap11['pgap']\n",
    "        fmap1 = fmap11.ravel()\n",
    "        \n",
    "#         image_path2 = os.path.join('/media/itu/CV_Lab2/Dr_waqas', 'DenseNetFeatures_product', line[0] + '.mat')\n",
    "        image_path2 = os.path.join(feature_map_save_path + 'pf_maps', line[0] + '.mat')\n",
    "        #print(image_path)\n",
    "        fmap22 = sio.loadmat(image_path2)\n",
    "        fmap22 = fmap22['pfmap']\n",
    "        fmap2 = np.zeros((fmap_size,fmap_size,1024))\n",
    "        fmap2[:,:,:] = fmap22[:,:,:]\n",
    "        \n",
    "        \n",
    "        batch_images.append(fmap)\n",
    "        batch_pmap.append(fmap1)\n",
    "        batch_product.append(fmap2)\n",
    "        batch_labels.append(np.int(line[1]))\n",
    "        \n",
    "    batch_images = np.array(batch_images, dtype=np.float32)\n",
    "    batch_pmap = np.array(batch_pmap, dtype=np.float32)\n",
    "    batch_product = np.array(batch_product, dtype=np.float32)\n",
    "    batch_labels = np.array(batch_labels, dtype=np.float32)\n",
    "    \n",
    "    \n",
    "    return batch_images, batch_pmap, batch_product, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    RMSE loss function\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "#     loss = K.clip(loss,0,5)\n",
    "    return  loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(lr_r):\n",
    "  \n",
    "    f_map = Input(shape=(1024,), name='DenseNet')\n",
    "    p_map = Input(shape=(1024,), name='SSNEt')\n",
    "    prod_map = Input(shape=(42,42,1024), name='product')\n",
    "    \n",
    "    x_d = Dense(512, init='glorot_normal')(f_map)\n",
    "    x_d = LeakyReLU(alpha=0.3)(x_d)\n",
    "    x_d = Dropout(0.5)(x_d)\n",
    "    \n",
    "    x_s = Dense(512, init='glorot_normal')(p_map)\n",
    "    x_s = LeakyReLU(alpha=0.3)(x_s)\n",
    "    x_s = Dropout(0.5)(x_s)\n",
    "    \n",
    "    x_p = Convolution2D(1, (1, 1), init='glorot_normal', W_regularizer=l1(0.001))(prod_map)\n",
    "    x_p = LeakyReLU(alpha=0.3)(x_p)\n",
    "    x_p = Flatten()(x_p)\n",
    "    x_p = Dense(512, init='glorot_normal')(x_p)\n",
    "    x_p = LeakyReLU(alpha=0.3)(x_p)\n",
    "    x_p = Dropout(0.5)(x_p)\n",
    "    \n",
    "    \n",
    "    x = Concatenate(axis=-1)([x_d,  x_s, x_p]) \n",
    "    x = Dense(512, init='glorot_normal')(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(32, init='glorot_normal')(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1, init='glorot_normal')(x)    \n",
    "    x = Activation('linear')(x)\n",
    "    \n",
    "    adagrad=Adagrad(lr=lr_r, epsilon=1e-08)\n",
    "    \n",
    "    model_finetune = Model(inputs=[f_map, p_map, prod_map], outputs=x)    \n",
    "    model_finetune.compile(loss=root_mean_squared_error, optimizer=adagrad, metrics=['accuracy'])\n",
    "#     model_finetune.compile(loss='mean_squared_error', optimizer=adagrad, metrics=['accuracy'])\n",
    "   \n",
    "    return model_finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhtar/Naseer/lib/python2.7/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, kernel_initializer=\"glorot_normal\")`\n",
      "  import sys\n",
      "/home/akhtar/Naseer/lib/python2.7/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, kernel_initializer=\"glorot_normal\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/akhtar/Naseer/lib/python2.7/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (1, 1), kernel_regularizer=<keras.reg..., kernel_initializer=\"glorot_normal\")`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/akhtar/Naseer/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1190: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/akhtar/Naseer/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1208: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/akhtar/Naseer/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1297: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "product (InputLayer)             (None, 42, 42, 1024)  0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 42, 42, 1)     1025        product[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)        (None, 42, 42, 1)     0           conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "DenseNet (InputLayer)            (None, 1024)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "SSNEt (InputLayer)               (None, 1024)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 1764)          0           leaky_re_lu_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 512)           524800      DenseNet[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 512)           524800      SSNEt[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 512)           903680      flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)        (None, 512)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)        (None, 512)           0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)        (None, 512)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 512)           0           leaky_re_lu_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 512)           0           leaky_re_lu_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 512)           0           leaky_re_lu_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 1536)          0           dropout_1[0][0]                  \n",
      "                                                                   dropout_2[0][0]                  \n",
      "                                                                   dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 512)           786944      concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)        (None, 512)           0           dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 512)           0           leaky_re_lu_5[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 32)            16416       dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)        (None, 32)            0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 32)            0           leaky_re_lu_6[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 1)             33          dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 1)             0           dense_6[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 2,757,698\n",
      "Trainable params: 2,757,698\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhtar/Naseer/lib/python2.7/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, kernel_initializer=\"glorot_normal\")`\n",
      "/home/akhtar/Naseer/lib/python2.7/site-packages/ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, kernel_initializer=\"glorot_normal\")`\n",
      "/home/akhtar/Naseer/lib/python2.7/site-packages/ipykernel_launcher.py:27: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, kernel_initializer=\"glorot_normal\")`\n",
      "/home/akhtar/Naseer/lib/python2.7/site-packages/ipykernel_launcher.py:30: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"glorot_normal\")`\n"
     ]
    }
   ],
   "source": [
    "lr_r = 0.01\n",
    "model = create_model(lr_r)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight_path = root_pth  + 'FusionNet_weights'\n",
    "create_dir(model_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(root_pth+'FusionNet_weights/weights_24_Epochs.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10755,)\n",
      "Epoch: 0 itrerations: 0 Training Loss: 4.28543615341\n",
      "Epoch: 0 itrerations: 100 Training Loss: 15.3545198441\n",
      "Epoch: 0 itrerations: 200 Training Loss: 12.1068267822\n",
      "Epoch: 0 itrerations: 300 Training Loss: 10.312046051\n",
      "Epoch: 0 itrerations: 400 Training Loss: 9.81051254272\n",
      "Epoch: 0 itrerations: 500 Training Loss: 8.7645483017\n",
      "Epoch: 0 itrerations: 600 Training Loss: 8.90727710724\n",
      "Epoch: 0 itrerations: 700 Training Loss: 8.68954753876\n",
      "Epoch: 0 itrerations: 800 Training Loss: 8.69816493988\n",
      "Epoch: 0 itrerations: 900 Training Loss: 8.48792076111\n",
      "Epoch: 0 itrerations: 1000 Training Loss: 8.30631542206\n",
      "Epoch: 0 itrerations: 1100 Training Loss: 8.14824581146\n",
      "Epoch: 0 itrerations: 1200 Training Loss: 8.25808906555\n",
      "Epoch: 0 itrerations: 1300 Training Loss: 8.05711174011\n",
      "Epoch: 0 itrerations: 1400 Training Loss: 8.11654758453\n",
      "Epoch: 0 itrerations: 1500 Training Loss: 8.018866539\n",
      "Epoch: 0 itrerations: 1600 Training Loss: 7.95171833038\n",
      "Epoch: 0 itrerations: 1700 Training Loss: 7.86840629578\n",
      "Epoch: 0 itrerations: 1800 Training Loss: 7.75822877884\n",
      "Epoch: 0 itrerations: 1900 Training Loss: 7.71231842041\n",
      "Epoch: 0 itrerations: 2000 Training Loss: 7.65927553177\n",
      "Epoch: 0 itrerations: 2100 Training Loss: 7.54465341568\n",
      "Epoch: 0 itrerations: 2200 Training Loss: 7.43590116501\n",
      "Epoch: 0 itrerations: 2300 Training Loss: 7.43531703949\n",
      "Epoch: 0 itrerations: 2400 Training Loss: 7.33277893066\n",
      "Epoch: 0 itrerations: 2500 Training Loss: 7.2882642746\n",
      "Epoch: 0 itrerations: 2600 Training Loss: 7.27768707275\n",
      "Epoch: 0 itrerations: 2700 Training Loss: 7.30810070038\n",
      "Epoch: 0 itrerations: 2800 Training Loss: 7.23364067078\n",
      "Epoch: 0 itrerations: 2900 Training Loss: 7.14996862411\n",
      "Epoch: 0 itrerations: 3000 Training Loss: 7.19708395004\n",
      "Epoch: 0 itrerations: 3100 Training Loss: 7.14401340485\n",
      "Epoch: 0 itrerations: 3200 Training Loss: 7.14018583298\n",
      "Epoch: 0 itrerations: 3300 Training Loss: 7.06831598282\n",
      "Epoch: 0 itrerations: 3400 Training Loss: 7.04357528687\n",
      "Epoch: 0 itrerations: 3500 Training Loss: 7.02869844437\n",
      "Epoch: 0 itrerations: 3600 Training Loss: 7.00050354004\n",
      "Epoch: 0 itrerations: 3700 Training Loss: 6.9427113533\n",
      "Epoch: 0 itrerations: 3800 Training Loss: 6.9370470047\n",
      "Epoch: 0 itrerations: 3900 Training Loss: 6.90311384201\n",
      "Epoch: 0 itrerations: 4000 Training Loss: 6.88893938065\n",
      "Epoch: 0 itrerations: 4100 Training Loss: 6.87023591995\n",
      "Epoch: 0 itrerations: 4200 Training Loss: 6.84213972092\n",
      "Epoch: 0 itrerations: 4300 Training Loss: 6.8311662674\n",
      "Epoch: 0 itrerations: 4400 Training Loss: 6.81093025208\n",
      "Epoch: 0 itrerations: 4500 Training Loss: 6.79343032837\n",
      "Epoch: 0 itrerations: 4600 Training Loss: 6.78112411499\n",
      "Epoch: 0 itrerations: 4700 Training Loss: 6.75559711456\n",
      "Epoch: 0 itrerations: 4800 Training Loss: 6.72772693634\n",
      "Epoch: 0 itrerations: 4900 Training Loss: 6.69562482834\n",
      "Epoch: 0 itrerations: 5000 Training Loss: 6.66476535797\n",
      "Epoch: 0 itrerations: 5100 Training Loss: 6.63977813721\n",
      "Epoch: 0 itrerations: 5200 Training Loss: 6.61153888702\n",
      "Epoch: 0 itrerations: 5300 Training Loss: 6.58861923218\n",
      "Epoch: 0 itrerations: 5400 Training Loss: 6.5903968811\n",
      "Epoch: 0 itrerations: 5500 Training Loss: 6.55427837372\n",
      "Epoch: 0 itrerations: 5600 Training Loss: 6.54691410065\n",
      "Epoch: 0 itrerations: 5700 Training Loss: 6.53574752808\n",
      "Epoch: 0 itrerations: 5800 Training Loss: 6.52704763412\n",
      "Epoch: 0 itrerations: 5900 Training Loss: 6.51031064987\n",
      "Epoch: 0 itrerations: 6000 Training Loss: 6.48654413223\n",
      "Epoch: 0 itrerations: 6100 Training Loss: 6.45035123825\n",
      "Epoch: 0 itrerations: 6200 Training Loss: 6.43920707703\n",
      "Epoch: 0 itrerations: 6300 Training Loss: 6.4286904335\n",
      "Epoch: 0 itrerations: 6400 Training Loss: 6.40983486176\n",
      "Epoch: 0 itrerations: 6500 Training Loss: 6.38351535797\n",
      "Epoch: 0 itrerations: 6600 Training Loss: 6.35546112061\n",
      "Epoch: 0 itrerations: 6700 Training Loss: 6.35464048386\n",
      "Epoch: 0 itrerations: 6800 Training Loss: 6.36530637741\n",
      "Epoch: 0 itrerations: 6900 Training Loss: 6.37062215805\n",
      "Epoch: 0 itrerations: 7000 Training Loss: 6.39104366302\n",
      "Epoch: 0 itrerations: 7100 Training Loss: 6.38681840897\n",
      "Epoch: 0 itrerations: 7200 Training Loss: 6.35370206833\n",
      "Epoch: 0 itrerations: 7300 Training Loss: 6.33881616592\n",
      "Epoch: 0 itrerations: 7400 Training Loss: 6.31291484833\n",
      "Epoch: 0 itrerations: 7500 Training Loss: 6.30401277542\n",
      "Epoch: 0 itrerations: 7600 Training Loss: 6.27299404144\n",
      "Epoch: 0 itrerations: 7700 Training Loss: 6.26006412506\n",
      "Epoch: 0 itrerations: 7800 Training Loss: 6.26105880737\n",
      "Epoch: 0 itrerations: 7900 Training Loss: 6.26863479614\n",
      "Epoch: 0 itrerations: 8000 Training Loss: 6.25595378876\n",
      "Epoch: 0 itrerations: 8100 Training Loss: 6.24753904343\n",
      "Epoch: 0 itrerations: 8200 Training Loss: 6.24326276779\n",
      "Epoch: 0 itrerations: 8300 Training Loss: 6.22836828232\n",
      "Epoch: 0 itrerations: 8400 Training Loss: 6.21972608566\n",
      "Epoch: 0 itrerations: 8500 Training Loss: 6.20925045013\n",
      "Epoch: 0 itrerations: 8600 Training Loss: 6.19510984421\n",
      "Epoch: 0 itrerations: 8700 Training Loss: 6.18546724319\n",
      "Epoch: 0 itrerations: 8800 Training Loss: 6.18118810654\n",
      "Epoch: 0 itrerations: 8900 Training Loss: 6.16954994202\n",
      "Epoch: 0 itrerations: 9000 Training Loss: 6.17302322388\n",
      "Epoch: 0 itrerations: 9100 Training Loss: 6.1682882309\n",
      "Epoch: 0 itrerations: 9200 Training Loss: 6.16224336624\n",
      "Epoch: 0 itrerations: 9300 Training Loss: 6.14684724808\n",
      "Epoch: 0 itrerations: 9400 Training Loss: 6.12959384918\n",
      "Epoch: 0 itrerations: 9500 Training Loss: 6.11810016632\n",
      "Epoch: 0 itrerations: 9600 Training Loss: 6.12556600571\n",
      "Epoch: 0 itrerations: 9700 Training Loss: 6.1203455925\n",
      "Epoch: 0 itrerations: 9800 Training Loss: 6.11532688141\n",
      "Epoch: 0 itrerations: 9900 Training Loss: 6.10712385178\n",
      "Epoch: 0 itrerations: 10000 Training Loss: 6.11356735229\n",
      "Epoch: 0 itrerations: 10100 Training Loss: 6.10800600052\n",
      "Epoch: 0 itrerations: 10200 Training Loss: 6.09256458282\n",
      "Epoch: 0 itrerations: 10300 Training Loss: 6.08823537827\n",
      "Epoch: 0 itrerations: 10400 Training Loss: 6.08263254166\n",
      "Epoch: 0 itrerations: 10500 Training Loss: 6.06326150894\n",
      "Epoch: 0 itrerations: 10600 Training Loss: 6.05753326416\n",
      "Epoch: 0 itrerations: 10700 Training Loss: 6.05068445206\n",
      "Epoch: 1 itrerations: 0 Training Loss: 3.42640781403\n",
      "Epoch: 1 itrerations: 100 Training Loss: 4.46555566788\n",
      "Epoch: 1 itrerations: 200 Training Loss: 4.4745721817\n",
      "Epoch: 1 itrerations: 300 Training Loss: 4.67340612411\n",
      "Epoch: 1 itrerations: 400 Training Loss: 4.65734910965\n",
      "Epoch: 1 itrerations: 500 Training Loss: 4.39555168152\n",
      "Epoch: 1 itrerations: 600 Training Loss: 4.56314277649\n",
      "Epoch: 1 itrerations: 700 Training Loss: 4.6326622963\n",
      "Epoch: 1 itrerations: 800 Training Loss: 4.98337364197\n",
      "Epoch: 1 itrerations: 900 Training Loss: 4.9577589035\n",
      "Epoch: 1 itrerations: 1000 Training Loss: 5.04892778397\n",
      "Epoch: 1 itrerations: 1100 Training Loss: 5.04310369492\n",
      "Epoch: 1 itrerations: 1200 Training Loss: 5.11918926239\n",
      "Epoch: 1 itrerations: 1300 Training Loss: 5.09856510162\n",
      "Epoch: 1 itrerations: 1400 Training Loss: 5.22296237946\n",
      "Epoch: 1 itrerations: 1500 Training Loss: 5.18582010269\n",
      "Epoch: 1 itrerations: 1600 Training Loss: 5.25122404099\n",
      "Epoch: 1 itrerations: 1700 Training Loss: 5.28352165222\n",
      "Epoch: 1 itrerations: 1800 Training Loss: 5.26272630692\n",
      "Epoch: 1 itrerations: 1900 Training Loss: 5.30505132675\n",
      "Epoch: 1 itrerations: 2000 Training Loss: 5.30616521835\n",
      "Epoch: 1 itrerations: 2100 Training Loss: 5.27289390564\n",
      "Epoch: 1 itrerations: 2200 Training Loss: 5.25398445129\n",
      "Epoch: 1 itrerations: 2300 Training Loss: 5.27905035019\n",
      "Epoch: 1 itrerations: 2400 Training Loss: 5.25569009781\n",
      "Epoch: 1 itrerations: 2500 Training Loss: 5.22781515121\n",
      "Epoch: 1 itrerations: 2600 Training Loss: 5.24903678894\n",
      "Epoch: 1 itrerations: 2700 Training Loss: 5.28916501999\n",
      "Epoch: 1 itrerations: 2800 Training Loss: 5.2181687355\n",
      "Epoch: 1 itrerations: 2900 Training Loss: 5.17484521866\n",
      "Epoch: 1 itrerations: 3000 Training Loss: 5.2196893692\n",
      "Epoch: 1 itrerations: 3100 Training Loss: 5.20262002945\n",
      "Epoch: 1 itrerations: 3200 Training Loss: 5.22019481659\n",
      "Epoch: 1 itrerations: 3300 Training Loss: 5.19859218597\n",
      "Epoch: 1 itrerations: 3400 Training Loss: 5.21582984924\n",
      "Epoch: 1 itrerations: 3500 Training Loss: 5.26161623001\n",
      "Epoch: 1 itrerations: 3600 Training Loss: 5.2684211731\n",
      "Epoch: 1 itrerations: 3700 Training Loss: 5.25985813141\n",
      "Epoch: 1 itrerations: 3800 Training Loss: 5.27094888687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 itrerations: 3900 Training Loss: 5.25096702576\n",
      "Epoch: 1 itrerations: 4000 Training Loss: 5.25128078461\n",
      "Epoch: 1 itrerations: 4100 Training Loss: 5.23433589935\n",
      "Epoch: 1 itrerations: 4200 Training Loss: 5.25011253357\n",
      "Epoch: 1 itrerations: 4300 Training Loss: 5.26156711578\n",
      "Epoch: 1 itrerations: 4400 Training Loss: 5.27962398529\n",
      "Epoch: 1 itrerations: 4500 Training Loss: 5.28161716461\n",
      "Epoch: 1 itrerations: 4600 Training Loss: 5.291036129\n",
      "Epoch: 1 itrerations: 4700 Training Loss: 5.2882065773\n",
      "Epoch: 1 itrerations: 4800 Training Loss: 5.28749227524\n",
      "Epoch: 1 itrerations: 4900 Training Loss: 5.26590061188\n",
      "Epoch: 1 itrerations: 5000 Training Loss: 5.25592374802\n",
      "Epoch: 1 itrerations: 5100 Training Loss: 5.25083208084\n",
      "Epoch: 1 itrerations: 5200 Training Loss: 5.25567054749\n",
      "Epoch: 1 itrerations: 5300 Training Loss: 5.23260593414\n",
      "Epoch: 1 itrerations: 5400 Training Loss: 5.24987983704\n",
      "Epoch: 1 itrerations: 5500 Training Loss: 5.23260736465\n",
      "Epoch: 1 itrerations: 5600 Training Loss: 5.23273420334\n",
      "Epoch: 1 itrerations: 5700 Training Loss: 5.22509050369\n",
      "Epoch: 1 itrerations: 5800 Training Loss: 5.24665594101\n",
      "Epoch: 1 itrerations: 5900 Training Loss: 5.24728870392\n",
      "Epoch: 1 itrerations: 6000 Training Loss: 5.24235200882\n",
      "Epoch: 1 itrerations: 6100 Training Loss: 5.23171520233\n",
      "Epoch: 1 itrerations: 6200 Training Loss: 5.22755098343\n",
      "Epoch: 1 itrerations: 6300 Training Loss: 5.22412252426\n",
      "Epoch: 1 itrerations: 6400 Training Loss: 5.21286582947\n",
      "Epoch: 1 itrerations: 6500 Training Loss: 5.20976257324\n",
      "Epoch: 1 itrerations: 6600 Training Loss: 5.19793987274\n",
      "Epoch: 1 itrerations: 6700 Training Loss: 5.19399499893\n",
      "Epoch: 1 itrerations: 6800 Training Loss: 5.21623277664\n",
      "Epoch: 1 itrerations: 6900 Training Loss: 5.23138856888\n",
      "Epoch: 1 itrerations: 7000 Training Loss: 5.25244140625\n",
      "Epoch: 1 itrerations: 7100 Training Loss: 5.26824712753\n",
      "Epoch: 1 itrerations: 7200 Training Loss: 5.26352930069\n",
      "Epoch: 1 itrerations: 7300 Training Loss: 5.26472568512\n",
      "Epoch: 1 itrerations: 7400 Training Loss: 5.24973964691\n",
      "Epoch: 1 itrerations: 7500 Training Loss: 5.25341463089\n",
      "Epoch: 1 itrerations: 7600 Training Loss: 5.24349498749\n",
      "Epoch: 1 itrerations: 7700 Training Loss: 5.2310962677\n",
      "Epoch: 1 itrerations: 7800 Training Loss: 5.23191547394\n",
      "Epoch: 1 itrerations: 7900 Training Loss: 5.23821878433\n",
      "Epoch: 1 itrerations: 8000 Training Loss: 5.24100542068\n",
      "Epoch: 1 itrerations: 8100 Training Loss: 5.2322177887\n",
      "Epoch: 1 itrerations: 8200 Training Loss: 5.2430305481\n",
      "Epoch: 1 itrerations: 8300 Training Loss: 5.24328422546\n",
      "Epoch: 1 itrerations: 8400 Training Loss: 5.23815536499\n",
      "Epoch: 1 itrerations: 8500 Training Loss: 5.23764705658\n",
      "Epoch: 1 itrerations: 8600 Training Loss: 5.22144412994\n",
      "Epoch: 1 itrerations: 8700 Training Loss: 5.21475124359\n",
      "Epoch: 1 itrerations: 8800 Training Loss: 5.22810602188\n",
      "Epoch: 1 itrerations: 8900 Training Loss: 5.21649217606\n",
      "Epoch: 1 itrerations: 9000 Training Loss: 5.22197771072\n",
      "Epoch: 1 itrerations: 9100 Training Loss: 5.21996450424\n",
      "Epoch: 1 itrerations: 9200 Training Loss: 5.22303915024\n",
      "Epoch: 1 itrerations: 9300 Training Loss: 5.21408319473\n",
      "Epoch: 1 itrerations: 9400 Training Loss: 5.20875358582\n",
      "Epoch: 1 itrerations: 9500 Training Loss: 5.20642995834\n",
      "Epoch: 1 itrerations: 9600 Training Loss: 5.22589540482\n",
      "Epoch: 1 itrerations: 9700 Training Loss: 5.22876596451\n",
      "Epoch: 1 itrerations: 9800 Training Loss: 5.22048997879\n",
      "Epoch: 1 itrerations: 9900 Training Loss: 5.20952033997\n",
      "Epoch: 1 itrerations: 10000 Training Loss: 5.21984243393\n",
      "Epoch: 1 itrerations: 10100 Training Loss: 5.22048473358\n",
      "Epoch: 1 itrerations: 10200 Training Loss: 5.20908117294\n",
      "Epoch: 1 itrerations: 10300 Training Loss: 5.20519256592\n",
      "Epoch: 1 itrerations: 10400 Training Loss: 5.20618200302\n",
      "Epoch: 1 itrerations: 10500 Training Loss: 5.20157432556\n",
      "Epoch: 1 itrerations: 10600 Training Loss: 5.20197963715\n",
      "Epoch: 1 itrerations: 10700 Training Loss: 5.20244693756\n",
      "Epoch: 2 itrerations: 0 Training Loss: 0.848159909248\n",
      "Epoch: 2 itrerations: 100 Training Loss: 4.88741493225\n",
      "Epoch: 2 itrerations: 200 Training Loss: 4.80634641647\n",
      "Epoch: 2 itrerations: 300 Training Loss: 4.52487945557\n",
      "Epoch: 2 itrerations: 400 Training Loss: 4.79534673691\n",
      "Epoch: 2 itrerations: 500 Training Loss: 4.46460580826\n",
      "Epoch: 2 itrerations: 600 Training Loss: 4.50911998749\n",
      "Epoch: 2 itrerations: 700 Training Loss: 4.60851812363\n",
      "Epoch: 2 itrerations: 800 Training Loss: 4.77591276169\n",
      "Epoch: 2 itrerations: 900 Training Loss: 4.76994800568\n",
      "Epoch: 2 itrerations: 1000 Training Loss: 4.85216808319\n",
      "Epoch: 2 itrerations: 1100 Training Loss: 4.77936840057\n",
      "Epoch: 2 itrerations: 1200 Training Loss: 4.96622467041\n",
      "Epoch: 2 itrerations: 1300 Training Loss: 4.95166826248\n",
      "Epoch: 2 itrerations: 1400 Training Loss: 5.0428981781\n",
      "Epoch: 2 itrerations: 1500 Training Loss: 5.06361722946\n",
      "Epoch: 2 itrerations: 1600 Training Loss: 5.08602523804\n",
      "Epoch: 2 itrerations: 1700 Training Loss: 5.10761213303\n",
      "Epoch: 2 itrerations: 1800 Training Loss: 5.1501584053\n",
      "Epoch: 2 itrerations: 1900 Training Loss: 5.15884447098\n",
      "Epoch: 2 itrerations: 2000 Training Loss: 5.13004636765\n",
      "Epoch: 2 itrerations: 2100 Training Loss: 5.0943107605\n",
      "Epoch: 2 itrerations: 2200 Training Loss: 5.03941869736\n",
      "Epoch: 2 itrerations: 2300 Training Loss: 5.02473783493\n",
      "Epoch: 2 itrerations: 2400 Training Loss: 5.0021739006\n",
      "Epoch: 2 itrerations: 2500 Training Loss: 4.98320531845\n",
      "Epoch: 2 itrerations: 2600 Training Loss: 4.97012662888\n",
      "Epoch: 2 itrerations: 2700 Training Loss: 5.00411128998\n",
      "Epoch: 2 itrerations: 2800 Training Loss: 4.97763586044\n",
      "Epoch: 2 itrerations: 2900 Training Loss: 4.93439102173\n",
      "Epoch: 2 itrerations: 3000 Training Loss: 5.01941680908\n",
      "Epoch: 2 itrerations: 3100 Training Loss: 5.0013961792\n",
      "Epoch: 2 itrerations: 3200 Training Loss: 5.01737833023\n",
      "Epoch: 2 itrerations: 3300 Training Loss: 5.00022363663\n",
      "Epoch: 2 itrerations: 3400 Training Loss: 5.02017450333\n",
      "Epoch: 2 itrerations: 3500 Training Loss: 5.0547337532\n",
      "Epoch: 2 itrerations: 3600 Training Loss: 5.07402896881\n",
      "Epoch: 2 itrerations: 3700 Training Loss: 5.04863739014\n",
      "Epoch: 2 itrerations: 3800 Training Loss: 5.08488321304\n",
      "Epoch: 2 itrerations: 3900 Training Loss: 5.0582947731\n",
      "Epoch: 2 itrerations: 4000 Training Loss: 5.05671977997\n",
      "Epoch: 2 itrerations: 4100 Training Loss: 5.06077957153\n",
      "Epoch: 2 itrerations: 4200 Training Loss: 5.05651473999\n",
      "Epoch: 2 itrerations: 4300 Training Loss: 5.06664705276\n",
      "Epoch: 2 itrerations: 4400 Training Loss: 5.08234643936\n",
      "Epoch: 2 itrerations: 4500 Training Loss: 5.08096981049\n",
      "Epoch: 2 itrerations: 4600 Training Loss: 5.0665974617\n",
      "Epoch: 2 itrerations: 4700 Training Loss: 5.06245326996\n",
      "Epoch: 2 itrerations: 4800 Training Loss: 5.05131196976\n",
      "Epoch: 2 itrerations: 4900 Training Loss: 5.03638648987\n",
      "Epoch: 2 itrerations: 5000 Training Loss: 5.01301765442\n",
      "Epoch: 2 itrerations: 5100 Training Loss: 5.00182199478\n",
      "Epoch: 2 itrerations: 5200 Training Loss: 4.98984479904\n",
      "Epoch: 2 itrerations: 5300 Training Loss: 4.98571157455\n",
      "Epoch: 2 itrerations: 5400 Training Loss: 4.98868513107\n",
      "Epoch: 2 itrerations: 5500 Training Loss: 4.97958374023\n",
      "Epoch: 2 itrerations: 5600 Training Loss: 4.97258234024\n",
      "Epoch: 2 itrerations: 5700 Training Loss: 4.9694018364\n",
      "Epoch: 2 itrerations: 5800 Training Loss: 4.96599674225\n",
      "Epoch: 2 itrerations: 5900 Training Loss: 4.96925067902\n",
      "Epoch: 2 itrerations: 6000 Training Loss: 4.97526311874\n",
      "Epoch: 2 itrerations: 6100 Training Loss: 4.96421480179\n",
      "Epoch: 2 itrerations: 6200 Training Loss: 4.97747421265\n",
      "Epoch: 2 itrerations: 6300 Training Loss: 4.97570514679\n",
      "Epoch: 2 itrerations: 6400 Training Loss: 4.96823167801\n",
      "Epoch: 2 itrerations: 6500 Training Loss: 4.96596956253\n",
      "Epoch: 2 itrerations: 6600 Training Loss: 4.96027183533\n",
      "Epoch: 2 itrerations: 6700 Training Loss: 4.96179771423\n",
      "Epoch: 2 itrerations: 6800 Training Loss: 4.9708108902\n",
      "Epoch: 2 itrerations: 6900 Training Loss: 4.99264383316\n",
      "Epoch: 2 itrerations: 7000 Training Loss: 5.01908063889\n",
      "Epoch: 2 itrerations: 7100 Training Loss: 5.02927970886\n",
      "Epoch: 2 itrerations: 7200 Training Loss: 5.01428794861\n",
      "Epoch: 2 itrerations: 7300 Training Loss: 5.01386356354\n",
      "Epoch: 2 itrerations: 7400 Training Loss: 5.00547027588\n",
      "Epoch: 2 itrerations: 7500 Training Loss: 5.01438236237\n",
      "Epoch: 2 itrerations: 7600 Training Loss: 4.99850845337\n",
      "Epoch: 2 itrerations: 7700 Training Loss: 4.99310874939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 itrerations: 7800 Training Loss: 4.99193000793\n",
      "Epoch: 2 itrerations: 7900 Training Loss: 4.9998998642\n",
      "Epoch: 2 itrerations: 8000 Training Loss: 4.99886846542\n",
      "Epoch: 2 itrerations: 8100 Training Loss: 4.98927974701\n",
      "Epoch: 2 itrerations: 8200 Training Loss: 4.99568557739\n",
      "Epoch: 2 itrerations: 8300 Training Loss: 4.98779153824\n",
      "Epoch: 2 itrerations: 8400 Training Loss: 4.98696708679\n",
      "Epoch: 2 itrerations: 8500 Training Loss: 4.98662519455\n",
      "Epoch: 2 itrerations: 8600 Training Loss: 4.98356723785\n",
      "Epoch: 2 itrerations: 8700 Training Loss: 4.98570728302\n",
      "Epoch: 2 itrerations: 8800 Training Loss: 4.98607587814\n",
      "Epoch: 2 itrerations: 8900 Training Loss: 4.98292589188\n",
      "Epoch: 2 itrerations: 9000 Training Loss: 4.98847103119\n",
      "Epoch: 2 itrerations: 9100 Training Loss: 4.98972511292\n",
      "Epoch: 2 itrerations: 9200 Training Loss: 4.99161434174\n",
      "Epoch: 2 itrerations: 9300 Training Loss: 4.98361968994\n",
      "Epoch: 2 itrerations: 9400 Training Loss: 4.97889566422\n",
      "Epoch: 2 itrerations: 9500 Training Loss: 4.97629213333\n",
      "Epoch: 2 itrerations: 9600 Training Loss: 4.98365211487\n",
      "Epoch: 2 itrerations: 9700 Training Loss: 4.97998428345\n",
      "Epoch: 2 itrerations: 9800 Training Loss: 4.97669887543\n",
      "Epoch: 2 itrerations: 9900 Training Loss: 4.97268342972\n",
      "Epoch: 2 itrerations: 10000 Training Loss: 4.99380874634\n",
      "Epoch: 2 itrerations: 10100 Training Loss: 4.98531150818\n",
      "Epoch: 2 itrerations: 10200 Training Loss: 4.97908973694\n",
      "Epoch: 2 itrerations: 10300 Training Loss: 4.97325563431\n",
      "Epoch: 2 itrerations: 10400 Training Loss: 4.9680557251\n",
      "Epoch: 2 itrerations: 10500 Training Loss: 4.96163892746\n",
      "Epoch: 2 itrerations: 10600 Training Loss: 4.96669960022\n",
      "Epoch: 2 itrerations: 10700 Training Loss: 4.96687984467\n",
      "Epoch: 3 itrerations: 0 Training Loss: 0.448629081249\n",
      "Epoch: 3 itrerations: 100 Training Loss: 5.03676271439\n",
      "Epoch: 3 itrerations: 200 Training Loss: 4.45911693573\n",
      "Epoch: 3 itrerations: 300 Training Loss: 4.44579982758\n",
      "Epoch: 3 itrerations: 400 Training Loss: 4.71202468872\n",
      "Epoch: 3 itrerations: 500 Training Loss: 4.36359357834\n",
      "Epoch: 3 itrerations: 600 Training Loss: 4.58695077896\n",
      "Epoch: 3 itrerations: 700 Training Loss: 4.60663366318\n",
      "Epoch: 3 itrerations: 800 Training Loss: 4.77593660355\n",
      "Epoch: 3 itrerations: 900 Training Loss: 4.82120990753\n",
      "Epoch: 3 itrerations: 1000 Training Loss: 4.89677143097\n",
      "Epoch: 3 itrerations: 1100 Training Loss: 4.87181615829\n",
      "Epoch: 3 itrerations: 1200 Training Loss: 4.99272680283\n",
      "Epoch: 3 itrerations: 1300 Training Loss: 4.96137714386\n",
      "Epoch: 3 itrerations: 1400 Training Loss: 5.00490427017\n",
      "Epoch: 3 itrerations: 1500 Training Loss: 4.97331809998\n",
      "Epoch: 3 itrerations: 1600 Training Loss: 4.97401428223\n",
      "Epoch: 3 itrerations: 1700 Training Loss: 4.97354459763\n",
      "Epoch: 3 itrerations: 1800 Training Loss: 4.99154043198\n",
      "Epoch: 3 itrerations: 1900 Training Loss: 5.05452728271\n",
      "Epoch: 3 itrerations: 2000 Training Loss: 5.058552742\n",
      "Epoch: 3 itrerations: 2100 Training Loss: 5.03195619583\n",
      "Epoch: 3 itrerations: 2200 Training Loss: 5.03344678879\n",
      "Epoch: 3 itrerations: 2300 Training Loss: 5.0170211792\n",
      "Epoch: 3 itrerations: 2400 Training Loss: 5.00560569763\n",
      "Epoch: 3 itrerations: 2500 Training Loss: 4.99388837814\n",
      "Epoch: 3 itrerations: 2600 Training Loss: 4.98332977295\n",
      "Epoch: 3 itrerations: 2700 Training Loss: 5.00443792343\n",
      "Epoch: 3 itrerations: 2800 Training Loss: 4.97065734863\n",
      "Epoch: 3 itrerations: 2900 Training Loss: 4.93231296539\n",
      "Epoch: 3 itrerations: 3000 Training Loss: 4.97084760666\n",
      "Epoch: 3 itrerations: 3100 Training Loss: 4.97237682343\n",
      "Epoch: 3 itrerations: 3200 Training Loss: 4.95920848846\n",
      "Epoch: 3 itrerations: 3300 Training Loss: 4.9183306694\n",
      "Epoch: 3 itrerations: 3400 Training Loss: 4.92799568176\n",
      "Epoch: 3 itrerations: 3500 Training Loss: 4.97504854202\n",
      "Epoch: 3 itrerations: 3600 Training Loss: 4.9870595932\n",
      "Epoch: 3 itrerations: 3700 Training Loss: 4.98542785645\n",
      "Epoch: 3 itrerations: 3800 Training Loss: 5.01160812378\n",
      "Epoch: 3 itrerations: 3900 Training Loss: 5.00644683838\n",
      "Epoch: 3 itrerations: 4000 Training Loss: 4.99429607391\n",
      "Epoch: 3 itrerations: 4100 Training Loss: 5.00802755356\n",
      "Epoch: 3 itrerations: 4200 Training Loss: 5.00393199921\n",
      "Epoch: 3 itrerations: 4300 Training Loss: 4.99237108231\n",
      "Epoch: 3 itrerations: 4400 Training Loss: 5.00312185287\n",
      "Epoch: 3 itrerations: 4500 Training Loss: 4.99294424057\n",
      "Epoch: 3 itrerations: 4600 Training Loss: 4.9898648262\n",
      "Epoch: 3 itrerations: 4700 Training Loss: 4.98924160004\n",
      "Epoch: 3 itrerations: 4800 Training Loss: 4.98069000244\n",
      "Epoch: 3 itrerations: 4900 Training Loss: 4.96903705597\n",
      "Epoch: 3 itrerations: 5000 Training Loss: 4.93572616577\n",
      "Epoch: 3 itrerations: 5100 Training Loss: 4.93103456497\n",
      "Epoch: 3 itrerations: 5200 Training Loss: 4.92464828491\n",
      "Epoch: 3 itrerations: 5300 Training Loss: 4.91037893295\n",
      "Epoch: 3 itrerations: 5400 Training Loss: 4.93416166306\n",
      "Epoch: 3 itrerations: 5500 Training Loss: 4.92379665375\n",
      "Epoch: 3 itrerations: 5600 Training Loss: 4.92864084244\n",
      "Epoch: 3 itrerations: 5700 Training Loss: 4.9267334938\n",
      "Epoch: 3 itrerations: 5800 Training Loss: 4.93404054642\n",
      "Epoch: 3 itrerations: 5900 Training Loss: 4.9407749176\n",
      "Epoch: 3 itrerations: 6000 Training Loss: 4.94472789764\n",
      "Epoch: 3 itrerations: 6100 Training Loss: 4.93782997131\n",
      "Epoch: 3 itrerations: 6200 Training Loss: 4.94371128082\n",
      "Epoch: 3 itrerations: 6300 Training Loss: 4.94573736191\n",
      "Epoch: 3 itrerations: 6400 Training Loss: 4.94283628464\n",
      "Epoch: 3 itrerations: 6500 Training Loss: 4.93632602692\n",
      "Epoch: 3 itrerations: 6600 Training Loss: 4.92417097092\n",
      "Epoch: 3 itrerations: 6700 Training Loss: 4.91828536987\n",
      "Epoch: 3 itrerations: 6800 Training Loss: 4.92022752762\n",
      "Epoch: 3 itrerations: 6900 Training Loss: 4.93830490112\n",
      "Epoch: 3 itrerations: 7000 Training Loss: 4.96051025391\n",
      "Epoch: 3 itrerations: 7100 Training Loss: 4.97441291809\n",
      "Epoch: 3 itrerations: 7200 Training Loss: 4.95816230774\n",
      "Epoch: 3 itrerations: 7300 Training Loss: 4.96013069153\n",
      "Epoch: 3 itrerations: 7400 Training Loss: 4.95584821701\n",
      "Epoch: 3 itrerations: 7500 Training Loss: 4.95230865479\n",
      "Epoch: 3 itrerations: 7600 Training Loss: 4.93690681458\n",
      "Epoch: 3 itrerations: 7700 Training Loss: 4.92950344086\n",
      "Epoch: 3 itrerations: 7800 Training Loss: 4.94255447388\n",
      "Epoch: 3 itrerations: 7900 Training Loss: 4.95317602158\n",
      "Epoch: 3 itrerations: 8000 Training Loss: 4.95186805725\n",
      "Epoch: 3 itrerations: 8100 Training Loss: 4.94165897369\n",
      "Epoch: 3 itrerations: 8200 Training Loss: 4.95311784744\n",
      "Epoch: 3 itrerations: 8300 Training Loss: 4.95080184937\n",
      "Epoch: 3 itrerations: 8400 Training Loss: 4.94818496704\n",
      "Epoch: 3 itrerations: 8500 Training Loss: 4.9474978447\n",
      "Epoch: 3 itrerations: 8600 Training Loss: 4.93417072296\n",
      "Epoch: 3 itrerations: 8700 Training Loss: 4.93265485764\n",
      "Epoch: 3 itrerations: 8800 Training Loss: 4.93416118622\n",
      "Epoch: 3 itrerations: 8900 Training Loss: 4.92308664322\n",
      "Epoch: 3 itrerations: 9000 Training Loss: 4.92399549484\n",
      "Epoch: 3 itrerations: 9100 Training Loss: 4.91915273666\n",
      "Epoch: 3 itrerations: 9200 Training Loss: 4.91475343704\n",
      "Epoch: 3 itrerations: 9300 Training Loss: 4.91007328033\n",
      "Epoch: 3 itrerations: 9400 Training Loss: 4.90355730057\n",
      "Epoch: 3 itrerations: 9500 Training Loss: 4.89428567886\n",
      "Epoch: 3 itrerations: 9600 Training Loss: 4.91102266312\n",
      "Epoch: 3 itrerations: 9700 Training Loss: 4.91873598099\n",
      "Epoch: 3 itrerations: 9800 Training Loss: 4.91064977646\n",
      "Epoch: 3 itrerations: 9900 Training Loss: 4.90759944916\n",
      "Epoch: 3 itrerations: 10000 Training Loss: 4.92375659943\n",
      "Epoch: 3 itrerations: 10100 Training Loss: 4.92004871368\n",
      "Epoch: 3 itrerations: 10200 Training Loss: 4.91596078873\n",
      "Epoch: 3 itrerations: 10300 Training Loss: 4.91600990295\n",
      "Epoch: 3 itrerations: 10400 Training Loss: 4.91490077972\n",
      "Epoch: 3 itrerations: 10500 Training Loss: 4.9066362381\n",
      "Epoch: 3 itrerations: 10600 Training Loss: 4.90980148315\n",
      "Epoch: 3 itrerations: 10700 Training Loss: 4.90818071365\n",
      "Epoch: 4 itrerations: 0 Training Loss: 6.05041599274\n",
      "Epoch: 4 itrerations: 100 Training Loss: 3.75733065605\n",
      "Epoch: 4 itrerations: 200 Training Loss: 3.57600331306\n",
      "Epoch: 4 itrerations: 300 Training Loss: 4.22135353088\n",
      "Epoch: 4 itrerations: 400 Training Loss: 4.37050914764\n",
      "Epoch: 4 itrerations: 500 Training Loss: 4.14715766907\n",
      "Epoch: 4 itrerations: 600 Training Loss: 4.41251468658\n",
      "Epoch: 4 itrerations: 700 Training Loss: 4.50907993317\n",
      "Epoch: 4 itrerations: 800 Training Loss: 4.67156219482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 itrerations: 900 Training Loss: 4.67462921143\n",
      "Epoch: 4 itrerations: 1000 Training Loss: 4.65913486481\n",
      "Epoch: 4 itrerations: 1100 Training Loss: 4.61375617981\n",
      "Epoch: 4 itrerations: 1200 Training Loss: 4.72556304932\n",
      "Epoch: 4 itrerations: 1300 Training Loss: 4.7138671875\n",
      "Epoch: 4 itrerations: 1400 Training Loss: 4.77700138092\n",
      "Epoch: 4 itrerations: 1500 Training Loss: 4.76285219193\n",
      "Epoch: 4 itrerations: 1600 Training Loss: 4.75558710098\n",
      "Epoch: 4 itrerations: 1700 Training Loss: 4.74680757523\n",
      "Epoch: 4 itrerations: 1800 Training Loss: 4.74588489532\n",
      "Epoch: 4 itrerations: 1900 Training Loss: 4.7876496315\n",
      "Epoch: 4 itrerations: 2000 Training Loss: 4.81170368195\n",
      "Epoch: 4 itrerations: 2100 Training Loss: 4.78027582169\n",
      "Epoch: 4 itrerations: 2200 Training Loss: 4.79221343994\n",
      "Epoch: 4 itrerations: 2300 Training Loss: 4.82284975052\n",
      "Epoch: 4 itrerations: 2400 Training Loss: 4.79176330566\n",
      "Epoch: 4 itrerations: 2500 Training Loss: 4.76947402954\n",
      "Epoch: 4 itrerations: 2600 Training Loss: 4.75718975067\n",
      "Epoch: 4 itrerations: 2700 Training Loss: 4.7932267189\n",
      "Epoch: 4 itrerations: 2800 Training Loss: 4.7576675415\n",
      "Epoch: 4 itrerations: 2900 Training Loss: 4.73964977264\n",
      "Epoch: 4 itrerations: 3000 Training Loss: 4.79590320587\n",
      "Epoch: 4 itrerations: 3100 Training Loss: 4.78229045868\n",
      "Epoch: 4 itrerations: 3200 Training Loss: 4.79868221283\n",
      "Epoch: 4 itrerations: 3300 Training Loss: 4.76746749878\n",
      "Epoch: 4 itrerations: 3400 Training Loss: 4.8055934906\n",
      "Epoch: 4 itrerations: 3500 Training Loss: 4.83118772507\n",
      "Epoch: 4 itrerations: 3600 Training Loss: 4.84300470352\n",
      "Epoch: 4 itrerations: 3700 Training Loss: 4.84205055237\n",
      "Epoch: 4 itrerations: 3800 Training Loss: 4.88861751556\n",
      "Epoch: 4 itrerations: 3900 Training Loss: 4.86840105057\n",
      "Epoch: 4 itrerations: 4000 Training Loss: 4.86907863617\n",
      "Epoch: 4 itrerations: 4100 Training Loss: 4.86573410034\n",
      "Epoch: 4 itrerations: 4200 Training Loss: 4.84656381607\n",
      "Epoch: 4 itrerations: 4300 Training Loss: 4.85630178452\n",
      "Epoch: 4 itrerations: 4400 Training Loss: 4.87085342407\n",
      "Epoch: 4 itrerations: 4500 Training Loss: 4.88158369064\n",
      "Epoch: 4 itrerations: 4600 Training Loss: 4.87166404724\n",
      "Epoch: 4 itrerations: 4700 Training Loss: 4.86942148209\n",
      "Epoch: 4 itrerations: 4800 Training Loss: 4.86048460007\n",
      "Epoch: 4 itrerations: 4900 Training Loss: 4.85608482361\n",
      "Epoch: 4 itrerations: 5000 Training Loss: 4.83299922943\n",
      "Epoch: 4 itrerations: 5100 Training Loss: 4.81396055222\n",
      "Epoch: 4 itrerations: 5200 Training Loss: 4.81000423431\n",
      "Epoch: 4 itrerations: 5300 Training Loss: 4.79992294312\n",
      "Epoch: 4 itrerations: 5400 Training Loss: 4.81795024872\n",
      "Epoch: 4 itrerations: 5500 Training Loss: 4.81104564667\n",
      "Epoch: 4 itrerations: 5600 Training Loss: 4.80607461929\n",
      "Epoch: 4 itrerations: 5700 Training Loss: 4.80784702301\n",
      "Epoch: 4 itrerations: 5800 Training Loss: 4.82343292236\n",
      "Epoch: 4 itrerations: 5900 Training Loss: 4.82731294632\n",
      "Epoch: 4 itrerations: 6000 Training Loss: 4.82705307007\n",
      "Epoch: 4 itrerations: 6100 Training Loss: 4.81743621826\n",
      "Epoch: 4 itrerations: 6200 Training Loss: 4.81254720688\n",
      "Epoch: 4 itrerations: 6300 Training Loss: 4.81496524811\n",
      "Epoch: 4 itrerations: 6400 Training Loss: 4.80572605133\n",
      "Epoch: 4 itrerations: 6500 Training Loss: 4.79992198944\n",
      "Epoch: 4 itrerations: 6600 Training Loss: 4.79932689667\n",
      "Epoch: 4 itrerations: 6700 Training Loss: 4.80598640442\n",
      "Epoch: 4 itrerations: 6800 Training Loss: 4.81028461456\n",
      "Epoch: 4 itrerations: 6900 Training Loss: 4.81688022614\n",
      "Epoch: 4 itrerations: 7000 Training Loss: 4.84235858917\n",
      "Epoch: 4 itrerations: 7100 Training Loss: 4.85288858414\n",
      "Epoch: 4 itrerations: 7200 Training Loss: 4.83893823624\n",
      "Epoch: 4 itrerations: 7300 Training Loss: 4.83693122864\n",
      "Epoch: 4 itrerations: 7400 Training Loss: 4.82535743713\n",
      "Epoch: 4 itrerations: 7500 Training Loss: 4.82945775986\n",
      "Epoch: 4 itrerations: 7600 Training Loss: 4.82450294495\n",
      "Epoch: 4 itrerations: 7700 Training Loss: 4.80681991577\n",
      "Epoch: 4 itrerations: 7800 Training Loss: 4.79700374603\n",
      "Epoch: 4 itrerations: 7900 Training Loss: 4.80507898331\n",
      "Epoch: 4 itrerations: 8000 Training Loss: 4.80651855469\n",
      "Epoch: 4 itrerations: 8100 Training Loss: 4.7994556427\n",
      "Epoch: 4 itrerations: 8200 Training Loss: 4.80337762833\n",
      "Epoch: 4 itrerations: 8300 Training Loss: 4.81481742859\n",
      "Epoch: 4 itrerations: 8400 Training Loss: 4.81435585022\n",
      "Epoch: 4 itrerations: 8500 Training Loss: 4.81668376923\n",
      "Epoch: 4 itrerations: 8600 Training Loss: 4.80848121643\n",
      "Epoch: 4 itrerations: 8700 Training Loss: 4.81052446365\n",
      "Epoch: 4 itrerations: 8800 Training Loss: 4.8158249855\n",
      "Epoch: 4 itrerations: 8900 Training Loss: 4.8137845993\n",
      "Epoch: 4 itrerations: 9000 Training Loss: 4.81593465805\n",
      "Epoch: 4 itrerations: 9100 Training Loss: 4.81758022308\n",
      "Epoch: 4 itrerations: 9200 Training Loss: 4.82639932632\n",
      "Epoch: 4 itrerations: 9300 Training Loss: 4.8211517334\n",
      "Epoch: 4 itrerations: 9400 Training Loss: 4.80721902847\n",
      "Epoch: 4 itrerations: 9500 Training Loss: 4.80486726761\n",
      "Epoch: 4 itrerations: 9600 Training Loss: 4.82313632965\n",
      "Epoch: 4 itrerations: 9700 Training Loss: 4.82909393311\n",
      "Epoch: 4 itrerations: 9800 Training Loss: 4.82349920273\n",
      "Epoch: 4 itrerations: 9900 Training Loss: 4.81804418564\n",
      "Epoch: 4 itrerations: 10000 Training Loss: 4.84065485001\n",
      "Epoch: 4 itrerations: 10100 Training Loss: 4.84227991104\n",
      "Epoch: 4 itrerations: 10200 Training Loss: 4.83564090729\n",
      "Epoch: 4 itrerations: 10300 Training Loss: 4.83740663528\n",
      "Epoch: 4 itrerations: 10400 Training Loss: 4.83554553986\n",
      "Epoch: 4 itrerations: 10500 Training Loss: 4.83041763306\n",
      "Epoch: 4 itrerations: 10600 Training Loss: 4.83243370056\n",
      "Epoch: 4 itrerations: 10700 Training Loss: 4.82807826996\n",
      "Epoch: 5 itrerations: 0 Training Loss: 4.70837926865\n",
      "Epoch: 5 itrerations: 100 Training Loss: 4.42775297165\n",
      "Epoch: 5 itrerations: 200 Training Loss: 4.5478181839\n",
      "Epoch: 5 itrerations: 300 Training Loss: 4.47864723206\n",
      "Epoch: 5 itrerations: 400 Training Loss: 4.57827425003\n",
      "Epoch: 5 itrerations: 500 Training Loss: 4.24512624741\n",
      "Epoch: 5 itrerations: 600 Training Loss: 4.58860492706\n",
      "Epoch: 5 itrerations: 700 Training Loss: 4.70770835876\n",
      "Epoch: 5 itrerations: 800 Training Loss: 4.97698545456\n",
      "Epoch: 5 itrerations: 900 Training Loss: 5.05024051666\n",
      "Epoch: 5 itrerations: 1000 Training Loss: 5.06284952164\n",
      "Epoch: 5 itrerations: 1100 Training Loss: 4.96245241165\n",
      "Epoch: 5 itrerations: 1200 Training Loss: 5.07815504074\n",
      "Epoch: 5 itrerations: 1300 Training Loss: 5.09473848343\n",
      "Epoch: 5 itrerations: 1400 Training Loss: 5.17047548294\n",
      "Epoch: 5 itrerations: 1500 Training Loss: 5.17639493942\n",
      "Epoch: 5 itrerations: 1600 Training Loss: 5.15970230103\n",
      "Epoch: 5 itrerations: 1700 Training Loss: 5.13395166397\n",
      "Epoch: 5 itrerations: 1800 Training Loss: 5.09979963303\n",
      "Epoch: 5 itrerations: 1900 Training Loss: 5.1453909874\n",
      "Epoch: 5 itrerations: 2000 Training Loss: 5.16990089417\n",
      "Epoch: 5 itrerations: 2100 Training Loss: 5.1259727478\n",
      "Epoch: 5 itrerations: 2200 Training Loss: 5.11348342896\n",
      "Epoch: 5 itrerations: 2300 Training Loss: 5.11412000656\n",
      "Epoch: 5 itrerations: 2400 Training Loss: 5.0472278595\n",
      "Epoch: 5 itrerations: 2500 Training Loss: 5.03875303268\n",
      "Epoch: 5 itrerations: 2600 Training Loss: 5.00016307831\n",
      "Epoch: 5 itrerations: 2700 Training Loss: 5.03811502457\n",
      "Epoch: 5 itrerations: 2800 Training Loss: 4.98945522308\n",
      "Epoch: 5 itrerations: 2900 Training Loss: 4.96693372726\n",
      "Epoch: 5 itrerations: 3000 Training Loss: 5.00481557846\n",
      "Epoch: 5 itrerations: 3100 Training Loss: 4.98293066025\n",
      "Epoch: 5 itrerations: 3200 Training Loss: 4.9969291687\n",
      "Epoch: 5 itrerations: 3300 Training Loss: 4.94600439072\n",
      "Epoch: 5 itrerations: 3400 Training Loss: 4.95478487015\n",
      "Epoch: 5 itrerations: 3500 Training Loss: 4.9691696167\n",
      "Epoch: 5 itrerations: 3600 Training Loss: 4.97532463074\n",
      "Epoch: 5 itrerations: 3700 Training Loss: 4.95130062103\n",
      "Epoch: 5 itrerations: 3800 Training Loss: 4.96251440048\n",
      "Epoch: 5 itrerations: 3900 Training Loss: 4.94389390945\n",
      "Epoch: 5 itrerations: 4000 Training Loss: 4.94029188156\n",
      "Epoch: 5 itrerations: 4100 Training Loss: 4.94203567505\n",
      "Epoch: 5 itrerations: 4200 Training Loss: 4.92307519913\n",
      "Epoch: 5 itrerations: 4300 Training Loss: 4.93237924576\n",
      "Epoch: 5 itrerations: 4400 Training Loss: 4.94277477264\n",
      "Epoch: 5 itrerations: 4500 Training Loss: 4.93968725204\n",
      "Epoch: 5 itrerations: 4600 Training Loss: 4.93320512772\n",
      "Epoch: 5 itrerations: 4700 Training Loss: 4.92475318909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 itrerations: 4800 Training Loss: 4.90960550308\n",
      "Epoch: 5 itrerations: 4900 Training Loss: 4.90338850021\n",
      "Epoch: 5 itrerations: 5000 Training Loss: 4.87696743011\n",
      "Epoch: 5 itrerations: 5100 Training Loss: 4.86368989944\n",
      "Epoch: 5 itrerations: 5200 Training Loss: 4.87488508224\n",
      "Epoch: 5 itrerations: 5300 Training Loss: 4.87199258804\n",
      "Epoch: 5 itrerations: 5400 Training Loss: 4.8784160614\n",
      "Epoch: 5 itrerations: 5500 Training Loss: 4.87297010422\n",
      "Epoch: 5 itrerations: 5600 Training Loss: 4.86943244934\n",
      "Epoch: 5 itrerations: 5700 Training Loss: 4.8657913208\n",
      "Epoch: 5 itrerations: 5800 Training Loss: 4.86817884445\n",
      "Epoch: 5 itrerations: 5900 Training Loss: 4.8650932312\n",
      "Epoch: 5 itrerations: 6000 Training Loss: 4.86750555038\n",
      "Epoch: 5 itrerations: 6100 Training Loss: 4.86302471161\n",
      "Epoch: 5 itrerations: 6200 Training Loss: 4.87208986282\n",
      "Epoch: 5 itrerations: 6300 Training Loss: 4.87150239944\n",
      "Epoch: 5 itrerations: 6400 Training Loss: 4.8614449501\n",
      "Epoch: 5 itrerations: 6500 Training Loss: 4.85844516754\n",
      "Epoch: 5 itrerations: 6600 Training Loss: 4.853307724\n",
      "Epoch: 5 itrerations: 6700 Training Loss: 4.85717487335\n",
      "Epoch: 5 itrerations: 6800 Training Loss: 4.86642026901\n",
      "Epoch: 5 itrerations: 6900 Training Loss: 4.87562561035\n",
      "Epoch: 5 itrerations: 7000 Training Loss: 4.88119220734\n",
      "Epoch: 5 itrerations: 7100 Training Loss: 4.89706850052\n",
      "Epoch: 5 itrerations: 7200 Training Loss: 4.88538885117\n",
      "Epoch: 5 itrerations: 7300 Training Loss: 4.89047622681\n",
      "Epoch: 5 itrerations: 7400 Training Loss: 4.88021421432\n",
      "Epoch: 5 itrerations: 7500 Training Loss: 4.87806940079\n",
      "Epoch: 5 itrerations: 7600 Training Loss: 4.86496114731\n",
      "Epoch: 5 itrerations: 7700 Training Loss: 4.86405992508\n",
      "Epoch: 5 itrerations: 7800 Training Loss: 4.86628723145\n",
      "Epoch: 5 itrerations: 7900 Training Loss: 4.87193393707\n",
      "Epoch: 5 itrerations: 8000 Training Loss: 4.87180709839\n",
      "Epoch: 5 itrerations: 8100 Training Loss: 4.86514759064\n",
      "Epoch: 5 itrerations: 8200 Training Loss: 4.87282514572\n",
      "Epoch: 5 itrerations: 8300 Training Loss: 4.87632751465\n",
      "Epoch: 5 itrerations: 8400 Training Loss: 4.87018489838\n",
      "Epoch: 5 itrerations: 8500 Training Loss: 4.86762237549\n",
      "Epoch: 5 itrerations: 8600 Training Loss: 4.85352420807\n",
      "Epoch: 5 itrerations: 8700 Training Loss: 4.84728002548\n",
      "Epoch: 5 itrerations: 8800 Training Loss: 4.84695911407\n",
      "Epoch: 5 itrerations: 8900 Training Loss: 4.84290647507\n",
      "Epoch: 5 itrerations: 9000 Training Loss: 4.85006904602\n",
      "Epoch: 5 itrerations: 9100 Training Loss: 4.85441589355\n",
      "Epoch: 5 itrerations: 9200 Training Loss: 4.85459661484\n",
      "Epoch: 5 itrerations: 9300 Training Loss: 4.85115194321\n",
      "Epoch: 5 itrerations: 9400 Training Loss: 4.83784103394\n",
      "Epoch: 5 itrerations: 9500 Training Loss: 4.83659362793\n",
      "Epoch: 5 itrerations: 9600 Training Loss: 4.85008430481\n",
      "Epoch: 5 itrerations: 9700 Training Loss: 4.8468708992\n",
      "Epoch: 5 itrerations: 9800 Training Loss: 4.84009695053\n",
      "Epoch: 5 itrerations: 9900 Training Loss: 4.83668470383\n",
      "Epoch: 5 itrerations: 10000 Training Loss: 4.85012960434\n",
      "Epoch: 5 itrerations: 10100 Training Loss: 4.85446453094\n",
      "Epoch: 5 itrerations: 10200 Training Loss: 4.85401439667\n",
      "Epoch: 5 itrerations: 10300 Training Loss: 4.8576836586\n",
      "Epoch: 5 itrerations: 10400 Training Loss: 4.85213518143\n",
      "Epoch: 5 itrerations: 10500 Training Loss: 4.8394370079\n",
      "Epoch: 5 itrerations: 10600 Training Loss: 4.84003019333\n",
      "Epoch: 5 itrerations: 10700 Training Loss: 4.84596061707\n",
      "Epoch: 6 itrerations: 0 Training Loss: 1.09929680824\n",
      "Epoch: 6 itrerations: 100 Training Loss: 4.00207471848\n",
      "Epoch: 6 itrerations: 200 Training Loss: 3.69628930092\n",
      "Epoch: 6 itrerations: 300 Training Loss: 3.90188741684\n",
      "Epoch: 6 itrerations: 400 Training Loss: 4.23475074768\n",
      "Epoch: 6 itrerations: 500 Training Loss: 3.98448467255\n",
      "Epoch: 6 itrerations: 600 Training Loss: 4.26149225235\n",
      "Epoch: 6 itrerations: 700 Training Loss: 4.36018419266\n",
      "Epoch: 6 itrerations: 800 Training Loss: 4.57038831711\n",
      "Epoch: 6 itrerations: 900 Training Loss: 4.63281011581\n",
      "Epoch: 6 itrerations: 1000 Training Loss: 4.67809820175\n",
      "Epoch: 6 itrerations: 1100 Training Loss: 4.65067815781\n",
      "Epoch: 6 itrerations: 1200 Training Loss: 4.78888368607\n",
      "Epoch: 6 itrerations: 1300 Training Loss: 4.8012638092\n",
      "Epoch: 6 itrerations: 1400 Training Loss: 4.86875152588\n",
      "Epoch: 6 itrerations: 1500 Training Loss: 4.84352350235\n",
      "Epoch: 6 itrerations: 1600 Training Loss: 4.87262678146\n",
      "Epoch: 6 itrerations: 1700 Training Loss: 4.85834503174\n",
      "Epoch: 6 itrerations: 1800 Training Loss: 4.82707166672\n",
      "Epoch: 6 itrerations: 1900 Training Loss: 4.8162984848\n",
      "Epoch: 6 itrerations: 2000 Training Loss: 4.83952665329\n",
      "Epoch: 6 itrerations: 2100 Training Loss: 4.78630781174\n",
      "Epoch: 6 itrerations: 2200 Training Loss: 4.7540268898\n",
      "Epoch: 6 itrerations: 2300 Training Loss: 4.74812221527\n",
      "Epoch: 6 itrerations: 2400 Training Loss: 4.72890520096\n",
      "Epoch: 6 itrerations: 2500 Training Loss: 4.73616409302\n",
      "Epoch: 6 itrerations: 2600 Training Loss: 4.73349046707\n",
      "Epoch: 6 itrerations: 2700 Training Loss: 4.77934503555\n",
      "Epoch: 6 itrerations: 2800 Training Loss: 4.72863340378\n",
      "Epoch: 6 itrerations: 2900 Training Loss: 4.67907094955\n",
      "Epoch: 6 itrerations: 3000 Training Loss: 4.71840810776\n",
      "Epoch: 6 itrerations: 3100 Training Loss: 4.71044158936\n",
      "Epoch: 6 itrerations: 3200 Training Loss: 4.71196126938\n",
      "Epoch: 6 itrerations: 3300 Training Loss: 4.68113088608\n",
      "Epoch: 6 itrerations: 3400 Training Loss: 4.70297718048\n",
      "Epoch: 6 itrerations: 3500 Training Loss: 4.71386623383\n",
      "Epoch: 6 itrerations: 3600 Training Loss: 4.72283506393\n",
      "Epoch: 6 itrerations: 3700 Training Loss: 4.72716283798\n",
      "Epoch: 6 itrerations: 3800 Training Loss: 4.76370859146\n",
      "Epoch: 6 itrerations: 3900 Training Loss: 4.76505613327\n",
      "Epoch: 6 itrerations: 4000 Training Loss: 4.76751375198\n",
      "Epoch: 6 itrerations: 4100 Training Loss: 4.77590084076\n",
      "Epoch: 6 itrerations: 4200 Training Loss: 4.76955604553\n",
      "Epoch: 6 itrerations: 4300 Training Loss: 4.76596689224\n",
      "Epoch: 6 itrerations: 4400 Training Loss: 4.79358768463\n",
      "Epoch: 6 itrerations: 4500 Training Loss: 4.79095172882\n",
      "Epoch: 6 itrerations: 4600 Training Loss: 4.77181863785\n",
      "Epoch: 6 itrerations: 4700 Training Loss: 4.76519107819\n",
      "Epoch: 6 itrerations: 4800 Training Loss: 4.75595426559\n",
      "Epoch: 6 itrerations: 4900 Training Loss: 4.75293684006\n",
      "Epoch: 6 itrerations: 5000 Training Loss: 4.73347377777\n",
      "Epoch: 6 itrerations: 5100 Training Loss: 4.72489976883\n",
      "Epoch: 6 itrerations: 5200 Training Loss: 4.72552061081\n",
      "Epoch: 6 itrerations: 5300 Training Loss: 4.7207570076\n",
      "Epoch: 6 itrerations: 5400 Training Loss: 4.73098421097\n",
      "Epoch: 6 itrerations: 5500 Training Loss: 4.71244907379\n",
      "Epoch: 6 itrerations: 5600 Training Loss: 4.72570323944\n",
      "Epoch: 6 itrerations: 5700 Training Loss: 4.73221158981\n",
      "Epoch: 6 itrerations: 5800 Training Loss: 4.72604179382\n",
      "Epoch: 6 itrerations: 5900 Training Loss: 4.72753810883\n",
      "Epoch: 6 itrerations: 6000 Training Loss: 4.71528720856\n",
      "Epoch: 6 itrerations: 6100 Training Loss: 4.70036411285\n",
      "Epoch: 6 itrerations: 6200 Training Loss: 4.71777534485\n",
      "Epoch: 6 itrerations: 6300 Training Loss: 4.71674346924\n",
      "Epoch: 6 itrerations: 6400 Training Loss: 4.7136592865\n",
      "Epoch: 6 itrerations: 6500 Training Loss: 4.7015914917\n",
      "Epoch: 6 itrerations: 6600 Training Loss: 4.69182872772\n",
      "Epoch: 6 itrerations: 6700 Training Loss: 4.69688653946\n",
      "Epoch: 6 itrerations: 6800 Training Loss: 4.69999551773\n",
      "Epoch: 6 itrerations: 6900 Training Loss: 4.71488618851\n",
      "Epoch: 6 itrerations: 7000 Training Loss: 4.7310667038\n",
      "Epoch: 6 itrerations: 7100 Training Loss: 4.73976278305\n",
      "Epoch: 6 itrerations: 7200 Training Loss: 4.71831178665\n",
      "Epoch: 6 itrerations: 7300 Training Loss: 4.72543668747\n",
      "Epoch: 6 itrerations: 7400 Training Loss: 4.71434688568\n",
      "Epoch: 6 itrerations: 7500 Training Loss: 4.70613431931\n",
      "Epoch: 6 itrerations: 7600 Training Loss: 4.70220899582\n",
      "Epoch: 6 itrerations: 7700 Training Loss: 4.68871259689\n",
      "Epoch: 6 itrerations: 7800 Training Loss: 4.69552469254\n",
      "Epoch: 6 itrerations: 7900 Training Loss: 4.70645952225\n",
      "Epoch: 6 itrerations: 8000 Training Loss: 4.70384645462\n",
      "Epoch: 6 itrerations: 8100 Training Loss: 4.6941576004\n",
      "Epoch: 6 itrerations: 8200 Training Loss: 4.70354700089\n",
      "Epoch: 6 itrerations: 8300 Training Loss: 4.70801973343\n",
      "Epoch: 6 itrerations: 8400 Training Loss: 4.71135807037\n",
      "Epoch: 6 itrerations: 8800 Training Loss: 4.71416521072\n",
      "Epoch: 6 itrerations: 8900 Training Loss: 4.70540904999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 itrerations: 9000 Training Loss: 4.71532440186\n",
      "Epoch: 6 itrerations: 9100 Training Loss: 4.71370410919\n",
      "Epoch: 6 itrerations: 9200 Training Loss: 4.71219015121\n",
      "Epoch: 6 itrerations: 9300 Training Loss: 4.7131357193\n",
      "Epoch: 6 itrerations: 9400 Training Loss: 4.70750665665\n",
      "Epoch: 6 itrerations: 9500 Training Loss: 4.70880413055\n",
      "Epoch: 6 itrerations: 9600 Training Loss: 4.72171592712\n",
      "Epoch: 6 itrerations: 9700 Training Loss: 4.72525882721\n",
      "Epoch: 6 itrerations: 9800 Training Loss: 4.72764015198\n",
      "Epoch: 6 itrerations: 9900 Training Loss: 4.72478723526\n",
      "Epoch: 6 itrerations: 10000 Training Loss: 4.74413633347\n",
      "Epoch: 6 itrerations: 10100 Training Loss: 4.74409866333\n",
      "Epoch: 6 itrerations: 10200 Training Loss: 4.74100875854\n",
      "Epoch: 6 itrerations: 10300 Training Loss: 4.73836421967\n",
      "Epoch: 6 itrerations: 10400 Training Loss: 4.73341464996\n",
      "Epoch: 6 itrerations: 10500 Training Loss: 4.72648525238\n",
      "Epoch: 6 itrerations: 10600 Training Loss: 4.73091554642\n",
      "Epoch: 6 itrerations: 10700 Training Loss: 4.72628164291\n",
      "Epoch: 7 itrerations: 0 Training Loss: 2.94036722183\n",
      "Epoch: 7 itrerations: 100 Training Loss: 4.26550483704\n",
      "Epoch: 7 itrerations: 200 Training Loss: 3.97489762306\n",
      "Epoch: 7 itrerations: 300 Training Loss: 4.16751718521\n",
      "Epoch: 7 itrerations: 400 Training Loss: 4.32488155365\n",
      "Epoch: 7 itrerations: 500 Training Loss: 4.09086036682\n",
      "Epoch: 7 itrerations: 600 Training Loss: 4.25684261322\n",
      "Epoch: 7 itrerations: 700 Training Loss: 4.24592065811\n",
      "Epoch: 7 itrerations: 800 Training Loss: 4.48679113388\n",
      "Epoch: 7 itrerations: 900 Training Loss: 4.50458574295\n",
      "Epoch: 7 itrerations: 1000 Training Loss: 4.53906822205\n",
      "Epoch: 7 itrerations: 1100 Training Loss: 4.53309297562\n",
      "Epoch: 7 itrerations: 1200 Training Loss: 4.69915342331\n",
      "Epoch: 7 itrerations: 1300 Training Loss: 4.68398475647\n",
      "Epoch: 7 itrerations: 1400 Training Loss: 4.78200817108\n",
      "Epoch: 7 itrerations: 1500 Training Loss: 4.73315906525\n",
      "Epoch: 7 itrerations: 1600 Training Loss: 4.74496030807\n",
      "Epoch: 7 itrerations: 1700 Training Loss: 4.74107694626\n",
      "Epoch: 7 itrerations: 1800 Training Loss: 4.77426052094\n",
      "Epoch: 7 itrerations: 1900 Training Loss: 4.77792644501\n",
      "Epoch: 7 itrerations: 2000 Training Loss: 4.77496004105\n",
      "Epoch: 7 itrerations: 2100 Training Loss: 4.72893667221\n",
      "Epoch: 7 itrerations: 2200 Training Loss: 4.69982910156\n",
      "Epoch: 7 itrerations: 2300 Training Loss: 4.72733831406\n",
      "Epoch: 7 itrerations: 2400 Training Loss: 4.68365383148\n",
      "Epoch: 7 itrerations: 2500 Training Loss: 4.65612649918\n",
      "Epoch: 7 itrerations: 2600 Training Loss: 4.65695810318\n",
      "Epoch: 7 itrerations: 2700 Training Loss: 4.68564891815\n",
      "Epoch: 7 itrerations: 2800 Training Loss: 4.63930130005\n",
      "Epoch: 7 itrerations: 2900 Training Loss: 4.61865997314\n",
      "Epoch: 7 itrerations: 3000 Training Loss: 4.66354131699\n",
      "Epoch: 7 itrerations: 3100 Training Loss: 4.6356921196\n",
      "Epoch: 7 itrerations: 3200 Training Loss: 4.64880943298\n",
      "Epoch: 7 itrerations: 3300 Training Loss: 4.63455104828\n",
      "Epoch: 7 itrerations: 3400 Training Loss: 4.64135122299\n",
      "Epoch: 7 itrerations: 3500 Training Loss: 4.669090271\n",
      "Epoch: 7 itrerations: 3600 Training Loss: 4.68808078766\n",
      "Epoch: 7 itrerations: 3700 Training Loss: 4.67806911469\n",
      "Epoch: 7 itrerations: 3800 Training Loss: 4.69836902618\n",
      "Epoch: 7 itrerations: 3900 Training Loss: 4.68416118622\n",
      "Epoch: 7 itrerations: 4000 Training Loss: 4.68132591248\n",
      "Epoch: 7 itrerations: 4100 Training Loss: 4.67245531082\n",
      "Epoch: 7 itrerations: 4200 Training Loss: 4.64858675003\n",
      "Epoch: 7 itrerations: 4300 Training Loss: 4.6559138298\n",
      "Epoch: 7 itrerations: 4400 Training Loss: 4.67646837234\n",
      "Epoch: 7 itrerations: 4500 Training Loss: 4.67733001709\n",
      "Epoch: 7 itrerations: 4600 Training Loss: 4.66679620743\n",
      "Epoch: 7 itrerations: 4700 Training Loss: 4.6731595993\n",
      "Epoch: 7 itrerations: 4800 Training Loss: 4.65783166885\n",
      "Epoch: 7 itrerations: 4900 Training Loss: 4.65757608414\n",
      "Epoch: 7 itrerations: 5000 Training Loss: 4.63778114319\n",
      "Epoch: 7 itrerations: 5100 Training Loss: 4.63181638718\n",
      "Epoch: 7 itrerations: 5200 Training Loss: 4.64326810837\n",
      "Epoch: 7 itrerations: 5300 Training Loss: 4.64451599121\n",
      "Epoch: 7 itrerations: 5400 Training Loss: 4.66847229004\n",
      "Epoch: 7 itrerations: 5500 Training Loss: 4.66801929474\n",
      "Epoch: 7 itrerations: 5600 Training Loss: 4.66358184814\n",
      "Epoch: 7 itrerations: 5700 Training Loss: 4.65276432037\n",
      "Epoch: 7 itrerations: 5800 Training Loss: 4.66049909592\n",
      "Epoch: 7 itrerations: 5900 Training Loss: 4.6721329689\n",
      "Epoch: 7 itrerations: 6000 Training Loss: 4.66443824768\n",
      "Epoch: 7 itrerations: 6100 Training Loss: 4.6598200798\n",
      "Epoch: 7 itrerations: 6200 Training Loss: 4.66363143921\n",
      "Epoch: 7 itrerations: 6300 Training Loss: 4.66762876511\n",
      "Epoch: 7 itrerations: 6400 Training Loss: 4.6763625145\n",
      "Epoch: 7 itrerations: 6500 Training Loss: 4.66900730133\n",
      "Epoch: 7 itrerations: 6600 Training Loss: 4.66112041473\n",
      "Epoch: 7 itrerations: 6700 Training Loss: 4.66698408127\n",
      "Epoch: 7 itrerations: 6800 Training Loss: 4.67184734344\n",
      "Epoch: 7 itrerations: 6900 Training Loss: 4.68591785431\n",
      "Epoch: 7 itrerations: 7000 Training Loss: 4.71302890778\n",
      "Epoch: 7 itrerations: 7100 Training Loss: 4.72531080246\n",
      "Epoch: 7 itrerations: 7200 Training Loss: 4.70709133148\n",
      "Epoch: 7 itrerations: 7300 Training Loss: 4.70839929581\n",
      "Epoch: 7 itrerations: 7400 Training Loss: 4.69454908371\n",
      "Epoch: 7 itrerations: 7500 Training Loss: 4.69874048233\n",
      "Epoch: 7 itrerations: 7600 Training Loss: 4.68810367584\n",
      "Epoch: 7 itrerations: 7700 Training Loss: 4.68120193481\n",
      "Epoch: 7 itrerations: 7800 Training Loss: 4.67978811264\n",
      "Epoch: 7 itrerations: 7900 Training Loss: 4.69046545029\n",
      "Epoch: 7 itrerations: 8000 Training Loss: 4.68943405151\n",
      "Epoch: 7 itrerations: 8100 Training Loss: 4.68233680725\n",
      "Epoch: 7 itrerations: 8200 Training Loss: 4.68578624725\n",
      "Epoch: 7 itrerations: 8300 Training Loss: 4.69622135162\n",
      "Epoch: 7 itrerations: 8400 Training Loss: 4.6954870224\n",
      "Epoch: 7 itrerations: 8500 Training Loss: 4.7031917572\n",
      "Epoch: 7 itrerations: 8600 Training Loss: 4.70651626587\n",
      "Epoch: 7 itrerations: 8700 Training Loss: 4.7113494873\n",
      "Epoch: 7 itrerations: 8800 Training Loss: 4.71872615814\n",
      "Epoch: 7 itrerations: 8900 Training Loss: 4.71258544922\n",
      "Epoch: 7 itrerations: 9000 Training Loss: 4.72780370712\n",
      "Epoch: 7 itrerations: 9100 Training Loss: 4.72613048553\n",
      "Epoch: 7 itrerations: 9200 Training Loss: 4.73097705841\n",
      "Epoch: 7 itrerations: 9300 Training Loss: 4.72870206833\n",
      "Epoch: 7 itrerations: 9400 Training Loss: 4.71518421173\n",
      "Epoch: 7 itrerations: 9500 Training Loss: 4.70770311356\n",
      "Epoch: 7 itrerations: 9600 Training Loss: 4.72155714035\n",
      "Epoch: 7 itrerations: 9700 Training Loss: 4.72231864929\n",
      "Epoch: 7 itrerations: 9800 Training Loss: 4.71954488754\n",
      "Epoch: 7 itrerations: 9900 Training Loss: 4.71990299225\n",
      "Epoch: 7 itrerations: 10000 Training Loss: 4.74002552032\n",
      "Epoch: 7 itrerations: 10100 Training Loss: 4.7395157814\n",
      "Epoch: 7 itrerations: 10200 Training Loss: 4.73769712448\n",
      "Epoch: 7 itrerations: 10300 Training Loss: 4.73400259018\n",
      "Epoch: 7 itrerations: 10400 Training Loss: 4.73798179626\n",
      "Epoch: 7 itrerations: 10500 Training Loss: 4.7307062149\n",
      "Epoch: 7 itrerations: 10600 Training Loss: 4.73277282715\n",
      "Epoch: 7 itrerations: 10700 Training Loss: 4.7329325676\n",
      "Epoch: 8 itrerations: 0 Training Loss: 3.41509199142\n",
      "Epoch: 8 itrerations: 100 Training Loss: 4.10177326202\n",
      "Epoch: 8 itrerations: 200 Training Loss: 3.82427883148\n",
      "Epoch: 8 itrerations: 300 Training Loss: 4.16389703751\n",
      "Epoch: 8 itrerations: 400 Training Loss: 4.31950759888\n",
      "Epoch: 8 itrerations: 500 Training Loss: 3.99479317665\n",
      "Epoch: 8 itrerations: 600 Training Loss: 4.17990207672\n",
      "Epoch: 8 itrerations: 700 Training Loss: 4.31951999664\n",
      "Epoch: 8 itrerations: 800 Training Loss: 4.53213262558\n",
      "Epoch: 8 itrerations: 900 Training Loss: 4.63799858093\n",
      "Epoch: 8 itrerations: 1000 Training Loss: 4.69575357437\n",
      "Epoch: 8 itrerations: 1100 Training Loss: 4.63983249664\n",
      "Epoch: 8 itrerations: 1200 Training Loss: 4.76421403885\n",
      "Epoch: 8 itrerations: 1300 Training Loss: 4.76656627655\n",
      "Epoch: 8 itrerations: 1400 Training Loss: 4.84273576736\n",
      "Epoch: 8 itrerations: 1500 Training Loss: 4.83672094345\n",
      "Epoch: 8 itrerations: 1600 Training Loss: 4.83275365829\n",
      "Epoch: 8 itrerations: 1700 Training Loss: 4.84147405624\n",
      "Epoch: 8 itrerations: 1800 Training Loss: 4.85815763474\n",
      "Epoch: 8 itrerations: 1900 Training Loss: 4.8602938652\n",
      "Epoch: 8 itrerations: 2000 Training Loss: 4.88686132431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 itrerations: 2100 Training Loss: 4.86934137344\n",
      "Epoch: 8 itrerations: 2200 Training Loss: 4.80818128586\n",
      "Epoch: 8 itrerations: 2300 Training Loss: 4.83851766586\n",
      "Epoch: 8 itrerations: 2400 Training Loss: 4.81164741516\n",
      "Epoch: 8 itrerations: 2500 Training Loss: 4.81145906448\n",
      "Epoch: 8 itrerations: 2600 Training Loss: 4.81145000458\n",
      "Epoch: 8 itrerations: 2700 Training Loss: 4.84468412399\n",
      "Epoch: 8 itrerations: 2800 Training Loss: 4.79613780975\n",
      "Epoch: 8 itrerations: 2900 Training Loss: 4.75772428513\n",
      "Epoch: 8 itrerations: 3000 Training Loss: 4.7971777916\n",
      "Epoch: 8 itrerations: 3100 Training Loss: 4.78082561493\n",
      "Epoch: 8 itrerations: 3200 Training Loss: 4.79825210571\n",
      "Epoch: 8 itrerations: 3300 Training Loss: 4.76379728317\n",
      "Epoch: 8 itrerations: 3400 Training Loss: 4.74538040161\n",
      "Epoch: 8 itrerations: 3500 Training Loss: 4.77148103714\n",
      "Epoch: 8 itrerations: 3600 Training Loss: 4.78754472733\n",
      "Epoch: 8 itrerations: 3700 Training Loss: 4.77414321899\n",
      "Epoch: 8 itrerations: 3800 Training Loss: 4.8043627739\n",
      "Epoch: 8 itrerations: 3900 Training Loss: 4.7828335762\n",
      "Epoch: 8 itrerations: 4000 Training Loss: 4.77726984024\n",
      "Epoch: 8 itrerations: 4100 Training Loss: 4.77830076218\n",
      "Epoch: 8 itrerations: 4200 Training Loss: 4.76192855835\n",
      "Epoch: 8 itrerations: 4300 Training Loss: 4.75944948196\n",
      "Epoch: 8 itrerations: 4400 Training Loss: 4.77770423889\n",
      "Epoch: 8 itrerations: 4500 Training Loss: 4.79082345963\n",
      "Epoch: 8 itrerations: 4600 Training Loss: 4.80109024048\n",
      "Epoch: 8 itrerations: 4700 Training Loss: 4.80439853668\n",
      "Epoch: 8 itrerations: 4800 Training Loss: 4.79652833939\n",
      "Epoch: 8 itrerations: 4900 Training Loss: 4.77917480469\n",
      "Epoch: 8 itrerations: 5000 Training Loss: 4.76117229462\n",
      "Epoch: 8 itrerations: 5100 Training Loss: 4.75342559814\n",
      "Epoch: 8 itrerations: 5200 Training Loss: 4.75793409348\n",
      "Epoch: 8 itrerations: 5300 Training Loss: 4.74276351929\n",
      "Epoch: 8 itrerations: 5400 Training Loss: 4.76316165924\n",
      "Epoch: 8 itrerations: 5500 Training Loss: 4.75477600098\n",
      "Epoch: 8 itrerations: 5600 Training Loss: 4.76004505157\n",
      "Epoch: 8 itrerations: 5700 Training Loss: 4.75328636169\n",
      "Epoch: 8 itrerations: 5800 Training Loss: 4.76144313812\n",
      "Epoch: 8 itrerations: 5900 Training Loss: 4.76936626434\n",
      "Epoch: 8 itrerations: 6000 Training Loss: 4.77478551865\n",
      "Epoch: 8 itrerations: 6100 Training Loss: 4.75969982147\n",
      "Epoch: 8 itrerations: 6200 Training Loss: 4.76459932327\n",
      "Epoch: 8 itrerations: 6300 Training Loss: 4.76867055893\n",
      "Epoch: 8 itrerations: 6400 Training Loss: 4.76205396652\n",
      "Epoch: 8 itrerations: 6500 Training Loss: 4.76334857941\n",
      "Epoch: 8 itrerations: 6600 Training Loss: 4.75183820724\n",
      "Epoch: 8 itrerations: 6700 Training Loss: 4.7534403801\n",
      "Epoch: 8 itrerations: 6800 Training Loss: 4.75765562057\n",
      "Epoch: 8 itrerations: 6900 Training Loss: 4.77077436447\n",
      "Epoch: 8 itrerations: 7000 Training Loss: 4.79292011261\n",
      "Epoch: 8 itrerations: 7100 Training Loss: 4.79250812531\n",
      "Epoch: 8 itrerations: 7200 Training Loss: 4.78415966034\n",
      "Epoch: 8 itrerations: 7300 Training Loss: 4.78618955612\n",
      "Epoch: 8 itrerations: 7400 Training Loss: 4.78761339188\n",
      "Epoch: 8 itrerations: 7500 Training Loss: 4.77341365814\n",
      "Epoch: 8 itrerations: 7600 Training Loss: 4.76255130768\n",
      "Epoch: 8 itrerations: 7700 Training Loss: 4.75177240372\n",
      "Epoch: 8 itrerations: 7800 Training Loss: 4.75253152847\n",
      "Epoch: 8 itrerations: 7900 Training Loss: 4.75923061371\n",
      "Epoch: 8 itrerations: 8000 Training Loss: 4.75823020935\n",
      "Epoch: 8 itrerations: 8100 Training Loss: 4.74663639069\n",
      "Epoch: 8 itrerations: 8200 Training Loss: 4.75768661499\n",
      "Epoch: 8 itrerations: 8300 Training Loss: 4.76266813278\n",
      "Epoch: 8 itrerations: 8400 Training Loss: 4.75681018829\n",
      "Epoch: 8 itrerations: 8500 Training Loss: 4.76188373566\n",
      "Epoch: 8 itrerations: 8600 Training Loss: 4.75286626816\n",
      "Epoch: 8 itrerations: 8700 Training Loss: 4.75482654572\n",
      "Epoch: 8 itrerations: 8800 Training Loss: 4.7489066124\n",
      "Epoch: 8 itrerations: 8900 Training Loss: 4.74268054962\n",
      "Epoch: 8 itrerations: 9000 Training Loss: 4.74402189255\n",
      "Epoch: 8 itrerations: 9100 Training Loss: 4.74086666107\n",
      "Epoch: 8 itrerations: 9200 Training Loss: 4.73873329163\n",
      "Epoch: 8 itrerations: 9300 Training Loss: 4.7392449379\n",
      "Epoch: 8 itrerations: 9400 Training Loss: 4.72760105133\n",
      "Epoch: 8 itrerations: 9500 Training Loss: 4.72899913788\n",
      "Epoch: 8 itrerations: 9600 Training Loss: 4.73929595947\n",
      "Epoch: 8 itrerations: 9700 Training Loss: 4.72983407974\n",
      "Epoch: 8 itrerations: 9800 Training Loss: 4.72705173492\n",
      "Epoch: 8 itrerations: 9900 Training Loss: 4.73002958298\n",
      "Epoch: 8 itrerations: 10000 Training Loss: 4.74700737\n",
      "Epoch: 8 itrerations: 10100 Training Loss: 4.75003099442\n",
      "Epoch: 8 itrerations: 10200 Training Loss: 4.74932527542\n",
      "Epoch: 8 itrerations: 10300 Training Loss: 4.75447320938\n",
      "Epoch: 8 itrerations: 10400 Training Loss: 4.75115633011\n",
      "Epoch: 8 itrerations: 10500 Training Loss: 4.74449968338\n",
      "Epoch: 8 itrerations: 10600 Training Loss: 4.74481344223\n",
      "Epoch: 8 itrerations: 10700 Training Loss: 4.74909973145\n",
      "Epoch: 9 itrerations: 0 Training Loss: 0.216034591198\n",
      "Epoch: 9 itrerations: 100 Training Loss: 4.26100158691\n",
      "Epoch: 9 itrerations: 200 Training Loss: 3.94361042976\n",
      "Epoch: 9 itrerations: 300 Training Loss: 4.07583189011\n",
      "Epoch: 9 itrerations: 400 Training Loss: 4.28894042969\n",
      "Epoch: 9 itrerations: 500 Training Loss: 4.01709413528\n",
      "Epoch: 9 itrerations: 600 Training Loss: 4.33577823639\n",
      "Epoch: 9 itrerations: 700 Training Loss: 4.38122177124\n",
      "Epoch: 9 itrerations: 800 Training Loss: 4.62123250961\n",
      "Epoch: 9 itrerations: 900 Training Loss: 4.66258907318\n",
      "Epoch: 9 itrerations: 1000 Training Loss: 4.72786474228\n",
      "Epoch: 9 itrerations: 1100 Training Loss: 4.68144798279\n",
      "Epoch: 9 itrerations: 1200 Training Loss: 4.84847211838\n",
      "Epoch: 9 itrerations: 1300 Training Loss: 4.82764148712\n",
      "Epoch: 9 itrerations: 1400 Training Loss: 4.94691896439\n",
      "Epoch: 9 itrerations: 1500 Training Loss: 4.91904163361\n",
      "Epoch: 9 itrerations: 1600 Training Loss: 4.94263362885\n",
      "Epoch: 9 itrerations: 1700 Training Loss: 4.89667272568\n",
      "Epoch: 9 itrerations: 1800 Training Loss: 4.90544605255\n",
      "Epoch: 9 itrerations: 1900 Training Loss: 4.92479801178\n",
      "Epoch: 9 itrerations: 2000 Training Loss: 4.95138454437\n",
      "Epoch: 9 itrerations: 2100 Training Loss: 4.93104171753\n",
      "Epoch: 9 itrerations: 2200 Training Loss: 4.92067909241\n",
      "Epoch: 9 itrerations: 2300 Training Loss: 4.909968853\n",
      "Epoch: 9 itrerations: 2400 Training Loss: 4.86363887787\n",
      "Epoch: 9 itrerations: 2500 Training Loss: 4.82599306107\n",
      "Epoch: 9 itrerations: 2600 Training Loss: 4.78657674789\n",
      "Epoch: 9 itrerations: 2700 Training Loss: 4.80618667603\n",
      "Epoch: 9 itrerations: 2800 Training Loss: 4.76335096359\n",
      "Epoch: 9 itrerations: 2900 Training Loss: 4.71546936035\n",
      "Epoch: 9 itrerations: 3000 Training Loss: 4.75259542465\n",
      "Epoch: 9 itrerations: 3100 Training Loss: 4.74590682983\n",
      "Epoch: 9 itrerations: 3200 Training Loss: 4.75397634506\n",
      "Epoch: 9 itrerations: 3300 Training Loss: 4.72187185287\n",
      "Epoch: 9 itrerations: 3400 Training Loss: 4.73324632645\n",
      "Epoch: 9 itrerations: 3500 Training Loss: 4.75846624374\n",
      "Epoch: 9 itrerations: 3600 Training Loss: 4.76571321487\n",
      "Epoch: 9 itrerations: 3700 Training Loss: 4.7538356781\n",
      "Epoch: 9 itrerations: 3800 Training Loss: 4.77368736267\n",
      "Epoch: 9 itrerations: 3900 Training Loss: 4.74937772751\n",
      "Epoch: 9 itrerations: 4000 Training Loss: 4.74186563492\n",
      "Epoch: 9 itrerations: 4100 Training Loss: 4.74633407593\n",
      "Epoch: 9 itrerations: 4200 Training Loss: 4.73588895798\n",
      "Epoch: 9 itrerations: 4300 Training Loss: 4.72646665573\n",
      "Epoch: 9 itrerations: 4400 Training Loss: 4.7155251503\n",
      "Epoch: 9 itrerations: 4500 Training Loss: 4.71662664413\n",
      "Epoch: 9 itrerations: 4600 Training Loss: 4.72923660278\n",
      "Epoch: 9 itrerations: 4700 Training Loss: 4.75377750397\n",
      "Epoch: 9 itrerations: 4800 Training Loss: 4.74458599091\n",
      "Epoch: 9 itrerations: 4900 Training Loss: 4.74462652206\n",
      "Epoch: 9 itrerations: 5000 Training Loss: 4.71363162994\n",
      "Epoch: 9 itrerations: 5100 Training Loss: 4.70746946335\n",
      "Epoch: 9 itrerations: 5200 Training Loss: 4.71380519867\n",
      "Epoch: 9 itrerations: 5300 Training Loss: 4.71128845215\n",
      "Epoch: 9 itrerations: 5400 Training Loss: 4.72397613525\n",
      "Epoch: 9 itrerations: 5500 Training Loss: 4.71726894379\n",
      "Epoch: 9 itrerations: 5600 Training Loss: 4.71141433716\n",
      "Epoch: 9 itrerations: 5700 Training Loss: 4.72283315659\n",
      "Epoch: 9 itrerations: 5800 Training Loss: 4.72886657715\n",
      "Epoch: 9 itrerations: 5900 Training Loss: 4.73062896729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 itrerations: 6000 Training Loss: 4.72662210464\n",
      "Epoch: 9 itrerations: 6100 Training Loss: 4.71216440201\n",
      "Epoch: 9 itrerations: 6200 Training Loss: 4.72633361816\n",
      "Epoch: 9 itrerations: 6300 Training Loss: 4.72573852539\n",
      "Epoch: 9 itrerations: 6400 Training Loss: 4.72141027451\n",
      "Epoch: 9 itrerations: 6500 Training Loss: 4.72390985489\n",
      "Epoch: 9 itrerations: 6600 Training Loss: 4.71824359894\n",
      "Epoch: 9 itrerations: 6700 Training Loss: 4.72299861908\n",
      "Epoch: 9 itrerations: 6800 Training Loss: 4.7249622345\n",
      "Epoch: 9 itrerations: 6900 Training Loss: 4.73148918152\n",
      "Epoch: 9 itrerations: 7000 Training Loss: 4.74683141708\n",
      "Epoch: 9 itrerations: 7100 Training Loss: 4.7537856102\n",
      "Epoch: 9 itrerations: 7200 Training Loss: 4.74618387222\n",
      "Epoch: 9 itrerations: 7300 Training Loss: 4.7488732338\n",
      "Epoch: 9 itrerations: 7400 Training Loss: 4.74272727966\n",
      "Epoch: 9 itrerations: 7500 Training Loss: 4.7421207428\n",
      "Epoch: 9 itrerations: 7600 Training Loss: 4.73622608185\n",
      "Epoch: 9 itrerations: 7700 Training Loss: 4.72014951706\n",
      "Epoch: 9 itrerations: 7800 Training Loss: 4.71281337738\n",
      "Epoch: 9 itrerations: 7900 Training Loss: 4.73073625565\n",
      "Epoch: 9 itrerations: 8000 Training Loss: 4.72834205627\n",
      "Epoch: 9 itrerations: 8100 Training Loss: 4.7225279808\n",
      "Epoch: 9 itrerations: 8200 Training Loss: 4.73262500763\n",
      "Epoch: 9 itrerations: 8300 Training Loss: 4.73859977722\n",
      "Epoch: 9 itrerations: 8400 Training Loss: 4.73550367355\n",
      "Epoch: 9 itrerations: 8500 Training Loss: 4.73893737793\n",
      "Epoch: 9 itrerations: 8600 Training Loss: 4.73454284668\n",
      "Epoch: 9 itrerations: 8700 Training Loss: 4.72969341278\n",
      "Epoch: 9 itrerations: 8800 Training Loss: 4.72773838043\n",
      "Epoch: 9 itrerations: 8900 Training Loss: 4.72300195694\n",
      "Epoch: 9 itrerations: 9000 Training Loss: 4.72844743729\n",
      "Epoch: 9 itrerations: 9100 Training Loss: 4.73603439331\n",
      "Epoch: 9 itrerations: 9200 Training Loss: 4.73601007462\n",
      "Epoch: 9 itrerations: 9300 Training Loss: 4.72756814957\n",
      "Epoch: 9 itrerations: 9400 Training Loss: 4.71720218658\n",
      "Epoch: 9 itrerations: 9500 Training Loss: 4.71835184097\n",
      "Epoch: 9 itrerations: 9600 Training Loss: 4.72985506058\n",
      "Epoch: 9 itrerations: 9700 Training Loss: 4.72808837891\n",
      "Epoch: 9 itrerations: 9800 Training Loss: 4.72585773468\n",
      "Epoch: 9 itrerations: 9900 Training Loss: 4.72676753998\n",
      "Epoch: 9 itrerations: 10000 Training Loss: 4.74292373657\n",
      "Epoch: 9 itrerations: 10100 Training Loss: 4.74593734741\n",
      "Epoch: 9 itrerations: 10200 Training Loss: 4.74187660217\n",
      "Epoch: 9 itrerations: 10300 Training Loss: 4.74454069138\n",
      "Epoch: 9 itrerations: 10400 Training Loss: 4.74123716354\n",
      "Epoch: 9 itrerations: 10500 Training Loss: 4.72875452042\n",
      "Epoch: 9 itrerations: 10600 Training Loss: 4.72744464874\n",
      "Epoch: 9 itrerations: 10700 Training Loss: 4.73091745377\n",
      "Epoch: 10 itrerations: 0 Training Loss: 0.823503732681\n",
      "Epoch: 10 itrerations: 100 Training Loss: 4.65975475311\n",
      "Epoch: 10 itrerations: 200 Training Loss: 4.40608596802\n",
      "Epoch: 10 itrerations: 300 Training Loss: 4.40745592117\n",
      "Epoch: 10 itrerations: 400 Training Loss: 4.5331120491\n",
      "Epoch: 10 itrerations: 500 Training Loss: 4.28411626816\n",
      "Epoch: 10 itrerations: 600 Training Loss: 4.42424345016\n",
      "Epoch: 10 itrerations: 700 Training Loss: 4.39799308777\n",
      "Epoch: 10 itrerations: 800 Training Loss: 4.57907342911\n",
      "Epoch: 10 itrerations: 900 Training Loss: 4.6026301384\n",
      "Epoch: 10 itrerations: 1000 Training Loss: 4.61470842361\n",
      "Epoch: 10 itrerations: 1100 Training Loss: 4.5780749321\n",
      "Epoch: 10 itrerations: 1200 Training Loss: 4.65880060196\n",
      "Epoch: 10 itrerations: 1300 Training Loss: 4.62837791443\n",
      "Epoch: 10 itrerations: 1400 Training Loss: 4.75328731537\n",
      "Epoch: 10 itrerations: 1500 Training Loss: 4.73022556305\n",
      "Epoch: 10 itrerations: 1600 Training Loss: 4.71443748474\n",
      "Epoch: 10 itrerations: 1700 Training Loss: 4.75977230072\n",
      "Epoch: 10 itrerations: 1800 Training Loss: 4.78095245361\n",
      "Epoch: 10 itrerations: 1900 Training Loss: 4.81465053558\n",
      "Epoch: 10 itrerations: 2000 Training Loss: 4.81690073013\n",
      "Epoch: 10 itrerations: 2100 Training Loss: 4.7734746933\n",
      "Epoch: 10 itrerations: 2200 Training Loss: 4.75035619736\n",
      "Epoch: 10 itrerations: 2300 Training Loss: 4.76930713654\n",
      "Epoch: 10 itrerations: 2400 Training Loss: 4.73950910568\n",
      "Epoch: 10 itrerations: 2500 Training Loss: 4.70186090469\n",
      "Epoch: 10 itrerations: 2600 Training Loss: 4.71179485321\n",
      "Epoch: 10 itrerations: 2700 Training Loss: 4.7333316803\n",
      "Epoch: 10 itrerations: 2800 Training Loss: 4.70370292664\n",
      "Epoch: 10 itrerations: 2900 Training Loss: 4.68023014069\n",
      "Epoch: 10 itrerations: 3000 Training Loss: 4.71593952179\n",
      "Epoch: 10 itrerations: 3100 Training Loss: 4.7191362381\n",
      "Epoch: 10 itrerations: 3200 Training Loss: 4.70749998093\n",
      "Epoch: 10 itrerations: 3300 Training Loss: 4.67030858994\n",
      "Epoch: 10 itrerations: 3400 Training Loss: 4.67239809036\n",
      "Epoch: 10 itrerations: 3500 Training Loss: 4.71353054047\n",
      "Epoch: 10 itrerations: 3600 Training Loss: 4.71419525146\n",
      "Epoch: 10 itrerations: 3700 Training Loss: 4.70491504669\n",
      "Epoch: 10 itrerations: 3800 Training Loss: 4.72784614563\n",
      "Epoch: 10 itrerations: 3900 Training Loss: 4.71692943573\n",
      "Epoch: 10 itrerations: 4000 Training Loss: 4.72280359268\n",
      "Epoch: 10 itrerations: 4100 Training Loss: 4.72595596313\n",
      "Epoch: 10 itrerations: 4200 Training Loss: 4.70739030838\n",
      "Epoch: 10 itrerations: 4300 Training Loss: 4.71350574493\n",
      "Epoch: 10 itrerations: 4400 Training Loss: 4.74411249161\n",
      "Epoch: 10 itrerations: 4500 Training Loss: 4.73620605469\n",
      "Epoch: 10 itrerations: 4600 Training Loss: 4.72823762894\n",
      "Epoch: 10 itrerations: 4700 Training Loss: 4.73742103577\n",
      "Epoch: 10 itrerations: 4800 Training Loss: 4.71400547028\n",
      "Epoch: 10 itrerations: 4900 Training Loss: 4.70663166046\n",
      "Epoch: 10 itrerations: 5000 Training Loss: 4.68246507645\n",
      "Epoch: 10 itrerations: 5100 Training Loss: 4.67232513428\n",
      "Epoch: 10 itrerations: 5200 Training Loss: 4.67985248566\n",
      "Epoch: 10 itrerations: 5300 Training Loss: 4.67932033539\n",
      "Epoch: 10 itrerations: 5400 Training Loss: 4.69229602814\n",
      "Epoch: 10 itrerations: 5500 Training Loss: 4.67988300323\n",
      "Epoch: 10 itrerations: 5600 Training Loss: 4.67634344101\n",
      "Epoch: 10 itrerations: 5700 Training Loss: 4.68028688431\n",
      "Epoch: 10 itrerations: 5800 Training Loss: 4.67949438095\n",
      "Epoch: 10 itrerations: 5900 Training Loss: 4.68265008926\n",
      "Epoch: 10 itrerations: 6000 Training Loss: 4.68113803864\n",
      "Epoch: 10 itrerations: 6100 Training Loss: 4.68040895462\n",
      "Epoch: 10 itrerations: 6200 Training Loss: 4.69264268875\n",
      "Epoch: 10 itrerations: 6300 Training Loss: 4.68416213989\n",
      "Epoch: 10 itrerations: 6400 Training Loss: 4.6773557663\n",
      "Epoch: 10 itrerations: 6500 Training Loss: 4.66795539856\n",
      "Epoch: 10 itrerations: 6600 Training Loss: 4.6490945816\n",
      "Epoch: 10 itrerations: 6700 Training Loss: 4.66227674484\n",
      "Epoch: 10 itrerations: 6800 Training Loss: 4.66735458374\n",
      "Epoch: 10 itrerations: 6900 Training Loss: 4.68549203873\n",
      "Epoch: 10 itrerations: 7000 Training Loss: 4.70851278305\n",
      "Epoch: 10 itrerations: 7100 Training Loss: 4.72116470337\n",
      "Epoch: 10 itrerations: 7200 Training Loss: 4.71114587784\n",
      "Epoch: 10 itrerations: 7300 Training Loss: 4.69686031342\n",
      "Epoch: 10 itrerations: 7400 Training Loss: 4.69024753571\n",
      "Epoch: 10 itrerations: 7500 Training Loss: 4.68963623047\n",
      "Epoch: 10 itrerations: 7600 Training Loss: 4.67713117599\n",
      "Epoch: 10 itrerations: 7700 Training Loss: 4.66400051117\n",
      "Epoch: 10 itrerations: 7800 Training Loss: 4.66475629807\n",
      "Epoch: 10 itrerations: 7900 Training Loss: 4.66386270523\n",
      "Epoch: 10 itrerations: 8000 Training Loss: 4.66343593597\n",
      "Epoch: 10 itrerations: 8100 Training Loss: 4.66183137894\n",
      "Epoch: 10 itrerations: 8200 Training Loss: 4.67165470123\n",
      "Epoch: 10 itrerations: 8300 Training Loss: 4.68048048019\n",
      "Epoch: 10 itrerations: 8400 Training Loss: 4.68721437454\n",
      "Epoch: 10 itrerations: 8500 Training Loss: 4.69393920898\n",
      "Epoch: 10 itrerations: 8600 Training Loss: 4.68573093414\n",
      "Epoch: 10 itrerations: 8700 Training Loss: 4.68146038055\n",
      "Epoch: 10 itrerations: 8800 Training Loss: 4.68857860565\n",
      "Epoch: 10 itrerations: 8900 Training Loss: 4.69173192978\n",
      "Epoch: 10 itrerations: 9000 Training Loss: 4.70011138916\n",
      "Epoch: 10 itrerations: 9100 Training Loss: 4.70124769211\n",
      "Epoch: 10 itrerations: 9200 Training Loss: 4.70157670975\n",
      "Epoch: 10 itrerations: 9300 Training Loss: 4.6953868866\n",
      "Epoch: 10 itrerations: 9400 Training Loss: 4.68663835526\n",
      "Epoch: 10 itrerations: 9500 Training Loss: 4.68865537643\n",
      "Epoch: 10 itrerations: 9600 Training Loss: 4.70804452896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 itrerations: 9700 Training Loss: 4.70845556259\n",
      "Epoch: 10 itrerations: 9800 Training Loss: 4.71407794952\n",
      "Epoch: 10 itrerations: 9900 Training Loss: 4.71145105362\n",
      "Epoch: 10 itrerations: 10000 Training Loss: 4.72563743591\n",
      "Epoch: 10 itrerations: 10100 Training Loss: 4.7270526886\n",
      "Epoch: 10 itrerations: 10200 Training Loss: 4.72786617279\n",
      "Epoch: 10 itrerations: 10300 Training Loss: 4.72413635254\n",
      "Epoch: 10 itrerations: 10400 Training Loss: 4.72186660767\n",
      "Epoch: 10 itrerations: 10500 Training Loss: 4.71638441086\n",
      "Epoch: 10 itrerations: 10600 Training Loss: 4.71657180786\n",
      "Epoch: 10 itrerations: 10700 Training Loss: 4.71555280685\n",
      "Epoch: 11 itrerations: 0 Training Loss: 0.891181886196\n",
      "Epoch: 11 itrerations: 100 Training Loss: 4.23420476913\n",
      "Epoch: 11 itrerations: 200 Training Loss: 4.36782026291\n",
      "Epoch: 11 itrerations: 300 Training Loss: 4.45227813721\n",
      "Epoch: 11 itrerations: 400 Training Loss: 4.3619260788\n",
      "Epoch: 11 itrerations: 500 Training Loss: 4.05242681503\n",
      "Epoch: 11 itrerations: 600 Training Loss: 4.3185749054\n",
      "Epoch: 11 itrerations: 700 Training Loss: 4.33848810196\n",
      "Epoch: 11 itrerations: 800 Training Loss: 4.50213432312\n",
      "Epoch: 11 itrerations: 900 Training Loss: 4.52442312241\n",
      "Epoch: 11 itrerations: 1000 Training Loss: 4.60366535187\n",
      "Epoch: 11 itrerations: 1100 Training Loss: 4.57181167603\n",
      "Epoch: 11 itrerations: 1200 Training Loss: 4.75212478638\n",
      "Epoch: 11 itrerations: 1300 Training Loss: 4.76607179642\n",
      "Epoch: 11 itrerations: 1400 Training Loss: 4.81069469452\n",
      "Epoch: 11 itrerations: 1500 Training Loss: 4.75252389908\n",
      "Epoch: 11 itrerations: 1600 Training Loss: 4.76020240784\n",
      "Epoch: 11 itrerations: 1700 Training Loss: 4.77014923096\n",
      "Epoch: 11 itrerations: 1800 Training Loss: 4.74002790451\n",
      "Epoch: 11 itrerations: 1900 Training Loss: 4.74968338013\n",
      "Epoch: 11 itrerations: 2000 Training Loss: 4.75049209595\n",
      "Epoch: 11 itrerations: 2100 Training Loss: 4.7259144783\n",
      "Epoch: 11 itrerations: 2200 Training Loss: 4.66458702087\n",
      "Epoch: 11 itrerations: 2300 Training Loss: 4.65361166\n",
      "Epoch: 11 itrerations: 2400 Training Loss: 4.60627508163\n",
      "Epoch: 11 itrerations: 2500 Training Loss: 4.58455228806\n",
      "Epoch: 11 itrerations: 2600 Training Loss: 4.5882897377\n",
      "Epoch: 11 itrerations: 2700 Training Loss: 4.61421060562\n",
      "Epoch: 11 itrerations: 2800 Training Loss: 4.56375360489\n",
      "Epoch: 11 itrerations: 2900 Training Loss: 4.55018520355\n",
      "Epoch: 11 itrerations: 3000 Training Loss: 4.601375103\n",
      "Epoch: 11 itrerations: 3100 Training Loss: 4.58153533936\n",
      "Epoch: 11 itrerations: 3200 Training Loss: 4.58947992325\n",
      "Epoch: 11 itrerations: 3300 Training Loss: 4.55751991272\n",
      "Epoch: 11 itrerations: 3400 Training Loss: 4.58917427063\n",
      "Epoch: 11 itrerations: 3500 Training Loss: 4.62051486969\n",
      "Epoch: 11 itrerations: 3600 Training Loss: 4.62335920334\n",
      "Epoch: 11 itrerations: 3700 Training Loss: 4.62609958649\n",
      "Epoch: 11 itrerations: 3800 Training Loss: 4.65214681625\n",
      "Epoch: 11 itrerations: 3900 Training Loss: 4.62689495087\n",
      "Epoch: 11 itrerations: 4000 Training Loss: 4.63018226624\n",
      "Epoch: 11 itrerations: 4100 Training Loss: 4.62600278854\n",
      "Epoch: 11 itrerations: 4200 Training Loss: 4.60945796967\n",
      "Epoch: 11 itrerations: 4300 Training Loss: 4.62461614609\n",
      "Epoch: 11 itrerations: 4400 Training Loss: 4.62337684631\n",
      "Epoch: 11 itrerations: 4500 Training Loss: 4.61765766144\n",
      "Epoch: 11 itrerations: 4600 Training Loss: 4.59996175766\n",
      "Epoch: 11 itrerations: 4700 Training Loss: 4.60773468018\n",
      "Epoch: 11 itrerations: 4800 Training Loss: 4.60614490509\n",
      "Epoch: 11 itrerations: 4900 Training Loss: 4.59903812408\n",
      "Epoch: 11 itrerations: 5000 Training Loss: 4.57455015182\n",
      "Epoch: 11 itrerations: 5100 Training Loss: 4.55899524689\n",
      "Epoch: 11 itrerations: 5200 Training Loss: 4.55597114563\n",
      "Epoch: 11 itrerations: 5300 Training Loss: 4.55336093903\n",
      "Epoch: 11 itrerations: 5400 Training Loss: 4.56526708603\n",
      "Epoch: 11 itrerations: 5500 Training Loss: 4.565762043\n",
      "Epoch: 11 itrerations: 5600 Training Loss: 4.5668463707\n",
      "Epoch: 11 itrerations: 5700 Training Loss: 4.5713224411\n",
      "Epoch: 11 itrerations: 5800 Training Loss: 4.57766866684\n",
      "Epoch: 11 itrerations: 5900 Training Loss: 4.57770872116\n",
      "Epoch: 11 itrerations: 6000 Training Loss: 4.5810174942\n",
      "Epoch: 11 itrerations: 6100 Training Loss: 4.56865501404\n",
      "Epoch: 11 itrerations: 6200 Training Loss: 4.57315301895\n",
      "Epoch: 11 itrerations: 6300 Training Loss: 4.57753276825\n",
      "Epoch: 11 itrerations: 6400 Training Loss: 4.57946157455\n",
      "Epoch: 11 itrerations: 6500 Training Loss: 4.58077001572\n",
      "Epoch: 11 itrerations: 6600 Training Loss: 4.57091665268\n",
      "Epoch: 11 itrerations: 6700 Training Loss: 4.57934761047\n",
      "Epoch: 11 itrerations: 6800 Training Loss: 4.59102392197\n",
      "Epoch: 11 itrerations: 6900 Training Loss: 4.5974111557\n",
      "Epoch: 11 itrerations: 7000 Training Loss: 4.61266851425\n",
      "Epoch: 11 itrerations: 7100 Training Loss: 4.62086772919\n",
      "Epoch: 11 itrerations: 7200 Training Loss: 4.62150764465\n",
      "Epoch: 11 itrerations: 7300 Training Loss: 4.6214299202\n",
      "Epoch: 11 itrerations: 7400 Training Loss: 4.61086130142\n",
      "Epoch: 11 itrerations: 7500 Training Loss: 4.61593341827\n",
      "Epoch: 11 itrerations: 7600 Training Loss: 4.6063952446\n",
      "Epoch: 11 itrerations: 7700 Training Loss: 4.59563064575\n",
      "Epoch: 11 itrerations: 7800 Training Loss: 4.60261201859\n",
      "Epoch: 11 itrerations: 7900 Training Loss: 4.61607599258\n",
      "Epoch: 11 itrerations: 8000 Training Loss: 4.61765813828\n",
      "Epoch: 11 itrerations: 8100 Training Loss: 4.60322904587\n",
      "Epoch: 11 itrerations: 8200 Training Loss: 4.61370611191\n",
      "Epoch: 11 itrerations: 8300 Training Loss: 4.62067699432\n",
      "Epoch: 11 itrerations: 8400 Training Loss: 4.62636184692\n",
      "Epoch: 11 itrerations: 8500 Training Loss: 4.63145828247\n",
      "Epoch: 11 itrerations: 8600 Training Loss: 4.62606573105\n",
      "Epoch: 11 itrerations: 8700 Training Loss: 4.62229442596\n",
      "Epoch: 11 itrerations: 8800 Training Loss: 4.6168627739\n",
      "Epoch: 11 itrerations: 8900 Training Loss: 4.62043857574\n",
      "Epoch: 11 itrerations: 9000 Training Loss: 4.62511205673\n",
      "Epoch: 11 itrerations: 9100 Training Loss: 4.62416315079\n",
      "Epoch: 11 itrerations: 9200 Training Loss: 4.62687778473\n",
      "Epoch: 11 itrerations: 9300 Training Loss: 4.62276315689\n",
      "Epoch: 11 itrerations: 9400 Training Loss: 4.61237430573\n",
      "Epoch: 11 itrerations: 9500 Training Loss: 4.6151175499\n",
      "Epoch: 11 itrerations: 9600 Training Loss: 4.62774419785\n",
      "Epoch: 11 itrerations: 9700 Training Loss: 4.6276102066\n",
      "Epoch: 11 itrerations: 9800 Training Loss: 4.62399959564\n",
      "Epoch: 11 itrerations: 9900 Training Loss: 4.62920999527\n",
      "Epoch: 11 itrerations: 10000 Training Loss: 4.64715719223\n",
      "Epoch: 11 itrerations: 10100 Training Loss: 4.64562892914\n",
      "Epoch: 11 itrerations: 10200 Training Loss: 4.64274930954\n",
      "Epoch: 11 itrerations: 10300 Training Loss: 4.64246511459\n",
      "Epoch: 11 itrerations: 10400 Training Loss: 4.64255046844\n",
      "Epoch: 11 itrerations: 10500 Training Loss: 4.63733053207\n",
      "Epoch: 11 itrerations: 10600 Training Loss: 4.63454151154\n",
      "Epoch: 11 itrerations: 10700 Training Loss: 4.63658428192\n",
      "Epoch: 12 itrerations: 0 Training Loss: 1.14528417587\n",
      "Epoch: 12 itrerations: 100 Training Loss: 4.74226284027\n",
      "Epoch: 12 itrerations: 200 Training Loss: 4.28213977814\n",
      "Epoch: 12 itrerations: 300 Training Loss: 4.14935016632\n",
      "Epoch: 12 itrerations: 400 Training Loss: 4.19292926788\n",
      "Epoch: 12 itrerations: 500 Training Loss: 4.00664758682\n",
      "Epoch: 12 itrerations: 600 Training Loss: 4.21972370148\n",
      "Epoch: 12 itrerations: 700 Training Loss: 4.24016284943\n",
      "Epoch: 12 itrerations: 800 Training Loss: 4.40412664413\n",
      "Epoch: 12 itrerations: 900 Training Loss: 4.45272445679\n",
      "Epoch: 12 itrerations: 1000 Training Loss: 4.46988344193\n",
      "Epoch: 12 itrerations: 1100 Training Loss: 4.48935651779\n",
      "Epoch: 12 itrerations: 1200 Training Loss: 4.59296941757\n",
      "Epoch: 12 itrerations: 1300 Training Loss: 4.55150127411\n",
      "Epoch: 12 itrerations: 1400 Training Loss: 4.6540760994\n",
      "Epoch: 12 itrerations: 1500 Training Loss: 4.65201663971\n",
      "Epoch: 12 itrerations: 1600 Training Loss: 4.63449144363\n",
      "Epoch: 12 itrerations: 1700 Training Loss: 4.65106678009\n",
      "Epoch: 12 itrerations: 1800 Training Loss: 4.63540315628\n",
      "Epoch: 12 itrerations: 1900 Training Loss: 4.69825077057\n",
      "Epoch: 12 itrerations: 2000 Training Loss: 4.72325372696\n",
      "Epoch: 12 itrerations: 2100 Training Loss: 4.69476795197\n",
      "Epoch: 12 itrerations: 2200 Training Loss: 4.69818401337\n",
      "Epoch: 12 itrerations: 2300 Training Loss: 4.70882749557\n",
      "Epoch: 12 itrerations: 2400 Training Loss: 4.70318984985\n",
      "Epoch: 12 itrerations: 2500 Training Loss: 4.66475200653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 itrerations: 2600 Training Loss: 4.66730642319\n",
      "Epoch: 12 itrerations: 2700 Training Loss: 4.71454572678\n",
      "Epoch: 12 itrerations: 2800 Training Loss: 4.6689825058\n",
      "Epoch: 12 itrerations: 2900 Training Loss: 4.64289236069\n",
      "Epoch: 12 itrerations: 3000 Training Loss: 4.69861555099\n",
      "Epoch: 12 itrerations: 3100 Training Loss: 4.65718412399\n",
      "Epoch: 12 itrerations: 3200 Training Loss: 4.66915893555\n",
      "Epoch: 12 itrerations: 3300 Training Loss: 4.62739849091\n",
      "Epoch: 12 itrerations: 3400 Training Loss: 4.63527107239\n",
      "Epoch: 12 itrerations: 3500 Training Loss: 4.65435218811\n",
      "Epoch: 12 itrerations: 3600 Training Loss: 4.64407110214\n",
      "Epoch: 12 itrerations: 3700 Training Loss: 4.64540863037\n",
      "Epoch: 12 itrerations: 3800 Training Loss: 4.65968322754\n",
      "Epoch: 12 itrerations: 3900 Training Loss: 4.64434623718\n",
      "Epoch: 12 itrerations: 4000 Training Loss: 4.65336799622\n",
      "Epoch: 12 itrerations: 4100 Training Loss: 4.63994693756\n",
      "Epoch: 12 itrerations: 4200 Training Loss: 4.62846469879\n",
      "Epoch: 12 itrerations: 4300 Training Loss: 4.64315462112\n",
      "Epoch: 12 itrerations: 4400 Training Loss: 4.66579389572\n",
      "Epoch: 12 itrerations: 4500 Training Loss: 4.6605052948\n",
      "Epoch: 12 itrerations: 4600 Training Loss: 4.66364812851\n",
      "Epoch: 12 itrerations: 4700 Training Loss: 4.67643642426\n",
      "Epoch: 12 itrerations: 4800 Training Loss: 4.66479969025\n",
      "Epoch: 12 itrerations: 4900 Training Loss: 4.66087341309\n",
      "Epoch: 12 itrerations: 5000 Training Loss: 4.63859033585\n",
      "Epoch: 12 itrerations: 5100 Training Loss: 4.62385463715\n",
      "Epoch: 12 itrerations: 5200 Training Loss: 4.62200832367\n",
      "Epoch: 12 itrerations: 5300 Training Loss: 4.60844755173\n",
      "Epoch: 12 itrerations: 5400 Training Loss: 4.61899471283\n",
      "Epoch: 12 itrerations: 5500 Training Loss: 4.6120390892\n",
      "Epoch: 12 itrerations: 5600 Training Loss: 4.61249303818\n",
      "Epoch: 12 itrerations: 5700 Training Loss: 4.61425352097\n",
      "Epoch: 12 itrerations: 5800 Training Loss: 4.61625862122\n",
      "Epoch: 12 itrerations: 5900 Training Loss: 4.61349058151\n",
      "Epoch: 12 itrerations: 6000 Training Loss: 4.60753726959\n",
      "Epoch: 12 itrerations: 6100 Training Loss: 4.59438228607\n",
      "Epoch: 12 itrerations: 6200 Training Loss: 4.58537340164\n",
      "Epoch: 12 itrerations: 6300 Training Loss: 4.58447885513\n",
      "Epoch: 12 itrerations: 6400 Training Loss: 4.56818294525\n",
      "Epoch: 12 itrerations: 6500 Training Loss: 4.55345630646\n",
      "Epoch: 12 itrerations: 6600 Training Loss: 4.54100513458\n",
      "Epoch: 12 itrerations: 6700 Training Loss: 4.54213523865\n",
      "Epoch: 12 itrerations: 6800 Training Loss: 4.55595731735\n",
      "Epoch: 12 itrerations: 6900 Training Loss: 4.57379102707\n",
      "Epoch: 12 itrerations: 7000 Training Loss: 4.58913755417\n",
      "Epoch: 12 itrerations: 7100 Training Loss: 4.59910345078\n",
      "Epoch: 12 itrerations: 7200 Training Loss: 4.58411598206\n",
      "Epoch: 12 itrerations: 7300 Training Loss: 4.59337902069\n",
      "Epoch: 12 itrerations: 7400 Training Loss: 4.59010839462\n",
      "Epoch: 12 itrerations: 7500 Training Loss: 4.58666276932\n",
      "Epoch: 12 itrerations: 7600 Training Loss: 4.57797431946\n",
      "Epoch: 12 itrerations: 7700 Training Loss: 4.56870222092\n",
      "Epoch: 12 itrerations: 7800 Training Loss: 4.57087039948\n",
      "Epoch: 12 itrerations: 7900 Training Loss: 4.58250331879\n",
      "Epoch: 12 itrerations: 8000 Training Loss: 4.57671976089\n",
      "Epoch: 12 itrerations: 8100 Training Loss: 4.57307100296\n",
      "Epoch: 12 itrerations: 8200 Training Loss: 4.57934045792\n",
      "Epoch: 12 itrerations: 8300 Training Loss: 4.57738924026\n",
      "Epoch: 12 itrerations: 8400 Training Loss: 4.57711362839\n",
      "Epoch: 12 itrerations: 8500 Training Loss: 4.57951784134\n",
      "Epoch: 12 itrerations: 8600 Training Loss: 4.575050354\n",
      "Epoch: 12 itrerations: 8700 Training Loss: 4.5726852417\n",
      "Epoch: 12 itrerations: 8800 Training Loss: 4.57311868668\n",
      "Epoch: 12 itrerations: 8900 Training Loss: 4.5667014122\n",
      "Epoch: 12 itrerations: 9000 Training Loss: 4.57875871658\n",
      "Epoch: 12 itrerations: 9100 Training Loss: 4.57647752762\n",
      "Epoch: 12 itrerations: 9200 Training Loss: 4.58183288574\n",
      "Epoch: 12 itrerations: 9300 Training Loss: 4.58766222\n",
      "Epoch: 12 itrerations: 9400 Training Loss: 4.58204936981\n",
      "Epoch: 12 itrerations: 9500 Training Loss: 4.58447360992\n",
      "Epoch: 12 itrerations: 9600 Training Loss: 4.60562086105\n",
      "Epoch: 12 itrerations: 9700 Training Loss: 4.61215734482\n",
      "Epoch: 12 itrerations: 9800 Training Loss: 4.60779571533\n",
      "Epoch: 12 itrerations: 9900 Training Loss: 4.60660123825\n",
      "Epoch: 12 itrerations: 10000 Training Loss: 4.62130880356\n",
      "Epoch: 12 itrerations: 10100 Training Loss: 4.61738348007\n",
      "Epoch: 12 itrerations: 10200 Training Loss: 4.61166572571\n",
      "Epoch: 12 itrerations: 10300 Training Loss: 4.62002801895\n",
      "Epoch: 12 itrerations: 10400 Training Loss: 4.6109919548\n",
      "Epoch: 12 itrerations: 10500 Training Loss: 4.60410261154\n",
      "Epoch: 12 itrerations: 10600 Training Loss: 4.60110807419\n",
      "Epoch: 12 itrerations: 10700 Training Loss: 4.59658193588\n",
      "Epoch: 13 itrerations: 0 Training Loss: 0.778533637524\n",
      "Epoch: 13 itrerations: 100 Training Loss: 4.38602209091\n",
      "Epoch: 13 itrerations: 200 Training Loss: 4.06542158127\n",
      "Epoch: 13 itrerations: 300 Training Loss: 4.15840005875\n",
      "Epoch: 13 itrerations: 400 Training Loss: 4.20772075653\n",
      "Epoch: 13 itrerations: 500 Training Loss: 3.90841579437\n",
      "Epoch: 13 itrerations: 600 Training Loss: 4.2051358223\n",
      "Epoch: 13 itrerations: 700 Training Loss: 4.31060123444\n",
      "Epoch: 13 itrerations: 800 Training Loss: 4.56693840027\n",
      "Epoch: 13 itrerations: 900 Training Loss: 4.6415014267\n",
      "Epoch: 13 itrerations: 1000 Training Loss: 4.72070121765\n",
      "Epoch: 13 itrerations: 1100 Training Loss: 4.67561912537\n",
      "Epoch: 13 itrerations: 1200 Training Loss: 4.73764801025\n",
      "Epoch: 13 itrerations: 1300 Training Loss: 4.71081161499\n",
      "Epoch: 13 itrerations: 1400 Training Loss: 4.8472495079\n",
      "Epoch: 13 itrerations: 1500 Training Loss: 4.83987665176\n",
      "Epoch: 13 itrerations: 1600 Training Loss: 4.83100175858\n",
      "Epoch: 13 itrerations: 1700 Training Loss: 4.82306575775\n",
      "Epoch: 13 itrerations: 1800 Training Loss: 4.81727075577\n",
      "Epoch: 13 itrerations: 1900 Training Loss: 4.83396911621\n",
      "Epoch: 13 itrerations: 2000 Training Loss: 4.8763666153\n",
      "Epoch: 13 itrerations: 2100 Training Loss: 4.83140611649\n",
      "Epoch: 13 itrerations: 2200 Training Loss: 4.81381845474\n",
      "Epoch: 13 itrerations: 2300 Training Loss: 4.82443189621\n",
      "Epoch: 13 itrerations: 2400 Training Loss: 4.79061937332\n",
      "Epoch: 13 itrerations: 2500 Training Loss: 4.74229383469\n",
      "Epoch: 13 itrerations: 2600 Training Loss: 4.71993780136\n",
      "Epoch: 13 itrerations: 2700 Training Loss: 4.76187372208\n",
      "Epoch: 13 itrerations: 2800 Training Loss: 4.69962882996\n",
      "Epoch: 13 itrerations: 2900 Training Loss: 4.66265058517\n",
      "Epoch: 13 itrerations: 3000 Training Loss: 4.7104973793\n",
      "Epoch: 13 itrerations: 3100 Training Loss: 4.67062425613\n",
      "Epoch: 13 itrerations: 3200 Training Loss: 4.67099142075\n",
      "Epoch: 13 itrerations: 3300 Training Loss: 4.6452870369\n",
      "Epoch: 13 itrerations: 3400 Training Loss: 4.65339708328\n",
      "Epoch: 13 itrerations: 3500 Training Loss: 4.68979883194\n",
      "Epoch: 13 itrerations: 3600 Training Loss: 4.71101999283\n",
      "Epoch: 13 itrerations: 3700 Training Loss: 4.69616937637\n",
      "Epoch: 13 itrerations: 3800 Training Loss: 4.72797870636\n",
      "Epoch: 13 itrerations: 3900 Training Loss: 4.71371746063\n",
      "Epoch: 13 itrerations: 4000 Training Loss: 4.71959924698\n",
      "Epoch: 13 itrerations: 4100 Training Loss: 4.70742464066\n",
      "Epoch: 13 itrerations: 4200 Training Loss: 4.68809223175\n",
      "Epoch: 13 itrerations: 4300 Training Loss: 4.68834781647\n",
      "Epoch: 13 itrerations: 4400 Training Loss: 4.69910001755\n",
      "Epoch: 13 itrerations: 4500 Training Loss: 4.68916559219\n",
      "Epoch: 13 itrerations: 4600 Training Loss: 4.67823934555\n",
      "Epoch: 13 itrerations: 4700 Training Loss: 4.66586971283\n",
      "Epoch: 13 itrerations: 4800 Training Loss: 4.65885543823\n",
      "Epoch: 13 itrerations: 4900 Training Loss: 4.65669155121\n",
      "Epoch: 13 itrerations: 5000 Training Loss: 4.63215255737\n",
      "Epoch: 13 itrerations: 5100 Training Loss: 4.63676738739\n",
      "Epoch: 13 itrerations: 5200 Training Loss: 4.62849903107\n",
      "Epoch: 13 itrerations: 5300 Training Loss: 4.60387039185\n",
      "Epoch: 13 itrerations: 5400 Training Loss: 4.60707712173\n",
      "Epoch: 13 itrerations: 5500 Training Loss: 4.60189342499\n",
      "Epoch: 13 itrerations: 5600 Training Loss: 4.59844446182\n",
      "Epoch: 13 itrerations: 5700 Training Loss: 4.59887886047\n",
      "Epoch: 13 itrerations: 5800 Training Loss: 4.60207986832\n",
      "Epoch: 13 itrerations: 5900 Training Loss: 4.60167455673\n",
      "Epoch: 13 itrerations: 6000 Training Loss: 4.59568405151\n",
      "Epoch: 13 itrerations: 6100 Training Loss: 4.58301544189\n",
      "Epoch: 13 itrerations: 6200 Training Loss: 4.59278249741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 itrerations: 6300 Training Loss: 4.59322309494\n",
      "Epoch: 13 itrerations: 6400 Training Loss: 4.59258079529\n",
      "Epoch: 13 itrerations: 6500 Training Loss: 4.58409690857\n",
      "Epoch: 13 itrerations: 6600 Training Loss: 4.58609676361\n",
      "Epoch: 13 itrerations: 6700 Training Loss: 4.61561155319\n",
      "Epoch: 13 itrerations: 6800 Training Loss: 4.62114906311\n",
      "Epoch: 13 itrerations: 6900 Training Loss: 4.6308555603\n",
      "Epoch: 13 itrerations: 7000 Training Loss: 4.65031099319\n",
      "Epoch: 13 itrerations: 7100 Training Loss: 4.65446090698\n",
      "Epoch: 13 itrerations: 7200 Training Loss: 4.64376163483\n",
      "Epoch: 13 itrerations: 7300 Training Loss: 4.64431095123\n",
      "Epoch: 13 itrerations: 7400 Training Loss: 4.63240909576\n",
      "Epoch: 13 itrerations: 7500 Training Loss: 4.62788724899\n",
      "Epoch: 13 itrerations: 7600 Training Loss: 4.615026474\n",
      "Epoch: 13 itrerations: 7700 Training Loss: 4.60792446136\n",
      "Epoch: 13 itrerations: 7800 Training Loss: 4.60939502716\n",
      "Epoch: 13 itrerations: 7900 Training Loss: 4.62466001511\n",
      "Epoch: 13 itrerations: 8000 Training Loss: 4.62742233276\n",
      "Epoch: 13 itrerations: 8100 Training Loss: 4.62342596054\n",
      "Epoch: 13 itrerations: 8200 Training Loss: 4.62943983078\n",
      "Epoch: 13 itrerations: 8300 Training Loss: 4.63056850433\n",
      "Epoch: 13 itrerations: 8400 Training Loss: 4.6367225647\n",
      "Epoch: 13 itrerations: 8500 Training Loss: 4.63087129593\n",
      "Epoch: 13 itrerations: 8600 Training Loss: 4.62099695206\n",
      "Epoch: 13 itrerations: 8700 Training Loss: 4.61780881882\n",
      "Epoch: 13 itrerations: 8800 Training Loss: 4.6149263382\n",
      "Epoch: 13 itrerations: 8900 Training Loss: 4.61213827133\n",
      "Epoch: 13 itrerations: 9000 Training Loss: 4.61532640457\n",
      "Epoch: 13 itrerations: 9100 Training Loss: 4.62111091614\n",
      "Epoch: 13 itrerations: 9200 Training Loss: 4.62948226929\n",
      "Epoch: 13 itrerations: 9300 Training Loss: 4.62806177139\n",
      "Epoch: 13 itrerations: 9400 Training Loss: 4.61052083969\n",
      "Epoch: 13 itrerations: 9500 Training Loss: 4.61233997345\n",
      "Epoch: 13 itrerations: 9600 Training Loss: 4.62922668457\n",
      "Epoch: 13 itrerations: 9700 Training Loss: 4.62701034546\n",
      "Epoch: 13 itrerations: 9800 Training Loss: 4.62484121323\n",
      "Epoch: 13 itrerations: 9900 Training Loss: 4.62345409393\n",
      "Epoch: 13 itrerations: 10000 Training Loss: 4.63504695892\n",
      "Epoch: 13 itrerations: 10100 Training Loss: 4.63338565826\n",
      "Epoch: 13 itrerations: 10200 Training Loss: 4.63010501862\n",
      "Epoch: 13 itrerations: 10300 Training Loss: 4.63040208817\n",
      "Epoch: 13 itrerations: 10400 Training Loss: 4.63054418564\n",
      "Epoch: 13 itrerations: 10500 Training Loss: 4.62776851654\n",
      "Epoch: 13 itrerations: 10600 Training Loss: 4.63052797318\n",
      "Epoch: 13 itrerations: 10700 Training Loss: 4.63253164291\n",
      "Epoch: 14 itrerations: 0 Training Loss: 3.33293437958\n",
      "Epoch: 14 itrerations: 100 Training Loss: 4.54051351547\n",
      "Epoch: 14 itrerations: 200 Training Loss: 4.27815389633\n",
      "Epoch: 14 itrerations: 300 Training Loss: 4.53679084778\n",
      "Epoch: 14 itrerations: 400 Training Loss: 4.53824043274\n",
      "Epoch: 14 itrerations: 500 Training Loss: 4.22938919067\n",
      "Epoch: 14 itrerations: 600 Training Loss: 4.35041999817\n",
      "Epoch: 14 itrerations: 700 Training Loss: 4.47327613831\n",
      "Epoch: 14 itrerations: 800 Training Loss: 4.68421649933\n",
      "Epoch: 14 itrerations: 900 Training Loss: 4.68873357773\n",
      "Epoch: 14 itrerations: 1000 Training Loss: 4.68503522873\n",
      "Epoch: 14 itrerations: 1100 Training Loss: 4.62815952301\n",
      "Epoch: 14 itrerations: 1200 Training Loss: 4.69426631927\n",
      "Epoch: 14 itrerations: 1300 Training Loss: 4.68709564209\n",
      "Epoch: 14 itrerations: 1400 Training Loss: 4.77709674835\n",
      "Epoch: 14 itrerations: 1500 Training Loss: 4.76282644272\n",
      "Epoch: 14 itrerations: 1600 Training Loss: 4.81199502945\n",
      "Epoch: 14 itrerations: 1700 Training Loss: 4.79283618927\n",
      "Epoch: 14 itrerations: 1800 Training Loss: 4.76009082794\n",
      "Epoch: 14 itrerations: 1900 Training Loss: 4.7870388031\n",
      "Epoch: 14 itrerations: 2000 Training Loss: 4.79170942307\n",
      "Epoch: 14 itrerations: 2100 Training Loss: 4.79833269119\n",
      "Epoch: 14 itrerations: 2200 Training Loss: 4.75502634048\n",
      "Epoch: 14 itrerations: 2300 Training Loss: 4.78089332581\n",
      "Epoch: 14 itrerations: 2400 Training Loss: 4.74553585052\n",
      "Epoch: 14 itrerations: 2500 Training Loss: 4.71416187286\n",
      "Epoch: 14 itrerations: 2600 Training Loss: 4.72088003159\n",
      "Epoch: 14 itrerations: 2700 Training Loss: 4.75297117233\n",
      "Epoch: 14 itrerations: 2800 Training Loss: 4.70862960815\n",
      "Epoch: 14 itrerations: 2900 Training Loss: 4.6627831459\n",
      "Epoch: 14 itrerations: 3000 Training Loss: 4.71004390717\n",
      "Epoch: 14 itrerations: 3100 Training Loss: 4.69821023941\n",
      "Epoch: 14 itrerations: 3200 Training Loss: 4.7127366066\n",
      "Epoch: 14 itrerations: 3300 Training Loss: 4.67467164993\n",
      "Epoch: 14 itrerations: 3400 Training Loss: 4.68117904663\n",
      "Epoch: 14 itrerations: 3500 Training Loss: 4.70861577988\n",
      "Epoch: 14 itrerations: 3600 Training Loss: 4.71068906784\n",
      "Epoch: 14 itrerations: 3700 Training Loss: 4.69819498062\n",
      "Epoch: 14 itrerations: 3800 Training Loss: 4.71529388428\n",
      "Epoch: 14 itrerations: 3900 Training Loss: 4.68903493881\n",
      "Epoch: 14 itrerations: 4000 Training Loss: 4.68437480927\n",
      "Epoch: 14 itrerations: 4100 Training Loss: 4.6769118309\n",
      "Epoch: 14 itrerations: 4200 Training Loss: 4.66667461395\n",
      "Epoch: 14 itrerations: 4300 Training Loss: 4.6620759964\n",
      "Epoch: 14 itrerations: 4400 Training Loss: 4.67240381241\n",
      "Epoch: 14 itrerations: 4500 Training Loss: 4.68041324615\n",
      "Epoch: 14 itrerations: 4600 Training Loss: 4.66290712357\n",
      "Epoch: 14 itrerations: 4700 Training Loss: 4.67032194138\n",
      "Epoch: 14 itrerations: 4800 Training Loss: 4.64116048813\n",
      "Epoch: 14 itrerations: 4900 Training Loss: 4.63104629517\n",
      "Epoch: 14 itrerations: 5000 Training Loss: 4.62241888046\n",
      "Epoch: 14 itrerations: 5100 Training Loss: 4.61530971527\n",
      "Epoch: 14 itrerations: 5200 Training Loss: 4.6254119873\n",
      "Epoch: 14 itrerations: 5300 Training Loss: 4.61906862259\n",
      "Epoch: 14 itrerations: 5400 Training Loss: 4.6340265274\n",
      "Epoch: 14 itrerations: 5500 Training Loss: 4.62216234207\n",
      "Epoch: 14 itrerations: 5600 Training Loss: 4.62760400772\n",
      "Epoch: 14 itrerations: 5700 Training Loss: 4.62999296188\n",
      "Epoch: 14 itrerations: 5800 Training Loss: 4.63277101517\n",
      "Epoch: 14 itrerations: 5900 Training Loss: 4.64310455322\n",
      "Epoch: 14 itrerations: 6000 Training Loss: 4.64454889297\n",
      "Epoch: 14 itrerations: 6100 Training Loss: 4.63949632645\n",
      "Epoch: 14 itrerations: 6200 Training Loss: 4.63738918304\n",
      "Epoch: 14 itrerations: 6300 Training Loss: 4.64459991455\n",
      "Epoch: 14 itrerations: 6400 Training Loss: 4.64599990845\n",
      "Epoch: 14 itrerations: 6500 Training Loss: 4.64412546158\n",
      "Epoch: 14 itrerations: 6600 Training Loss: 4.63372182846\n",
      "Epoch: 14 itrerations: 6700 Training Loss: 4.64720964432\n",
      "Epoch: 14 itrerations: 6800 Training Loss: 4.65497303009\n",
      "Epoch: 14 itrerations: 6900 Training Loss: 4.66829442978\n",
      "Epoch: 14 itrerations: 7000 Training Loss: 4.68052339554\n",
      "Epoch: 14 itrerations: 7100 Training Loss: 4.67941141129\n",
      "Epoch: 14 itrerations: 7200 Training Loss: 4.6688079834\n",
      "Epoch: 14 itrerations: 7300 Training Loss: 4.67712306976\n",
      "Epoch: 14 itrerations: 7400 Training Loss: 4.6725063324\n",
      "Epoch: 14 itrerations: 7500 Training Loss: 4.67319297791\n",
      "Epoch: 14 itrerations: 7600 Training Loss: 4.66406822205\n",
      "Epoch: 14 itrerations: 7700 Training Loss: 4.65038585663\n",
      "Epoch: 14 itrerations: 7800 Training Loss: 4.6412191391\n",
      "Epoch: 14 itrerations: 7900 Training Loss: 4.64927911758\n",
      "Epoch: 14 itrerations: 8000 Training Loss: 4.64876174927\n",
      "Epoch: 14 itrerations: 8100 Training Loss: 4.6366071701\n",
      "Epoch: 14 itrerations: 8200 Training Loss: 4.63927745819\n",
      "Epoch: 14 itrerations: 8300 Training Loss: 4.64485025406\n",
      "Epoch: 14 itrerations: 8400 Training Loss: 4.63945674896\n",
      "Epoch: 14 itrerations: 8500 Training Loss: 4.63819265366\n",
      "Epoch: 14 itrerations: 8600 Training Loss: 4.62764692307\n",
      "Epoch: 14 itrerations: 8700 Training Loss: 4.63287401199\n",
      "Epoch: 14 itrerations: 8800 Training Loss: 4.63616752625\n",
      "Epoch: 14 itrerations: 8900 Training Loss: 4.6329536438\n",
      "Epoch: 14 itrerations: 9000 Training Loss: 4.64234161377\n",
      "Epoch: 14 itrerations: 9100 Training Loss: 4.64148616791\n",
      "Epoch: 14 itrerations: 9200 Training Loss: 4.65262365341\n",
      "Epoch: 14 itrerations: 9300 Training Loss: 4.64135217667\n",
      "Epoch: 14 itrerations: 9400 Training Loss: 4.63440608978\n",
      "Epoch: 14 itrerations: 9500 Training Loss: 4.63633775711\n",
      "Epoch: 14 itrerations: 9600 Training Loss: 4.64841938019\n",
      "Epoch: 14 itrerations: 9700 Training Loss: 4.64574670792\n",
      "Epoch: 14 itrerations: 9800 Training Loss: 4.6423125267\n",
      "Epoch: 14 itrerations: 9900 Training Loss: 4.64990139008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 itrerations: 10000 Training Loss: 4.66595411301\n",
      "Epoch: 14 itrerations: 10100 Training Loss: 4.66699457169\n",
      "Epoch: 14 itrerations: 10200 Training Loss: 4.66557025909\n",
      "Epoch: 14 itrerations: 10300 Training Loss: 4.66079425812\n",
      "Epoch: 14 itrerations: 10400 Training Loss: 4.65565776825\n",
      "Epoch: 14 itrerations: 10500 Training Loss: 4.64830636978\n",
      "Epoch: 14 itrerations: 10600 Training Loss: 4.64641952515\n",
      "Epoch: 14 itrerations: 10700 Training Loss: 4.64530897141\n",
      "Epoch: 15 itrerations: 0 Training Loss: 8.75120830536\n",
      "Epoch: 15 itrerations: 100 Training Loss: 4.31791639328\n",
      "Epoch: 15 itrerations: 200 Training Loss: 4.07145738602\n",
      "Epoch: 15 itrerations: 300 Training Loss: 4.08623695374\n",
      "Epoch: 15 itrerations: 400 Training Loss: 4.30928516388\n",
      "Epoch: 15 itrerations: 500 Training Loss: 3.97758078575\n",
      "Epoch: 15 itrerations: 600 Training Loss: 4.22032880783\n",
      "Epoch: 15 itrerations: 700 Training Loss: 4.26135396957\n",
      "Epoch: 15 itrerations: 800 Training Loss: 4.49792718887\n",
      "Epoch: 15 itrerations: 900 Training Loss: 4.47981739044\n",
      "Epoch: 15 itrerations: 1000 Training Loss: 4.51327085495\n",
      "Epoch: 15 itrerations: 1100 Training Loss: 4.47780275345\n",
      "Epoch: 15 itrerations: 1200 Training Loss: 4.59334135056\n",
      "Epoch: 15 itrerations: 1300 Training Loss: 4.57103538513\n",
      "Epoch: 15 itrerations: 1400 Training Loss: 4.61715126038\n",
      "Epoch: 15 itrerations: 1500 Training Loss: 4.56235599518\n",
      "Epoch: 15 itrerations: 1600 Training Loss: 4.59774637222\n",
      "Epoch: 15 itrerations: 1700 Training Loss: 4.59852075577\n",
      "Epoch: 15 itrerations: 1800 Training Loss: 4.57972288132\n",
      "Epoch: 15 itrerations: 1900 Training Loss: 4.61575222015\n",
      "Epoch: 15 itrerations: 2000 Training Loss: 4.62820482254\n",
      "Epoch: 15 itrerations: 2100 Training Loss: 4.62771463394\n",
      "Epoch: 15 itrerations: 2200 Training Loss: 4.6030125618\n",
      "Epoch: 15 itrerations: 2300 Training Loss: 4.59762477875\n",
      "Epoch: 15 itrerations: 2400 Training Loss: 4.56793642044\n",
      "Epoch: 15 itrerations: 2500 Training Loss: 4.55048418045\n",
      "Epoch: 15 itrerations: 2600 Training Loss: 4.56820678711\n",
      "Epoch: 15 itrerations: 2700 Training Loss: 4.58917474747\n",
      "Epoch: 15 itrerations: 2800 Training Loss: 4.53222894669\n",
      "Epoch: 15 itrerations: 2900 Training Loss: 4.50923061371\n",
      "Epoch: 15 itrerations: 3000 Training Loss: 4.55240058899\n",
      "Epoch: 15 itrerations: 3100 Training Loss: 4.52534246445\n",
      "Epoch: 15 itrerations: 3200 Training Loss: 4.53664112091\n",
      "Epoch: 15 itrerations: 3300 Training Loss: 4.50680923462\n",
      "Epoch: 15 itrerations: 3400 Training Loss: 4.50213813782\n",
      "Epoch: 15 itrerations: 3500 Training Loss: 4.54688262939\n",
      "Epoch: 15 itrerations: 3600 Training Loss: 4.54633283615\n",
      "Epoch: 15 itrerations: 3700 Training Loss: 4.54914236069\n",
      "Epoch: 15 itrerations: 3800 Training Loss: 4.56528377533\n",
      "Epoch: 15 itrerations: 3900 Training Loss: 4.54496526718\n",
      "Epoch: 15 itrerations: 4000 Training Loss: 4.54242801666\n",
      "Epoch: 15 itrerations: 4100 Training Loss: 4.54529953003\n",
      "Epoch: 15 itrerations: 4200 Training Loss: 4.53881883621\n",
      "Epoch: 15 itrerations: 4300 Training Loss: 4.53769636154\n",
      "Epoch: 15 itrerations: 4400 Training Loss: 4.5634264946\n",
      "Epoch: 15 itrerations: 4500 Training Loss: 4.55395793915\n",
      "Epoch: 15 itrerations: 4600 Training Loss: 4.56511640549\n",
      "Epoch: 15 itrerations: 4700 Training Loss: 4.57124471664\n",
      "Epoch: 15 itrerations: 4800 Training Loss: 4.56351613998\n",
      "Epoch: 15 itrerations: 4900 Training Loss: 4.55305814743\n",
      "Epoch: 15 itrerations: 5000 Training Loss: 4.53524160385\n",
      "Epoch: 15 itrerations: 5100 Training Loss: 4.52684211731\n",
      "Epoch: 15 itrerations: 5200 Training Loss: 4.52931213379\n",
      "Epoch: 15 itrerations: 5300 Training Loss: 4.52237176895\n",
      "Epoch: 15 itrerations: 5400 Training Loss: 4.54221725464\n",
      "Epoch: 15 itrerations: 5500 Training Loss: 4.53362894058\n",
      "Epoch: 15 itrerations: 5600 Training Loss: 4.52771139145\n",
      "Epoch: 15 itrerations: 5700 Training Loss: 4.5181055069\n",
      "Epoch: 15 itrerations: 5800 Training Loss: 4.52287578583\n",
      "Epoch: 15 itrerations: 5900 Training Loss: 4.53626251221\n",
      "Epoch: 15 itrerations: 6000 Training Loss: 4.53346347809\n",
      "Epoch: 15 itrerations: 6100 Training Loss: 4.53588485718\n",
      "Epoch: 15 itrerations: 6200 Training Loss: 4.54016160965\n",
      "Epoch: 15 itrerations: 6300 Training Loss: 4.53808546066\n",
      "Epoch: 15 itrerations: 6400 Training Loss: 4.53629732132\n",
      "Epoch: 15 itrerations: 6500 Training Loss: 4.52622652054\n",
      "Epoch: 15 itrerations: 6600 Training Loss: 4.51176452637\n",
      "Epoch: 15 itrerations: 6700 Training Loss: 4.52264547348\n",
      "Epoch: 15 itrerations: 6800 Training Loss: 4.53036499023\n",
      "Epoch: 15 itrerations: 6900 Training Loss: 4.54351139069\n",
      "Epoch: 15 itrerations: 7000 Training Loss: 4.56613302231\n",
      "Epoch: 15 itrerations: 7100 Training Loss: 4.56973934174\n",
      "Epoch: 15 itrerations: 7200 Training Loss: 4.55426311493\n",
      "Epoch: 15 itrerations: 7300 Training Loss: 4.56515789032\n",
      "Epoch: 15 itrerations: 7400 Training Loss: 4.55098962784\n",
      "Epoch: 15 itrerations: 7500 Training Loss: 4.55539369583\n",
      "Epoch: 15 itrerations: 7600 Training Loss: 4.54707527161\n",
      "Epoch: 15 itrerations: 7700 Training Loss: 4.53926324844\n",
      "Epoch: 15 itrerations: 7800 Training Loss: 4.54641342163\n",
      "Epoch: 15 itrerations: 7900 Training Loss: 4.55797052383\n",
      "Epoch: 15 itrerations: 8000 Training Loss: 4.55551671982\n",
      "Epoch: 15 itrerations: 8100 Training Loss: 4.55137586594\n",
      "Epoch: 15 itrerations: 8200 Training Loss: 4.55026817322\n",
      "Epoch: 15 itrerations: 8300 Training Loss: 4.5487947464\n",
      "Epoch: 15 itrerations: 8400 Training Loss: 4.5495300293\n",
      "Epoch: 15 itrerations: 8500 Training Loss: 4.55042076111\n",
      "Epoch: 15 itrerations: 8600 Training Loss: 4.54624223709\n",
      "Epoch: 15 itrerations: 8700 Training Loss: 4.54833507538\n",
      "Epoch: 15 itrerations: 8800 Training Loss: 4.54707717896\n",
      "Epoch: 15 itrerations: 8900 Training Loss: 4.54123592377\n",
      "Epoch: 15 itrerations: 9000 Training Loss: 4.54364728928\n",
      "Epoch: 15 itrerations: 9100 Training Loss: 4.54811096191\n",
      "Epoch: 15 itrerations: 9200 Training Loss: 4.55489873886\n",
      "Epoch: 15 itrerations: 9300 Training Loss: 4.55000591278\n",
      "Epoch: 15 itrerations: 9400 Training Loss: 4.54321146011\n",
      "Epoch: 15 itrerations: 9500 Training Loss: 4.54083538055\n",
      "Epoch: 15 itrerations: 9600 Training Loss: 4.55273723602\n",
      "Epoch: 15 itrerations: 9700 Training Loss: 4.55660009384\n",
      "Epoch: 15 itrerations: 9800 Training Loss: 4.5593419075\n",
      "Epoch: 15 itrerations: 9900 Training Loss: 4.55391311646\n",
      "Epoch: 15 itrerations: 10000 Training Loss: 4.57486915588\n",
      "Epoch: 15 itrerations: 10100 Training Loss: 4.57432126999\n",
      "Epoch: 15 itrerations: 10200 Training Loss: 4.57389879227\n",
      "Epoch: 15 itrerations: 10300 Training Loss: 4.57499361038\n",
      "Epoch: 15 itrerations: 10400 Training Loss: 4.57734584808\n",
      "Epoch: 15 itrerations: 10500 Training Loss: 4.56967067719\n",
      "Epoch: 15 itrerations: 10600 Training Loss: 4.5713224411\n",
      "Epoch: 15 itrerations: 10700 Training Loss: 4.57500839233\n",
      "Epoch: 16 itrerations: 0 Training Loss: 4.01795530319\n",
      "Epoch: 16 itrerations: 100 Training Loss: 4.17266988754\n",
      "Epoch: 16 itrerations: 200 Training Loss: 4.20471477509\n",
      "Epoch: 16 itrerations: 300 Training Loss: 4.26773786545\n",
      "Epoch: 16 itrerations: 400 Training Loss: 4.26993608475\n",
      "Epoch: 16 itrerations: 500 Training Loss: 4.0253071785\n",
      "Epoch: 16 itrerations: 600 Training Loss: 4.15778303146\n",
      "Epoch: 16 itrerations: 700 Training Loss: 4.19930648804\n",
      "Epoch: 16 itrerations: 800 Training Loss: 4.43769311905\n",
      "Epoch: 16 itrerations: 900 Training Loss: 4.35739088058\n",
      "Epoch: 16 itrerations: 1000 Training Loss: 4.44795179367\n",
      "Epoch: 16 itrerations: 1100 Training Loss: 4.42303180695\n",
      "Epoch: 16 itrerations: 1200 Training Loss: 4.5557103157\n",
      "Epoch: 16 itrerations: 1300 Training Loss: 4.55375814438\n",
      "Epoch: 16 itrerations: 1400 Training Loss: 4.64220714569\n",
      "Epoch: 16 itrerations: 1500 Training Loss: 4.64090681076\n",
      "Epoch: 16 itrerations: 1600 Training Loss: 4.69715690613\n",
      "Epoch: 16 itrerations: 1700 Training Loss: 4.66658210754\n",
      "Epoch: 16 itrerations: 1800 Training Loss: 4.6383600235\n",
      "Epoch: 16 itrerations: 1900 Training Loss: 4.66745853424\n",
      "Epoch: 16 itrerations: 2000 Training Loss: 4.71743535995\n",
      "Epoch: 16 itrerations: 2100 Training Loss: 4.69244813919\n",
      "Epoch: 16 itrerations: 2200 Training Loss: 4.66118717194\n",
      "Epoch: 16 itrerations: 2300 Training Loss: 4.71536064148\n",
      "Epoch: 16 itrerations: 2400 Training Loss: 4.68786048889\n",
      "Epoch: 16 itrerations: 2500 Training Loss: 4.64720964432\n",
      "Epoch: 16 itrerations: 2600 Training Loss: 4.64319753647\n",
      "Epoch: 16 itrerations: 2700 Training Loss: 4.65332365036\n",
      "Epoch: 16 itrerations: 2800 Training Loss: 4.5918211937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 itrerations: 2900 Training Loss: 4.55461835861\n",
      "Epoch: 16 itrerations: 3000 Training Loss: 4.60755681992\n",
      "Epoch: 16 itrerations: 3100 Training Loss: 4.58341503143\n",
      "Epoch: 16 itrerations: 3200 Training Loss: 4.58056735992\n",
      "Epoch: 16 itrerations: 3300 Training Loss: 4.54267787933\n",
      "Epoch: 16 itrerations: 3400 Training Loss: 4.55785369873\n",
      "Epoch: 16 itrerations: 3500 Training Loss: 4.58234548569\n",
      "Epoch: 16 itrerations: 3600 Training Loss: 4.59434556961\n",
      "Epoch: 16 itrerations: 3700 Training Loss: 4.5787062645\n",
      "Epoch: 16 itrerations: 3800 Training Loss: 4.58915805817\n",
      "Epoch: 16 itrerations: 3900 Training Loss: 4.57414150238\n",
      "Epoch: 16 itrerations: 4000 Training Loss: 4.57979631424\n",
      "Epoch: 16 itrerations: 4100 Training Loss: 4.57875204086\n",
      "Epoch: 16 itrerations: 4200 Training Loss: 4.5608754158\n",
      "Epoch: 16 itrerations: 4300 Training Loss: 4.55815076828\n",
      "Epoch: 16 itrerations: 4400 Training Loss: 4.57068967819\n",
      "Epoch: 16 itrerations: 4500 Training Loss: 4.57913970947\n",
      "Epoch: 16 itrerations: 4600 Training Loss: 4.57835102081\n",
      "Epoch: 16 itrerations: 4700 Training Loss: 4.59201002121\n",
      "Epoch: 16 itrerations: 4800 Training Loss: 4.58272790909\n",
      "Epoch: 16 itrerations: 4900 Training Loss: 4.56959295273\n",
      "Epoch: 16 itrerations: 5000 Training Loss: 4.54124736786\n",
      "Epoch: 16 itrerations: 5100 Training Loss: 4.53312540054\n",
      "Epoch: 16 itrerations: 5200 Training Loss: 4.53240633011\n",
      "Epoch: 16 itrerations: 5300 Training Loss: 4.52303552628\n",
      "Epoch: 16 itrerations: 5400 Training Loss: 4.54946947098\n",
      "Epoch: 16 itrerations: 5500 Training Loss: 4.5423617363\n",
      "Epoch: 16 itrerations: 5600 Training Loss: 4.53995513916\n",
      "Epoch: 16 itrerations: 5700 Training Loss: 4.53888368607\n",
      "Epoch: 16 itrerations: 5800 Training Loss: 4.53936767578\n",
      "Epoch: 16 itrerations: 5900 Training Loss: 4.54478788376\n",
      "Epoch: 16 itrerations: 6000 Training Loss: 4.54246425629\n",
      "Epoch: 16 itrerations: 6100 Training Loss: 4.53229236603\n",
      "Epoch: 16 itrerations: 6200 Training Loss: 4.54278898239\n",
      "Epoch: 16 itrerations: 6300 Training Loss: 4.54384660721\n",
      "Epoch: 16 itrerations: 6400 Training Loss: 4.54170513153\n",
      "Epoch: 16 itrerations: 6500 Training Loss: 4.53783607483\n",
      "Epoch: 16 itrerations: 6600 Training Loss: 4.53646230698\n",
      "Epoch: 16 itrerations: 6700 Training Loss: 4.54541063309\n",
      "Epoch: 16 itrerations: 6800 Training Loss: 4.5540971756\n",
      "Epoch: 16 itrerations: 6900 Training Loss: 4.56548070908\n",
      "Epoch: 16 itrerations: 7000 Training Loss: 4.58826494217\n",
      "Epoch: 16 itrerations: 7100 Training Loss: 4.59675264359\n",
      "Epoch: 16 itrerations: 7200 Training Loss: 4.59316492081\n",
      "Epoch: 16 itrerations: 7300 Training Loss: 4.58362197876\n",
      "Epoch: 16 itrerations: 7400 Training Loss: 4.56992149353\n",
      "Epoch: 16 itrerations: 7500 Training Loss: 4.57271194458\n",
      "Epoch: 16 itrerations: 7600 Training Loss: 4.56631565094\n",
      "Epoch: 16 itrerations: 7700 Training Loss: 4.55247783661\n",
      "Epoch: 16 itrerations: 7800 Training Loss: 4.5511841774\n",
      "Epoch: 16 itrerations: 7900 Training Loss: 4.55300140381\n",
      "Epoch: 16 itrerations: 8000 Training Loss: 4.55185413361\n",
      "Epoch: 16 itrerations: 8100 Training Loss: 4.55120944977\n",
      "Epoch: 16 itrerations: 8200 Training Loss: 4.55586099625\n",
      "Epoch: 16 itrerations: 8300 Training Loss: 4.55704832077\n",
      "Epoch: 16 itrerations: 8400 Training Loss: 4.55924272537\n",
      "Epoch: 16 itrerations: 8500 Training Loss: 4.56514883041\n",
      "Epoch: 16 itrerations: 8600 Training Loss: 4.56306266785\n",
      "Epoch: 16 itrerations: 8700 Training Loss: 4.56702423096\n",
      "Epoch: 16 itrerations: 8800 Training Loss: 4.56863164902\n",
      "Epoch: 16 itrerations: 8900 Training Loss: 4.56272411346\n",
      "Epoch: 16 itrerations: 9000 Training Loss: 4.56739473343\n",
      "Epoch: 16 itrerations: 9100 Training Loss: 4.56846427917\n",
      "Epoch: 16 itrerations: 9200 Training Loss: 4.57293510437\n",
      "Epoch: 16 itrerations: 9300 Training Loss: 4.5713968277\n",
      "Epoch: 16 itrerations: 9400 Training Loss: 4.56530714035\n",
      "Epoch: 16 itrerations: 9500 Training Loss: 4.56781148911\n",
      "Epoch: 16 itrerations: 9600 Training Loss: 4.57327270508\n",
      "Epoch: 16 itrerations: 9700 Training Loss: 4.58114004135\n",
      "Epoch: 16 itrerations: 9800 Training Loss: 4.57579421997\n",
      "Epoch: 16 itrerations: 9900 Training Loss: 4.57642602921\n",
      "Epoch: 16 itrerations: 10000 Training Loss: 4.58939218521\n",
      "Epoch: 16 itrerations: 10100 Training Loss: 4.5849313736\n",
      "Epoch: 16 itrerations: 10200 Training Loss: 4.57922315598\n",
      "Epoch: 16 itrerations: 10300 Training Loss: 4.57823038101\n",
      "Epoch: 16 itrerations: 10400 Training Loss: 4.58059215546\n",
      "Epoch: 16 itrerations: 10500 Training Loss: 4.56859302521\n",
      "Epoch: 16 itrerations: 10600 Training Loss: 4.56638097763\n",
      "Epoch: 16 itrerations: 10700 Training Loss: 4.56589794159\n",
      "Epoch: 17 itrerations: 0 Training Loss: 1.6772390604\n",
      "Epoch: 17 itrerations: 100 Training Loss: 4.37072849274\n",
      "Epoch: 17 itrerations: 200 Training Loss: 3.99812030792\n",
      "Epoch: 17 itrerations: 300 Training Loss: 3.94297480583\n",
      "Epoch: 17 itrerations: 400 Training Loss: 4.24243688583\n",
      "Epoch: 17 itrerations: 500 Training Loss: 4.00624656677\n",
      "Epoch: 17 itrerations: 600 Training Loss: 4.28850030899\n",
      "Epoch: 17 itrerations: 700 Training Loss: 4.27724075317\n",
      "Epoch: 17 itrerations: 800 Training Loss: 4.38190460205\n",
      "Epoch: 17 itrerations: 900 Training Loss: 4.38432216644\n",
      "Epoch: 17 itrerations: 1000 Training Loss: 4.39245700836\n",
      "Epoch: 17 itrerations: 1100 Training Loss: 4.37114429474\n",
      "Epoch: 17 itrerations: 1200 Training Loss: 4.55391407013\n",
      "Epoch: 17 itrerations: 1300 Training Loss: 4.54763555527\n",
      "Epoch: 17 itrerations: 1400 Training Loss: 4.55643939972\n",
      "Epoch: 17 itrerations: 1500 Training Loss: 4.55290365219\n",
      "Epoch: 17 itrerations: 1600 Training Loss: 4.56280660629\n",
      "Epoch: 17 itrerations: 1700 Training Loss: 4.57318401337\n",
      "Epoch: 17 itrerations: 1800 Training Loss: 4.55842447281\n",
      "Epoch: 17 itrerations: 1900 Training Loss: 4.63215065002\n",
      "Epoch: 17 itrerations: 2000 Training Loss: 4.64417552948\n",
      "Epoch: 17 itrerations: 2100 Training Loss: 4.60129404068\n",
      "Epoch: 17 itrerations: 2200 Training Loss: 4.5537443161\n",
      "Epoch: 17 itrerations: 2300 Training Loss: 4.55907440186\n",
      "Epoch: 17 itrerations: 2400 Training Loss: 4.55181074142\n",
      "Epoch: 17 itrerations: 2500 Training Loss: 4.54502010345\n",
      "Epoch: 17 itrerations: 2600 Training Loss: 4.55743551254\n",
      "Epoch: 17 itrerations: 2700 Training Loss: 4.61883449554\n",
      "Epoch: 17 itrerations: 2800 Training Loss: 4.56469488144\n",
      "Epoch: 17 itrerations: 2900 Training Loss: 4.53042650223\n",
      "Epoch: 17 itrerations: 3000 Training Loss: 4.57785892487\n",
      "Epoch: 17 itrerations: 3100 Training Loss: 4.56447839737\n",
      "Epoch: 17 itrerations: 3200 Training Loss: 4.55436849594\n",
      "Epoch: 17 itrerations: 3300 Training Loss: 4.52467346191\n",
      "Epoch: 17 itrerations: 3400 Training Loss: 4.54523324966\n",
      "Epoch: 17 itrerations: 3500 Training Loss: 4.56493997574\n",
      "Epoch: 17 itrerations: 3600 Training Loss: 4.58187484741\n",
      "Epoch: 17 itrerations: 3700 Training Loss: 4.58728170395\n",
      "Epoch: 17 itrerations: 3800 Training Loss: 4.60320997238\n",
      "Epoch: 17 itrerations: 3900 Training Loss: 4.59027719498\n",
      "Epoch: 17 itrerations: 4000 Training Loss: 4.5896821022\n",
      "Epoch: 17 itrerations: 4100 Training Loss: 4.57702970505\n",
      "Epoch: 17 itrerations: 4200 Training Loss: 4.56066083908\n",
      "Epoch: 17 itrerations: 4300 Training Loss: 4.55008745193\n",
      "Epoch: 17 itrerations: 4400 Training Loss: 4.57481241226\n",
      "Epoch: 17 itrerations: 4500 Training Loss: 4.56619787216\n",
      "Epoch: 17 itrerations: 4600 Training Loss: 4.55885314941\n",
      "Epoch: 17 itrerations: 4700 Training Loss: 4.54655838013\n",
      "Epoch: 17 itrerations: 4800 Training Loss: 4.53100252151\n",
      "Epoch: 17 itrerations: 4900 Training Loss: 4.51968050003\n",
      "Epoch: 17 itrerations: 5000 Training Loss: 4.49669361115\n",
      "Epoch: 17 itrerations: 5100 Training Loss: 4.4851064682\n",
      "Epoch: 17 itrerations: 5200 Training Loss: 4.49327898026\n",
      "Epoch: 17 itrerations: 5300 Training Loss: 4.48561048508\n",
      "Epoch: 17 itrerations: 5400 Training Loss: 4.49708223343\n",
      "Epoch: 17 itrerations: 5500 Training Loss: 4.48452568054\n",
      "Epoch: 17 itrerations: 5600 Training Loss: 4.48933696747\n",
      "Epoch: 17 itrerations: 5700 Training Loss: 4.47953462601\n",
      "Epoch: 17 itrerations: 5800 Training Loss: 4.47506141663\n",
      "Epoch: 17 itrerations: 5900 Training Loss: 4.47913551331\n",
      "Epoch: 17 itrerations: 6000 Training Loss: 4.47087526321\n",
      "Epoch: 17 itrerations: 6100 Training Loss: 4.45825767517\n",
      "Epoch: 17 itrerations: 6200 Training Loss: 4.47210073471\n",
      "Epoch: 17 itrerations: 6300 Training Loss: 4.47396183014\n",
      "Epoch: 17 itrerations: 6400 Training Loss: 4.46923208237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 itrerations: 6500 Training Loss: 4.46159219742\n",
      "Epoch: 17 itrerations: 6600 Training Loss: 4.45547914505\n",
      "Epoch: 17 itrerations: 6700 Training Loss: 4.46775007248\n",
      "Epoch: 17 itrerations: 6800 Training Loss: 4.48157405853\n",
      "Epoch: 17 itrerations: 6900 Training Loss: 4.49802446365\n",
      "Epoch: 17 itrerations: 7000 Training Loss: 4.51439809799\n",
      "Epoch: 17 itrerations: 7100 Training Loss: 4.53468513489\n",
      "Epoch: 17 itrerations: 7200 Training Loss: 4.51828384399\n",
      "Epoch: 17 itrerations: 7300 Training Loss: 4.5197095871\n",
      "Epoch: 17 itrerations: 7400 Training Loss: 4.51293087006\n",
      "Epoch: 17 itrerations: 7500 Training Loss: 4.5047044754\n",
      "Epoch: 17 itrerations: 7600 Training Loss: 4.49608278275\n",
      "Epoch: 17 itrerations: 7700 Training Loss: 4.4867477417\n",
      "Epoch: 17 itrerations: 7800 Training Loss: 4.49112176895\n",
      "Epoch: 17 itrerations: 7900 Training Loss: 4.50006628036\n",
      "Epoch: 17 itrerations: 8000 Training Loss: 4.49946594238\n",
      "Epoch: 17 itrerations: 8100 Training Loss: 4.48796844482\n",
      "Epoch: 17 itrerations: 8200 Training Loss: 4.49569225311\n",
      "Epoch: 17 itrerations: 8300 Training Loss: 4.49959897995\n",
      "Epoch: 17 itrerations: 8400 Training Loss: 4.50120353699\n",
      "Epoch: 17 itrerations: 8500 Training Loss: 4.50363111496\n",
      "Epoch: 17 itrerations: 8600 Training Loss: 4.49484539032\n",
      "Epoch: 17 itrerations: 8700 Training Loss: 4.49304485321\n",
      "Epoch: 17 itrerations: 8800 Training Loss: 4.49434995651\n",
      "Epoch: 17 itrerations: 8900 Training Loss: 4.49293470383\n",
      "Epoch: 17 itrerations: 9000 Training Loss: 4.50268650055\n",
      "Epoch: 17 itrerations: 9100 Training Loss: 4.49671983719\n",
      "Epoch: 17 itrerations: 9200 Training Loss: 4.51541900635\n",
      "Epoch: 17 itrerations: 9300 Training Loss: 4.51328134537\n",
      "Epoch: 17 itrerations: 9400 Training Loss: 4.50709056854\n",
      "Epoch: 17 itrerations: 9500 Training Loss: 4.50766801834\n",
      "Epoch: 17 itrerations: 9600 Training Loss: 4.5185675621\n",
      "Epoch: 17 itrerations: 9700 Training Loss: 4.5184006691\n",
      "Epoch: 17 itrerations: 9800 Training Loss: 4.51651620865\n",
      "Epoch: 17 itrerations: 9900 Training Loss: 4.5168504715\n",
      "Epoch: 17 itrerations: 10000 Training Loss: 4.53293275833\n",
      "Epoch: 17 itrerations: 10100 Training Loss: 4.53616285324\n",
      "Epoch: 17 itrerations: 10200 Training Loss: 4.53712654114\n",
      "Epoch: 17 itrerations: 10300 Training Loss: 4.53929710388\n",
      "Epoch: 17 itrerations: 10400 Training Loss: 4.53814554214\n",
      "Epoch: 17 itrerations: 10500 Training Loss: 4.52953004837\n",
      "Epoch: 17 itrerations: 10600 Training Loss: 4.5228433609\n",
      "Epoch: 17 itrerations: 10700 Training Loss: 4.52505588531\n",
      "Epoch: 18 itrerations: 0 Training Loss: 3.35165309906\n",
      "Epoch: 18 itrerations: 100 Training Loss: 4.55407571793\n",
      "Epoch: 18 itrerations: 200 Training Loss: 4.24967622757\n",
      "Epoch: 18 itrerations: 300 Training Loss: 4.36342430115\n",
      "Epoch: 18 itrerations: 400 Training Loss: 4.41303062439\n",
      "Epoch: 18 itrerations: 500 Training Loss: 4.14433240891\n",
      "Epoch: 18 itrerations: 600 Training Loss: 4.37105321884\n",
      "Epoch: 18 itrerations: 700 Training Loss: 4.35366725922\n",
      "Epoch: 18 itrerations: 800 Training Loss: 4.55809164047\n",
      "Epoch: 18 itrerations: 900 Training Loss: 4.50268888474\n",
      "Epoch: 18 itrerations: 1000 Training Loss: 4.53942632675\n",
      "Epoch: 18 itrerations: 1100 Training Loss: 4.45226573944\n",
      "Epoch: 18 itrerations: 1200 Training Loss: 4.57952260971\n",
      "Epoch: 18 itrerations: 1300 Training Loss: 4.57212781906\n",
      "Epoch: 18 itrerations: 1400 Training Loss: 4.64964485168\n",
      "Epoch: 18 itrerations: 1500 Training Loss: 4.64220714569\n",
      "Epoch: 18 itrerations: 1600 Training Loss: 4.64329004288\n",
      "Epoch: 18 itrerations: 1700 Training Loss: 4.64391326904\n",
      "Epoch: 18 itrerations: 1800 Training Loss: 4.62420940399\n",
      "Epoch: 18 itrerations: 1900 Training Loss: 4.62931919098\n",
      "Epoch: 18 itrerations: 2000 Training Loss: 4.62592315674\n",
      "Epoch: 18 itrerations: 2100 Training Loss: 4.59266901016\n",
      "Epoch: 18 itrerations: 2200 Training Loss: 4.56520175934\n",
      "Epoch: 18 itrerations: 2300 Training Loss: 4.57989454269\n",
      "Epoch: 18 itrerations: 2400 Training Loss: 4.55256414413\n",
      "Epoch: 18 itrerations: 2500 Training Loss: 4.53567075729\n",
      "Epoch: 18 itrerations: 2600 Training Loss: 4.55567932129\n",
      "Epoch: 18 itrerations: 2700 Training Loss: 4.60634422302\n",
      "Epoch: 18 itrerations: 2800 Training Loss: 4.55757761002\n",
      "Epoch: 18 itrerations: 2900 Training Loss: 4.5243434906\n",
      "Epoch: 18 itrerations: 3000 Training Loss: 4.56854486465\n",
      "Epoch: 18 itrerations: 3100 Training Loss: 4.56044578552\n",
      "Epoch: 18 itrerations: 3200 Training Loss: 4.56572961807\n",
      "Epoch: 18 itrerations: 3300 Training Loss: 4.54643917084\n",
      "Epoch: 18 itrerations: 3400 Training Loss: 4.55182981491\n",
      "Epoch: 18 itrerations: 3500 Training Loss: 4.58588027954\n",
      "Epoch: 18 itrerations: 3600 Training Loss: 4.58791399002\n",
      "Epoch: 18 itrerations: 3700 Training Loss: 4.58315229416\n",
      "Epoch: 18 itrerations: 3800 Training Loss: 4.60474634171\n",
      "Epoch: 18 itrerations: 3900 Training Loss: 4.59269571304\n",
      "Epoch: 18 itrerations: 4000 Training Loss: 4.59150791168\n",
      "Epoch: 18 itrerations: 4100 Training Loss: 4.59145069122\n",
      "Epoch: 18 itrerations: 4200 Training Loss: 4.56664896011\n",
      "Epoch: 18 itrerations: 4300 Training Loss: 4.57312774658\n",
      "Epoch: 18 itrerations: 4400 Training Loss: 4.58233356476\n",
      "Epoch: 18 itrerations: 4500 Training Loss: 4.57722425461\n",
      "Epoch: 18 itrerations: 4600 Training Loss: 4.57961559296\n",
      "Epoch: 18 itrerations: 4700 Training Loss: 4.5820069313\n",
      "Epoch: 18 itrerations: 4800 Training Loss: 4.56522750854\n",
      "Epoch: 18 itrerations: 4900 Training Loss: 4.5591340065\n",
      "Epoch: 18 itrerations: 5000 Training Loss: 4.53598833084\n",
      "Epoch: 18 itrerations: 5100 Training Loss: 4.52898693085\n",
      "Epoch: 18 itrerations: 5200 Training Loss: 4.53466653824\n",
      "Epoch: 18 itrerations: 5300 Training Loss: 4.52750778198\n",
      "Epoch: 18 itrerations: 5400 Training Loss: 4.5396566391\n",
      "Epoch: 18 itrerations: 5500 Training Loss: 4.53507757187\n",
      "Epoch: 18 itrerations: 5600 Training Loss: 4.52930736542\n",
      "Epoch: 18 itrerations: 5700 Training Loss: 4.52147626877\n",
      "Epoch: 18 itrerations: 5800 Training Loss: 4.51301908493\n",
      "Epoch: 18 itrerations: 5900 Training Loss: 4.50531768799\n",
      "Epoch: 18 itrerations: 6000 Training Loss: 4.50164842606\n",
      "Epoch: 18 itrerations: 6100 Training Loss: 4.49782562256\n",
      "Epoch: 18 itrerations: 6200 Training Loss: 4.50771093369\n",
      "Epoch: 18 itrerations: 6300 Training Loss: 4.50039100647\n",
      "Epoch: 18 itrerations: 6400 Training Loss: 4.50715970993\n",
      "Epoch: 18 itrerations: 6500 Training Loss: 4.49807977676\n",
      "Epoch: 18 itrerations: 6600 Training Loss: 4.50304412842\n",
      "Epoch: 18 itrerations: 6700 Training Loss: 4.51366853714\n",
      "Epoch: 18 itrerations: 6800 Training Loss: 4.51732158661\n",
      "Epoch: 18 itrerations: 6900 Training Loss: 4.53591012955\n",
      "Epoch: 18 itrerations: 7000 Training Loss: 4.55836820602\n",
      "Epoch: 18 itrerations: 7100 Training Loss: 4.56799268723\n",
      "Epoch: 18 itrerations: 7200 Training Loss: 4.55954742432\n",
      "Epoch: 18 itrerations: 7300 Training Loss: 4.56196069717\n",
      "Epoch: 18 itrerations: 7400 Training Loss: 4.55033636093\n",
      "Epoch: 18 itrerations: 7500 Training Loss: 4.54204559326\n",
      "Epoch: 18 itrerations: 7600 Training Loss: 4.53183412552\n",
      "Epoch: 18 itrerations: 7700 Training Loss: 4.51786375046\n",
      "Epoch: 18 itrerations: 7800 Training Loss: 4.51724433899\n",
      "Epoch: 18 itrerations: 7900 Training Loss: 4.51457023621\n",
      "Epoch: 18 itrerations: 8000 Training Loss: 4.51613521576\n",
      "Epoch: 18 itrerations: 8100 Training Loss: 4.51895427704\n",
      "Epoch: 18 itrerations: 8200 Training Loss: 4.53201818466\n",
      "Epoch: 18 itrerations: 8300 Training Loss: 4.53652572632\n",
      "Epoch: 18 itrerations: 8400 Training Loss: 4.53140974045\n",
      "Epoch: 18 itrerations: 8500 Training Loss: 4.54192590714\n",
      "Epoch: 18 itrerations: 8600 Training Loss: 4.5325961113\n",
      "Epoch: 18 itrerations: 8700 Training Loss: 4.53866672516\n",
      "Epoch: 18 itrerations: 8800 Training Loss: 4.54190731049\n",
      "Epoch: 18 itrerations: 8900 Training Loss: 4.53228473663\n",
      "Epoch: 18 itrerations: 9000 Training Loss: 4.53085899353\n",
      "Epoch: 18 itrerations: 9100 Training Loss: 4.53163909912\n",
      "Epoch: 18 itrerations: 9200 Training Loss: 4.53484773636\n",
      "Epoch: 18 itrerations: 9300 Training Loss: 4.53679418564\n",
      "Epoch: 18 itrerations: 9400 Training Loss: 4.52310991287\n",
      "Epoch: 18 itrerations: 9500 Training Loss: 4.51983547211\n",
      "Epoch: 18 itrerations: 9600 Training Loss: 4.52896308899\n",
      "Epoch: 18 itrerations: 9700 Training Loss: 4.52532720566\n",
      "Epoch: 18 itrerations: 9800 Training Loss: 4.52059841156\n",
      "Epoch: 18 itrerations: 9900 Training Loss: 4.51729869843\n",
      "Epoch: 18 itrerations: 10000 Training Loss: 4.52713298798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 itrerations: 10100 Training Loss: 4.52466392517\n",
      "Epoch: 18 itrerations: 10200 Training Loss: 4.53515481949\n",
      "Epoch: 18 itrerations: 10300 Training Loss: 4.53581094742\n",
      "Epoch: 18 itrerations: 10400 Training Loss: 4.53579854965\n",
      "Epoch: 18 itrerations: 10500 Training Loss: 4.52817726135\n",
      "Epoch: 18 itrerations: 10600 Training Loss: 4.5289516449\n",
      "Epoch: 18 itrerations: 10700 Training Loss: 4.52902126312\n",
      "Epoch: 19 itrerations: 0 Training Loss: 3.29048013687\n",
      "Epoch: 19 itrerations: 100 Training Loss: 4.67074584961\n",
      "Epoch: 19 itrerations: 200 Training Loss: 4.3257818222\n",
      "Epoch: 19 itrerations: 300 Training Loss: 4.28785324097\n",
      "Epoch: 19 itrerations: 400 Training Loss: 4.28950977325\n",
      "Epoch: 19 itrerations: 500 Training Loss: 3.91003799438\n",
      "Epoch: 19 itrerations: 600 Training Loss: 4.13240814209\n",
      "Epoch: 19 itrerations: 700 Training Loss: 4.19066667557\n",
      "Epoch: 19 itrerations: 800 Training Loss: 4.41943025589\n",
      "Epoch: 19 itrerations: 900 Training Loss: 4.38735103607\n",
      "Epoch: 19 itrerations: 1000 Training Loss: 4.46646213531\n",
      "Epoch: 19 itrerations: 1100 Training Loss: 4.40672254562\n",
      "Epoch: 19 itrerations: 1200 Training Loss: 4.58378696442\n",
      "Epoch: 19 itrerations: 1300 Training Loss: 4.52023458481\n",
      "Epoch: 19 itrerations: 1400 Training Loss: 4.59461450577\n",
      "Epoch: 19 itrerations: 1500 Training Loss: 4.57356882095\n",
      "Epoch: 19 itrerations: 1600 Training Loss: 4.58975172043\n",
      "Epoch: 19 itrerations: 1700 Training Loss: 4.59681606293\n",
      "Epoch: 19 itrerations: 1800 Training Loss: 4.57926034927\n",
      "Epoch: 19 itrerations: 1900 Training Loss: 4.63946819305\n",
      "Epoch: 19 itrerations: 2000 Training Loss: 4.68430805206\n",
      "Epoch: 19 itrerations: 2100 Training Loss: 4.64362001419\n",
      "Epoch: 19 itrerations: 2200 Training Loss: 4.60707139969\n",
      "Epoch: 19 itrerations: 2300 Training Loss: 4.59785556793\n",
      "Epoch: 19 itrerations: 2400 Training Loss: 4.585252285\n",
      "Epoch: 19 itrerations: 2500 Training Loss: 4.57225036621\n",
      "Epoch: 19 itrerations: 2600 Training Loss: 4.56617116928\n",
      "Epoch: 19 itrerations: 2700 Training Loss: 4.59997797012\n",
      "Epoch: 19 itrerations: 2800 Training Loss: 4.56994867325\n",
      "Epoch: 19 itrerations: 2900 Training Loss: 4.52745294571\n",
      "Epoch: 19 itrerations: 3000 Training Loss: 4.58326816559\n",
      "Epoch: 19 itrerations: 3100 Training Loss: 4.56047677994\n",
      "Epoch: 19 itrerations: 3200 Training Loss: 4.57576084137\n",
      "Epoch: 19 itrerations: 3300 Training Loss: 4.54293107986\n",
      "Epoch: 19 itrerations: 3400 Training Loss: 4.55229854584\n",
      "Epoch: 19 itrerations: 3500 Training Loss: 4.58095026016\n",
      "Epoch: 19 itrerations: 3600 Training Loss: 4.58471488953\n",
      "Epoch: 19 itrerations: 3700 Training Loss: 4.57316875458\n",
      "Epoch: 19 itrerations: 3800 Training Loss: 4.58650493622\n",
      "Epoch: 19 itrerations: 3900 Training Loss: 4.59176635742\n",
      "Epoch: 19 itrerations: 4000 Training Loss: 4.59044027328\n",
      "Epoch: 19 itrerations: 4100 Training Loss: 4.58469343185\n",
      "Epoch: 19 itrerations: 4200 Training Loss: 4.5709862709\n",
      "Epoch: 19 itrerations: 4300 Training Loss: 4.56743049622\n",
      "Epoch: 19 itrerations: 4400 Training Loss: 4.57863092422\n",
      "Epoch: 19 itrerations: 4500 Training Loss: 4.57569026947\n",
      "Epoch: 19 itrerations: 4600 Training Loss: 4.57533454895\n",
      "Epoch: 19 itrerations: 4700 Training Loss: 4.55999565125\n",
      "Epoch: 19 itrerations: 4800 Training Loss: 4.54222822189\n",
      "Epoch: 19 itrerations: 4900 Training Loss: 4.53290176392\n",
      "Epoch: 19 itrerations: 5000 Training Loss: 4.51316261292\n",
      "Epoch: 19 itrerations: 5100 Training Loss: 4.51506614685\n",
      "Epoch: 19 itrerations: 5200 Training Loss: 4.51494693756\n",
      "Epoch: 19 itrerations: 5300 Training Loss: 4.51165628433\n",
      "Epoch: 19 itrerations: 5400 Training Loss: 4.52961063385\n",
      "Epoch: 19 itrerations: 5500 Training Loss: 4.51724863052\n",
      "Epoch: 19 itrerations: 5600 Training Loss: 4.51897144318\n",
      "Epoch: 19 itrerations: 5700 Training Loss: 4.52356767654\n",
      "Epoch: 19 itrerations: 5800 Training Loss: 4.53032636642\n",
      "Epoch: 19 itrerations: 5900 Training Loss: 4.53802871704\n",
      "Epoch: 19 itrerations: 6000 Training Loss: 4.52473497391\n",
      "Epoch: 19 itrerations: 6100 Training Loss: 4.50842189789\n",
      "Epoch: 19 itrerations: 6200 Training Loss: 4.50171327591\n",
      "Epoch: 19 itrerations: 6300 Training Loss: 4.51707553864\n",
      "Epoch: 19 itrerations: 6400 Training Loss: 4.51997566223\n",
      "Epoch: 19 itrerations: 6500 Training Loss: 4.51560926437\n",
      "Epoch: 19 itrerations: 6600 Training Loss: 4.51334476471\n",
      "Epoch: 19 itrerations: 6700 Training Loss: 4.52387619019\n",
      "Epoch: 19 itrerations: 6800 Training Loss: 4.5399107933\n",
      "Epoch: 19 itrerations: 6900 Training Loss: 4.55381059647\n",
      "Epoch: 19 itrerations: 7000 Training Loss: 4.57083034515\n",
      "Epoch: 19 itrerations: 7100 Training Loss: 4.58145093918\n",
      "Epoch: 19 itrerations: 7200 Training Loss: 4.56312799454\n",
      "Epoch: 19 itrerations: 7300 Training Loss: 4.55970048904\n",
      "Epoch: 19 itrerations: 7400 Training Loss: 4.54790401459\n",
      "Epoch: 19 itrerations: 7500 Training Loss: 4.54286813736\n",
      "Epoch: 19 itrerations: 7600 Training Loss: 4.53419065475\n",
      "Epoch: 19 itrerations: 7700 Training Loss: 4.52693319321\n",
      "Epoch: 19 itrerations: 7800 Training Loss: 4.5271654129\n",
      "Epoch: 19 itrerations: 7900 Training Loss: 4.53687763214\n",
      "Epoch: 19 itrerations: 8000 Training Loss: 4.53598070145\n",
      "Epoch: 19 itrerations: 8100 Training Loss: 4.53155374527\n",
      "Epoch: 19 itrerations: 8200 Training Loss: 4.53936576843\n",
      "Epoch: 19 itrerations: 8300 Training Loss: 4.54331874847\n",
      "Epoch: 19 itrerations: 8400 Training Loss: 4.5409245491\n",
      "Epoch: 19 itrerations: 8500 Training Loss: 4.54224395752\n",
      "Epoch: 19 itrerations: 8600 Training Loss: 4.53869485855\n",
      "Epoch: 19 itrerations: 8700 Training Loss: 4.53357601166\n",
      "Epoch: 19 itrerations: 8800 Training Loss: 4.5287976265\n",
      "Epoch: 19 itrerations: 8900 Training Loss: 4.52147817612\n",
      "Epoch: 19 itrerations: 9000 Training Loss: 4.53002691269\n",
      "Epoch: 19 itrerations: 9100 Training Loss: 4.53116226196\n",
      "Epoch: 19 itrerations: 9200 Training Loss: 4.5396566391\n",
      "Epoch: 19 itrerations: 9300 Training Loss: 4.5340461731\n",
      "Epoch: 19 itrerations: 9400 Training Loss: 4.53184890747\n",
      "Epoch: 19 itrerations: 9500 Training Loss: 4.53126430511\n",
      "Epoch: 19 itrerations: 9600 Training Loss: 4.54311466217\n",
      "Epoch: 19 itrerations: 9700 Training Loss: 4.55128622055\n",
      "Epoch: 19 itrerations: 9800 Training Loss: 4.55236530304\n",
      "Epoch: 19 itrerations: 9900 Training Loss: 4.54901599884\n",
      "Epoch: 19 itrerations: 10000 Training Loss: 4.56492614746\n",
      "Epoch: 19 itrerations: 10100 Training Loss: 4.56511831284\n",
      "Epoch: 19 itrerations: 10200 Training Loss: 4.56273317337\n",
      "Epoch: 19 itrerations: 10300 Training Loss: 4.5670580864\n",
      "Epoch: 19 itrerations: 10400 Training Loss: 4.56386375427\n",
      "Epoch: 19 itrerations: 10500 Training Loss: 4.55264377594\n",
      "Epoch: 19 itrerations: 10600 Training Loss: 4.55105876923\n",
      "Epoch: 19 itrerations: 10700 Training Loss: 4.55350828171\n",
      "Epoch: 20 itrerations: 0 Training Loss: 3.28554439545\n",
      "Epoch: 20 itrerations: 100 Training Loss: 4.0797419548\n",
      "Epoch: 20 itrerations: 200 Training Loss: 3.75118374825\n",
      "Epoch: 20 itrerations: 300 Training Loss: 3.99914908409\n",
      "Epoch: 20 itrerations: 400 Training Loss: 4.01962280273\n",
      "Epoch: 20 itrerations: 500 Training Loss: 3.80656909943\n",
      "Epoch: 20 itrerations: 600 Training Loss: 3.99593853951\n",
      "Epoch: 20 itrerations: 700 Training Loss: 3.99855279922\n",
      "Epoch: 20 itrerations: 800 Training Loss: 4.29051351547\n",
      "Epoch: 20 itrerations: 900 Training Loss: 4.30767774582\n",
      "Epoch: 20 itrerations: 1000 Training Loss: 4.35211372375\n",
      "Epoch: 20 itrerations: 1100 Training Loss: 4.30320167542\n",
      "Epoch: 20 itrerations: 1200 Training Loss: 4.38553667068\n",
      "Epoch: 20 itrerations: 1300 Training Loss: 4.39710283279\n",
      "Epoch: 20 itrerations: 1400 Training Loss: 4.49733066559\n",
      "Epoch: 20 itrerations: 1500 Training Loss: 4.49748802185\n",
      "Epoch: 20 itrerations: 1600 Training Loss: 4.52681732178\n",
      "Epoch: 20 itrerations: 1700 Training Loss: 4.54594421387\n",
      "Epoch: 20 itrerations: 1800 Training Loss: 4.58162498474\n",
      "Epoch: 20 itrerations: 1900 Training Loss: 4.57682418823\n",
      "Epoch: 20 itrerations: 2000 Training Loss: 4.57482767105\n",
      "Epoch: 20 itrerations: 2100 Training Loss: 4.52921724319\n",
      "Epoch: 20 itrerations: 2200 Training Loss: 4.49602651596\n",
      "Epoch: 20 itrerations: 2300 Training Loss: 4.53823566437\n",
      "Epoch: 20 itrerations: 2400 Training Loss: 4.50730800629\n",
      "Epoch: 20 itrerations: 2500 Training Loss: 4.46606063843\n",
      "Epoch: 20 itrerations: 2600 Training Loss: 4.46738767624\n",
      "Epoch: 20 itrerations: 2700 Training Loss: 4.51501274109\n",
      "Epoch: 20 itrerations: 2800 Training Loss: 4.47627162933\n",
      "Epoch: 20 itrerations: 2900 Training Loss: 4.44115018845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 itrerations: 3000 Training Loss: 4.49672222137\n",
      "Epoch: 20 itrerations: 3100 Training Loss: 4.4743680954\n",
      "Epoch: 20 itrerations: 3200 Training Loss: 4.51025056839\n",
      "Epoch: 20 itrerations: 3300 Training Loss: 4.4862780571\n",
      "Epoch: 20 itrerations: 3400 Training Loss: 4.49519681931\n",
      "Epoch: 20 itrerations: 3500 Training Loss: 4.53532886505\n",
      "Epoch: 20 itrerations: 3600 Training Loss: 4.53424072266\n",
      "Epoch: 20 itrerations: 3700 Training Loss: 4.53502416611\n",
      "Epoch: 20 itrerations: 3800 Training Loss: 4.55537080765\n",
      "Epoch: 20 itrerations: 3900 Training Loss: 4.53460550308\n",
      "Epoch: 20 itrerations: 4000 Training Loss: 4.55013179779\n",
      "Epoch: 20 itrerations: 4100 Training Loss: 4.53290271759\n",
      "Epoch: 20 itrerations: 4200 Training Loss: 4.52730846405\n",
      "Epoch: 20 itrerations: 4300 Training Loss: 4.54138469696\n",
      "Epoch: 20 itrerations: 4400 Training Loss: 4.54723215103\n",
      "Epoch: 20 itrerations: 4500 Training Loss: 4.55351305008\n",
      "Epoch: 20 itrerations: 4600 Training Loss: 4.55574560165\n",
      "Epoch: 20 itrerations: 4700 Training Loss: 4.56483125687\n",
      "Epoch: 20 itrerations: 4800 Training Loss: 4.55017328262\n",
      "Epoch: 20 itrerations: 4900 Training Loss: 4.53313159943\n",
      "Epoch: 20 itrerations: 5000 Training Loss: 4.5069026947\n",
      "Epoch: 20 itrerations: 5100 Training Loss: 4.50236177444\n",
      "Epoch: 20 itrerations: 5200 Training Loss: 4.49534082413\n",
      "Epoch: 20 itrerations: 5300 Training Loss: 4.49855327606\n",
      "Epoch: 20 itrerations: 5400 Training Loss: 4.51898670197\n",
      "Epoch: 20 itrerations: 5500 Training Loss: 4.50505924225\n",
      "Epoch: 20 itrerations: 5600 Training Loss: 4.50459527969\n",
      "Epoch: 20 itrerations: 5700 Training Loss: 4.5034327507\n",
      "Epoch: 20 itrerations: 5800 Training Loss: 4.50552797318\n",
      "Epoch: 20 itrerations: 5900 Training Loss: 4.50837039948\n",
      "Epoch: 20 itrerations: 6000 Training Loss: 4.50339841843\n",
      "Epoch: 20 itrerations: 6100 Training Loss: 4.50663280487\n",
      "Epoch: 20 itrerations: 6200 Training Loss: 4.51564884186\n",
      "Epoch: 20 itrerations: 6300 Training Loss: 4.51236772537\n",
      "Epoch: 20 itrerations: 6400 Training Loss: 4.51166391373\n",
      "Epoch: 20 itrerations: 6500 Training Loss: 4.50835847855\n",
      "Epoch: 20 itrerations: 6600 Training Loss: 4.50391769409\n",
      "Epoch: 20 itrerations: 6700 Training Loss: 4.50176811218\n",
      "Epoch: 20 itrerations: 6800 Training Loss: 4.50861930847\n",
      "Epoch: 20 itrerations: 6900 Training Loss: 4.52546262741\n",
      "Epoch: 20 itrerations: 7000 Training Loss: 4.545586586\n",
      "Epoch: 20 itrerations: 7100 Training Loss: 4.55708217621\n",
      "Epoch: 20 itrerations: 7200 Training Loss: 4.54125547409\n",
      "Epoch: 20 itrerations: 7300 Training Loss: 4.53793144226\n",
      "Epoch: 20 itrerations: 7400 Training Loss: 4.52358198166\n",
      "Epoch: 20 itrerations: 7500 Training Loss: 4.53012561798\n",
      "Epoch: 20 itrerations: 7600 Training Loss: 4.51673603058\n",
      "Epoch: 20 itrerations: 7700 Training Loss: 4.50280380249\n",
      "Epoch: 20 itrerations: 7800 Training Loss: 4.50692176819\n",
      "Epoch: 20 itrerations: 7900 Training Loss: 4.51297187805\n",
      "Epoch: 20 itrerations: 8000 Training Loss: 4.51704835892\n",
      "Epoch: 20 itrerations: 8100 Training Loss: 4.5116724968\n",
      "Epoch: 20 itrerations: 8200 Training Loss: 4.52545452118\n",
      "Epoch: 20 itrerations: 8300 Training Loss: 4.53301143646\n",
      "Epoch: 20 itrerations: 8400 Training Loss: 4.52836513519\n",
      "Epoch: 20 itrerations: 8500 Training Loss: 4.52616548538\n",
      "Epoch: 20 itrerations: 8600 Training Loss: 4.51898527145\n",
      "Epoch: 20 itrerations: 8700 Training Loss: 4.51443052292\n",
      "Epoch: 20 itrerations: 8800 Training Loss: 4.5143404007\n",
      "Epoch: 20 itrerations: 8900 Training Loss: 4.50712203979\n",
      "Epoch: 20 itrerations: 9000 Training Loss: 4.52250576019\n",
      "Epoch: 20 itrerations: 9100 Training Loss: 4.53077745438\n",
      "Epoch: 20 itrerations: 9200 Training Loss: 4.534512043\n",
      "Epoch: 20 itrerations: 9300 Training Loss: 4.52829837799\n",
      "Epoch: 20 itrerations: 9400 Training Loss: 4.51421785355\n",
      "Epoch: 20 itrerations: 9500 Training Loss: 4.51223468781\n",
      "Epoch: 20 itrerations: 9600 Training Loss: 4.51970386505\n",
      "Epoch: 20 itrerations: 9700 Training Loss: 4.51951885223\n",
      "Epoch: 20 itrerations: 9800 Training Loss: 4.51633262634\n",
      "Epoch: 20 itrerations: 9900 Training Loss: 4.51635313034\n",
      "Epoch: 20 itrerations: 10000 Training Loss: 4.52458572388\n",
      "Epoch: 20 itrerations: 10100 Training Loss: 4.52536487579\n",
      "Epoch: 20 itrerations: 10200 Training Loss: 4.51900863647\n",
      "Epoch: 20 itrerations: 10300 Training Loss: 4.52283382416\n",
      "Epoch: 20 itrerations: 10400 Training Loss: 4.51980113983\n",
      "Epoch: 20 itrerations: 10500 Training Loss: 4.51317596436\n",
      "Epoch: 20 itrerations: 10600 Training Loss: 4.51248645782\n",
      "Epoch: 20 itrerations: 10700 Training Loss: 4.51645088196\n",
      "Epoch: 21 itrerations: 0 Training Loss: 2.25226259232\n",
      "Epoch: 21 itrerations: 100 Training Loss: 3.77528190613\n",
      "Epoch: 21 itrerations: 200 Training Loss: 3.80848789215\n",
      "Epoch: 21 itrerations: 300 Training Loss: 3.93397951126\n",
      "Epoch: 21 itrerations: 400 Training Loss: 4.11614561081\n",
      "Epoch: 21 itrerations: 500 Training Loss: 3.81505846977\n",
      "Epoch: 21 itrerations: 600 Training Loss: 4.12436485291\n",
      "Epoch: 21 itrerations: 700 Training Loss: 4.12443828583\n",
      "Epoch: 21 itrerations: 800 Training Loss: 4.32065486908\n",
      "Epoch: 21 itrerations: 900 Training Loss: 4.36692857742\n",
      "Epoch: 21 itrerations: 1000 Training Loss: 4.43338680267\n",
      "Epoch: 21 itrerations: 1100 Training Loss: 4.39428329468\n",
      "Epoch: 21 itrerations: 1200 Training Loss: 4.48730707169\n",
      "Epoch: 21 itrerations: 1300 Training Loss: 4.49552679062\n",
      "Epoch: 21 itrerations: 1400 Training Loss: 4.54822397232\n",
      "Epoch: 21 itrerations: 1500 Training Loss: 4.52732896805\n",
      "Epoch: 21 itrerations: 1600 Training Loss: 4.56110095978\n",
      "Epoch: 21 itrerations: 1700 Training Loss: 4.57176351547\n",
      "Epoch: 21 itrerations: 1800 Training Loss: 4.56750059128\n",
      "Epoch: 21 itrerations: 1900 Training Loss: 4.61271858215\n",
      "Epoch: 21 itrerations: 2000 Training Loss: 4.66529512405\n",
      "Epoch: 21 itrerations: 2100 Training Loss: 4.63368606567\n",
      "Epoch: 21 itrerations: 2200 Training Loss: 4.60003089905\n",
      "Epoch: 21 itrerations: 2300 Training Loss: 4.60514879227\n",
      "Epoch: 21 itrerations: 2400 Training Loss: 4.57894182205\n",
      "Epoch: 21 itrerations: 2500 Training Loss: 4.54735517502\n",
      "Epoch: 21 itrerations: 2600 Training Loss: 4.54931545258\n",
      "Epoch: 21 itrerations: 2700 Training Loss: 4.58189201355\n",
      "Epoch: 21 itrerations: 2800 Training Loss: 4.5510392189\n",
      "Epoch: 21 itrerations: 2900 Training Loss: 4.50951910019\n",
      "Epoch: 21 itrerations: 3000 Training Loss: 4.54939842224\n",
      "Epoch: 21 itrerations: 3100 Training Loss: 4.53894329071\n",
      "Epoch: 21 itrerations: 3200 Training Loss: 4.55596780777\n",
      "Epoch: 21 itrerations: 3300 Training Loss: 4.52983617783\n",
      "Epoch: 21 itrerations: 3400 Training Loss: 4.54303455353\n",
      "Epoch: 21 itrerations: 3500 Training Loss: 4.56933927536\n",
      "Epoch: 21 itrerations: 3600 Training Loss: 4.58099746704\n",
      "Epoch: 21 itrerations: 3700 Training Loss: 4.56379365921\n",
      "Epoch: 21 itrerations: 3800 Training Loss: 4.5801448822\n",
      "Epoch: 21 itrerations: 3900 Training Loss: 4.56504440308\n",
      "Epoch: 21 itrerations: 4000 Training Loss: 4.5636472702\n",
      "Epoch: 21 itrerations: 4100 Training Loss: 4.56035089493\n",
      "Epoch: 21 itrerations: 4200 Training Loss: 4.54500341415\n",
      "Epoch: 21 itrerations: 4300 Training Loss: 4.53642034531\n",
      "Epoch: 21 itrerations: 4400 Training Loss: 4.56128978729\n",
      "Epoch: 21 itrerations: 4500 Training Loss: 4.55828666687\n",
      "Epoch: 21 itrerations: 4600 Training Loss: 4.57955026627\n",
      "Epoch: 21 itrerations: 4700 Training Loss: 4.57347679138\n",
      "Epoch: 21 itrerations: 4800 Training Loss: 4.55806064606\n",
      "Epoch: 21 itrerations: 4900 Training Loss: 4.54179620743\n",
      "Epoch: 21 itrerations: 5000 Training Loss: 4.51361703873\n",
      "Epoch: 21 itrerations: 5100 Training Loss: 4.50169849396\n",
      "Epoch: 21 itrerations: 5200 Training Loss: 4.49828481674\n",
      "Epoch: 21 itrerations: 5300 Training Loss: 4.49623346329\n",
      "Epoch: 21 itrerations: 5400 Training Loss: 4.51499414444\n",
      "Epoch: 21 itrerations: 5500 Training Loss: 4.50368595123\n",
      "Epoch: 21 itrerations: 5600 Training Loss: 4.50003433228\n",
      "Epoch: 21 itrerations: 5700 Training Loss: 4.49325180054\n",
      "Epoch: 21 itrerations: 5800 Training Loss: 4.50136041641\n",
      "Epoch: 21 itrerations: 5900 Training Loss: 4.49660396576\n",
      "Epoch: 21 itrerations: 6000 Training Loss: 4.50159502029\n",
      "Epoch: 21 itrerations: 6100 Training Loss: 4.48307037354\n",
      "Epoch: 21 itrerations: 6200 Training Loss: 4.49523973465\n",
      "Epoch: 21 itrerations: 6300 Training Loss: 4.4913315773\n",
      "Epoch: 21 itrerations: 6400 Training Loss: 4.49093914032\n",
      "Epoch: 21 itrerations: 6500 Training Loss: 4.4858212471\n",
      "Epoch: 21 itrerations: 6600 Training Loss: 4.47567081451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 itrerations: 6700 Training Loss: 4.4806933403\n",
      "Epoch: 21 itrerations: 6800 Training Loss: 4.49050712585\n",
      "Epoch: 21 itrerations: 6900 Training Loss: 4.50142240524\n",
      "Epoch: 21 itrerations: 7000 Training Loss: 4.52264308929\n",
      "Epoch: 21 itrerations: 7100 Training Loss: 4.52638673782\n",
      "Epoch: 21 itrerations: 7200 Training Loss: 4.51130104065\n",
      "Epoch: 21 itrerations: 7300 Training Loss: 4.51609897614\n",
      "Epoch: 21 itrerations: 7400 Training Loss: 4.51254987717\n",
      "Epoch: 21 itrerations: 7500 Training Loss: 4.5139875412\n",
      "Epoch: 21 itrerations: 7600 Training Loss: 4.50101947784\n",
      "Epoch: 21 itrerations: 7700 Training Loss: 4.49441337585\n",
      "Epoch: 21 itrerations: 7800 Training Loss: 4.49694442749\n",
      "Epoch: 21 itrerations: 7900 Training Loss: 4.49836063385\n",
      "Epoch: 21 itrerations: 8000 Training Loss: 4.49476242065\n",
      "Epoch: 21 itrerations: 8100 Training Loss: 4.4877705574\n",
      "Epoch: 21 itrerations: 8200 Training Loss: 4.49299478531\n",
      "Epoch: 21 itrerations: 8300 Training Loss: 4.49694681168\n",
      "Epoch: 21 itrerations: 8400 Training Loss: 4.50265741348\n",
      "Epoch: 21 itrerations: 8500 Training Loss: 4.50709819794\n",
      "Epoch: 21 itrerations: 8600 Training Loss: 4.50475502014\n",
      "Epoch: 21 itrerations: 8700 Training Loss: 4.50302743912\n",
      "Epoch: 21 itrerations: 8800 Training Loss: 4.50304269791\n",
      "Epoch: 21 itrerations: 8900 Training Loss: 4.50774908066\n",
      "Epoch: 21 itrerations: 9000 Training Loss: 4.51928853989\n",
      "Epoch: 21 itrerations: 9100 Training Loss: 4.51579999924\n",
      "Epoch: 21 itrerations: 9200 Training Loss: 4.52629423141\n",
      "Epoch: 21 itrerations: 9300 Training Loss: 4.5205578804\n",
      "Epoch: 21 itrerations: 9400 Training Loss: 4.51022148132\n",
      "Epoch: 21 itrerations: 9500 Training Loss: 4.50639438629\n",
      "Epoch: 21 itrerations: 9600 Training Loss: 4.51702260971\n",
      "Epoch: 21 itrerations: 9700 Training Loss: 4.51616382599\n",
      "Epoch: 21 itrerations: 9800 Training Loss: 4.51972961426\n",
      "Epoch: 21 itrerations: 9900 Training Loss: 4.51750802994\n",
      "Epoch: 21 itrerations: 10000 Training Loss: 4.53632259369\n",
      "Epoch: 21 itrerations: 10100 Training Loss: 4.53490543365\n",
      "Epoch: 21 itrerations: 10200 Training Loss: 4.5296664238\n",
      "Epoch: 21 itrerations: 10300 Training Loss: 4.5295381546\n",
      "Epoch: 21 itrerations: 10400 Training Loss: 4.52912139893\n",
      "Epoch: 21 itrerations: 10500 Training Loss: 4.51886558533\n",
      "Epoch: 21 itrerations: 10600 Training Loss: 4.51616811752\n",
      "Epoch: 21 itrerations: 10700 Training Loss: 4.51630306244\n",
      "Epoch: 22 itrerations: 0 Training Loss: 1.83425676823\n",
      "Epoch: 22 itrerations: 100 Training Loss: 4.12324285507\n",
      "Epoch: 22 itrerations: 200 Training Loss: 3.94319295883\n",
      "Epoch: 22 itrerations: 300 Training Loss: 3.83369445801\n",
      "Epoch: 22 itrerations: 400 Training Loss: 4.06069946289\n",
      "Epoch: 22 itrerations: 500 Training Loss: 3.90744805336\n",
      "Epoch: 22 itrerations: 600 Training Loss: 4.09184741974\n",
      "Epoch: 22 itrerations: 700 Training Loss: 4.10853815079\n",
      "Epoch: 22 itrerations: 800 Training Loss: 4.37272691727\n",
      "Epoch: 22 itrerations: 900 Training Loss: 4.36244297028\n",
      "Epoch: 22 itrerations: 1000 Training Loss: 4.41373729706\n",
      "Epoch: 22 itrerations: 1100 Training Loss: 4.39612436295\n",
      "Epoch: 22 itrerations: 1200 Training Loss: 4.49298429489\n",
      "Epoch: 22 itrerations: 1300 Training Loss: 4.54049825668\n",
      "Epoch: 22 itrerations: 1400 Training Loss: 4.62663125992\n",
      "Epoch: 22 itrerations: 1500 Training Loss: 4.59863042831\n",
      "Epoch: 22 itrerations: 1600 Training Loss: 4.62617301941\n",
      "Epoch: 22 itrerations: 1700 Training Loss: 4.64551925659\n",
      "Epoch: 22 itrerations: 1800 Training Loss: 4.64405012131\n",
      "Epoch: 22 itrerations: 1900 Training Loss: 4.66586971283\n",
      "Epoch: 22 itrerations: 2000 Training Loss: 4.67847442627\n",
      "Epoch: 22 itrerations: 2100 Training Loss: 4.64960050583\n",
      "Epoch: 22 itrerations: 2200 Training Loss: 4.62209177017\n",
      "Epoch: 22 itrerations: 2300 Training Loss: 4.649933815\n",
      "Epoch: 22 itrerations: 2400 Training Loss: 4.61473417282\n",
      "Epoch: 22 itrerations: 2500 Training Loss: 4.59071922302\n",
      "Epoch: 22 itrerations: 2600 Training Loss: 4.57040023804\n",
      "Epoch: 22 itrerations: 2700 Training Loss: 4.58562469482\n",
      "Epoch: 22 itrerations: 2800 Training Loss: 4.5388045311\n",
      "Epoch: 22 itrerations: 2900 Training Loss: 4.4983754158\n",
      "Epoch: 22 itrerations: 3000 Training Loss: 4.55492258072\n",
      "Epoch: 22 itrerations: 3100 Training Loss: 4.5357670784\n",
      "Epoch: 22 itrerations: 3200 Training Loss: 4.53750371933\n",
      "Epoch: 22 itrerations: 3300 Training Loss: 4.50773906708\n",
      "Epoch: 22 itrerations: 3400 Training Loss: 4.51124286652\n",
      "Epoch: 22 itrerations: 3500 Training Loss: 4.54370164871\n",
      "Epoch: 22 itrerations: 3600 Training Loss: 4.54219293594\n",
      "Epoch: 22 itrerations: 3700 Training Loss: 4.5363445282\n",
      "Epoch: 22 itrerations: 3800 Training Loss: 4.53734540939\n",
      "Epoch: 22 itrerations: 3900 Training Loss: 4.51182794571\n",
      "Epoch: 22 itrerations: 4000 Training Loss: 4.51581144333\n",
      "Epoch: 22 itrerations: 4100 Training Loss: 4.51485824585\n",
      "Epoch: 22 itrerations: 4200 Training Loss: 4.50677871704\n",
      "Epoch: 22 itrerations: 4300 Training Loss: 4.50057220459\n",
      "Epoch: 22 itrerations: 4400 Training Loss: 4.5238609314\n",
      "Epoch: 22 itrerations: 4500 Training Loss: 4.52315807343\n",
      "Epoch: 22 itrerations: 4600 Training Loss: 4.52305698395\n",
      "Epoch: 22 itrerations: 4700 Training Loss: 4.5200881958\n",
      "Epoch: 22 itrerations: 4800 Training Loss: 4.52468538284\n",
      "Epoch: 22 itrerations: 4900 Training Loss: 4.51668596268\n",
      "Epoch: 22 itrerations: 5000 Training Loss: 4.49409246445\n",
      "Epoch: 22 itrerations: 5100 Training Loss: 4.49362802505\n",
      "Epoch: 22 itrerations: 5200 Training Loss: 4.48779296875\n",
      "Epoch: 22 itrerations: 5300 Training Loss: 4.50013685226\n",
      "Epoch: 22 itrerations: 5400 Training Loss: 4.52403068542\n",
      "Epoch: 22 itrerations: 5500 Training Loss: 4.5178861618\n",
      "Epoch: 22 itrerations: 5600 Training Loss: 4.51675415039\n",
      "Epoch: 22 itrerations: 5700 Training Loss: 4.5180888176\n",
      "Epoch: 22 itrerations: 5800 Training Loss: 4.51666688919\n",
      "Epoch: 22 itrerations: 5900 Training Loss: 4.51348590851\n",
      "Epoch: 22 itrerations: 6000 Training Loss: 4.50717592239\n",
      "Epoch: 22 itrerations: 6100 Training Loss: 4.49298858643\n",
      "Epoch: 22 itrerations: 6200 Training Loss: 4.49574422836\n",
      "Epoch: 22 itrerations: 6300 Training Loss: 4.50179624557\n",
      "Epoch: 22 itrerations: 6400 Training Loss: 4.49979686737\n",
      "Epoch: 22 itrerations: 6500 Training Loss: 4.48918151855\n",
      "Epoch: 22 itrerations: 6600 Training Loss: 4.47985553741\n",
      "Epoch: 22 itrerations: 6700 Training Loss: 4.4877948761\n",
      "Epoch: 22 itrerations: 6800 Training Loss: 4.48345947266\n",
      "Epoch: 22 itrerations: 6900 Training Loss: 4.49509859085\n",
      "Epoch: 22 itrerations: 7000 Training Loss: 4.51612281799\n",
      "Epoch: 22 itrerations: 7100 Training Loss: 4.52404069901\n",
      "Epoch: 22 itrerations: 7200 Training Loss: 4.51607370377\n",
      "Epoch: 22 itrerations: 7300 Training Loss: 4.51786994934\n",
      "Epoch: 22 itrerations: 7400 Training Loss: 4.50845813751\n",
      "Epoch: 22 itrerations: 7500 Training Loss: 4.50688171387\n",
      "Epoch: 22 itrerations: 7600 Training Loss: 4.50114822388\n",
      "Epoch: 22 itrerations: 7700 Training Loss: 4.48884773254\n",
      "Epoch: 22 itrerations: 7800 Training Loss: 4.48830652237\n",
      "Epoch: 22 itrerations: 7900 Training Loss: 4.48906612396\n",
      "Epoch: 22 itrerations: 8000 Training Loss: 4.48382854462\n",
      "Epoch: 22 itrerations: 8100 Training Loss: 4.47264528275\n",
      "Epoch: 22 itrerations: 8200 Training Loss: 4.47845935822\n",
      "Epoch: 22 itrerations: 8300 Training Loss: 4.48034954071\n",
      "Epoch: 22 itrerations: 8400 Training Loss: 4.48442173004\n",
      "Epoch: 22 itrerations: 8500 Training Loss: 4.48600530624\n",
      "Epoch: 22 itrerations: 8600 Training Loss: 4.48077583313\n",
      "Epoch: 22 itrerations: 8700 Training Loss: 4.48274421692\n",
      "Epoch: 22 itrerations: 8800 Training Loss: 4.49162960052\n",
      "Epoch: 22 itrerations: 8900 Training Loss: 4.48640346527\n",
      "Epoch: 22 itrerations: 9000 Training Loss: 4.49519968033\n",
      "Epoch: 22 itrerations: 9100 Training Loss: 4.49350690842\n",
      "Epoch: 22 itrerations: 9200 Training Loss: 4.49541473389\n",
      "Epoch: 22 itrerations: 9300 Training Loss: 4.48859834671\n",
      "Epoch: 22 itrerations: 9400 Training Loss: 4.47556781769\n",
      "Epoch: 22 itrerations: 9500 Training Loss: 4.48267555237\n",
      "Epoch: 22 itrerations: 9600 Training Loss: 4.49960327148\n",
      "Epoch: 22 itrerations: 9700 Training Loss: 4.50418901443\n",
      "Epoch: 22 itrerations: 9800 Training Loss: 4.50130176544\n",
      "Epoch: 22 itrerations: 9900 Training Loss: 4.50132703781\n",
      "Epoch: 22 itrerations: 10000 Training Loss: 4.51264238358\n",
      "Epoch: 22 itrerations: 10100 Training Loss: 4.51793622971\n",
      "Epoch: 22 itrerations: 10200 Training Loss: 4.51099777222\n",
      "Epoch: 22 itrerations: 10300 Training Loss: 4.51230382919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 itrerations: 10400 Training Loss: 4.51544761658\n",
      "Epoch: 22 itrerations: 10500 Training Loss: 4.50804424286\n",
      "Epoch: 22 itrerations: 10600 Training Loss: 4.50871801376\n",
      "Epoch: 22 itrerations: 10700 Training Loss: 4.50817108154\n",
      "Epoch: 23 itrerations: 0 Training Loss: 3.96354079247\n",
      "Epoch: 23 itrerations: 100 Training Loss: 4.44594478607\n",
      "Epoch: 23 itrerations: 200 Training Loss: 4.15736198425\n",
      "Epoch: 23 itrerations: 300 Training Loss: 4.46490287781\n",
      "Epoch: 23 itrerations: 400 Training Loss: 4.41442775726\n",
      "Epoch: 23 itrerations: 500 Training Loss: 4.04104232788\n",
      "Epoch: 23 itrerations: 600 Training Loss: 4.31094264984\n",
      "Epoch: 23 itrerations: 700 Training Loss: 4.33809804916\n",
      "Epoch: 23 itrerations: 800 Training Loss: 4.52755022049\n",
      "Epoch: 23 itrerations: 900 Training Loss: 4.42766141891\n",
      "Epoch: 23 itrerations: 1000 Training Loss: 4.49453783035\n",
      "Epoch: 23 itrerations: 1100 Training Loss: 4.51566410065\n",
      "Epoch: 23 itrerations: 1200 Training Loss: 4.61538410187\n",
      "Epoch: 23 itrerations: 1300 Training Loss: 4.60607337952\n",
      "Epoch: 23 itrerations: 1400 Training Loss: 4.66476535797\n",
      "Epoch: 23 itrerations: 1500 Training Loss: 4.64738798141\n",
      "Epoch: 23 itrerations: 1600 Training Loss: 4.65463733673\n",
      "Epoch: 23 itrerations: 1700 Training Loss: 4.62779474258\n",
      "Epoch: 23 itrerations: 1800 Training Loss: 4.62059164047\n",
      "Epoch: 23 itrerations: 1900 Training Loss: 4.62889385223\n",
      "Epoch: 23 itrerations: 2000 Training Loss: 4.64468860626\n",
      "Epoch: 23 itrerations: 2100 Training Loss: 4.59432840347\n",
      "Epoch: 23 itrerations: 2200 Training Loss: 4.53151655197\n",
      "Epoch: 23 itrerations: 2300 Training Loss: 4.54966163635\n",
      "Epoch: 23 itrerations: 2400 Training Loss: 4.50091934204\n",
      "Epoch: 23 itrerations: 2500 Training Loss: 4.48638820648\n",
      "Epoch: 23 itrerations: 2600 Training Loss: 4.47885656357\n",
      "Epoch: 23 itrerations: 2700 Training Loss: 4.49571084976\n",
      "Epoch: 23 itrerations: 2800 Training Loss: 4.43149518967\n",
      "Epoch: 23 itrerations: 2900 Training Loss: 4.40715837479\n",
      "Epoch: 23 itrerations: 3000 Training Loss: 4.46087789536\n",
      "Epoch: 23 itrerations: 3100 Training Loss: 4.44443178177\n",
      "Epoch: 23 itrerations: 3200 Training Loss: 4.44166946411\n",
      "Epoch: 23 itrerations: 3300 Training Loss: 4.40912437439\n",
      "Epoch: 23 itrerations: 3400 Training Loss: 4.43772935867\n",
      "Epoch: 23 itrerations: 3500 Training Loss: 4.45277690887\n",
      "Epoch: 23 itrerations: 3600 Training Loss: 4.4794049263\n",
      "Epoch: 23 itrerations: 3700 Training Loss: 4.46873521805\n",
      "Epoch: 23 itrerations: 3800 Training Loss: 4.47962284088\n",
      "Epoch: 23 itrerations: 3900 Training Loss: 4.47285842896\n",
      "Epoch: 23 itrerations: 4000 Training Loss: 4.46644687653\n",
      "Epoch: 23 itrerations: 4100 Training Loss: 4.45542621613\n",
      "Epoch: 23 itrerations: 4200 Training Loss: 4.43684148788\n",
      "Epoch: 23 itrerations: 4300 Training Loss: 4.46014595032\n",
      "Epoch: 23 itrerations: 4400 Training Loss: 4.46842336655\n",
      "Epoch: 23 itrerations: 4500 Training Loss: 4.47476482391\n",
      "Epoch: 23 itrerations: 4600 Training Loss: 4.47443914413\n",
      "Epoch: 23 itrerations: 4700 Training Loss: 4.47738361359\n",
      "Epoch: 23 itrerations: 4800 Training Loss: 4.45786380768\n",
      "Epoch: 23 itrerations: 4900 Training Loss: 4.44681310654\n",
      "Epoch: 23 itrerations: 5000 Training Loss: 4.42729520798\n",
      "Epoch: 23 itrerations: 5100 Training Loss: 4.40508508682\n",
      "Epoch: 23 itrerations: 5200 Training Loss: 4.41113328934\n",
      "Epoch: 23 itrerations: 5300 Training Loss: 4.39995241165\n",
      "Epoch: 23 itrerations: 5400 Training Loss: 4.4257440567\n",
      "Epoch: 23 itrerations: 5500 Training Loss: 4.41368341446\n",
      "Epoch: 23 itrerations: 5600 Training Loss: 4.42702388763\n",
      "Epoch: 23 itrerations: 5700 Training Loss: 4.42752933502\n",
      "Epoch: 23 itrerations: 5800 Training Loss: 4.43256139755\n",
      "Epoch: 23 itrerations: 5900 Training Loss: 4.42746925354\n",
      "Epoch: 23 itrerations: 6000 Training Loss: 4.41665124893\n",
      "Epoch: 23 itrerations: 6100 Training Loss: 4.40502071381\n",
      "Epoch: 23 itrerations: 6200 Training Loss: 4.41346645355\n",
      "Epoch: 23 itrerations: 6300 Training Loss: 4.41602420807\n",
      "Epoch: 23 itrerations: 6400 Training Loss: 4.41429424286\n",
      "Epoch: 23 itrerations: 6500 Training Loss: 4.41098499298\n",
      "Epoch: 23 itrerations: 6600 Training Loss: 4.39851999283\n",
      "Epoch: 23 itrerations: 6700 Training Loss: 4.40384435654\n",
      "Epoch: 23 itrerations: 6800 Training Loss: 4.41110181808\n",
      "Epoch: 23 itrerations: 6900 Training Loss: 4.42569589615\n",
      "Epoch: 23 itrerations: 7000 Training Loss: 4.4519610405\n",
      "Epoch: 23 itrerations: 7100 Training Loss: 4.45480632782\n",
      "Epoch: 23 itrerations: 7200 Training Loss: 4.44733428955\n",
      "Epoch: 23 itrerations: 7300 Training Loss: 4.44629096985\n",
      "Epoch: 23 itrerations: 7400 Training Loss: 4.43706607819\n",
      "Epoch: 23 itrerations: 7500 Training Loss: 4.44857978821\n",
      "Epoch: 23 itrerations: 7600 Training Loss: 4.4389462471\n",
      "Epoch: 23 itrerations: 7700 Training Loss: 4.42964982986\n",
      "Epoch: 23 itrerations: 7800 Training Loss: 4.43805265427\n",
      "Epoch: 23 itrerations: 7900 Training Loss: 4.44730901718\n",
      "Epoch: 23 itrerations: 8000 Training Loss: 4.44491481781\n",
      "Epoch: 23 itrerations: 8100 Training Loss: 4.44225597382\n",
      "Epoch: 23 itrerations: 8200 Training Loss: 4.45584344864\n",
      "Epoch: 23 itrerations: 8300 Training Loss: 4.46115446091\n",
      "Epoch: 23 itrerations: 8400 Training Loss: 4.46107912064\n",
      "Epoch: 23 itrerations: 8500 Training Loss: 4.46924638748\n",
      "Epoch: 23 itrerations: 8600 Training Loss: 4.45589494705\n",
      "Epoch: 23 itrerations: 8700 Training Loss: 4.45774888992\n",
      "Epoch: 23 itrerations: 8800 Training Loss: 4.45690155029\n",
      "Epoch: 23 itrerations: 8900 Training Loss: 4.44967317581\n",
      "Epoch: 23 itrerations: 9000 Training Loss: 4.44716501236\n",
      "Epoch: 23 itrerations: 9100 Training Loss: 4.44187021255\n",
      "Epoch: 23 itrerations: 9200 Training Loss: 4.44576883316\n",
      "Epoch: 23 itrerations: 9300 Training Loss: 4.43615341187\n",
      "Epoch: 23 itrerations: 9400 Training Loss: 4.42311191559\n",
      "Epoch: 23 itrerations: 9500 Training Loss: 4.4262213707\n",
      "Epoch: 23 itrerations: 9600 Training Loss: 4.44931221008\n",
      "Epoch: 23 itrerations: 9700 Training Loss: 4.45032405853\n",
      "Epoch: 23 itrerations: 9800 Training Loss: 4.45197296143\n",
      "Epoch: 23 itrerations: 9900 Training Loss: 4.45346260071\n",
      "Epoch: 23 itrerations: 10000 Training Loss: 4.46336555481\n",
      "Epoch: 23 itrerations: 10100 Training Loss: 4.46305274963\n",
      "Epoch: 23 itrerations: 10200 Training Loss: 4.46186971664\n",
      "Epoch: 23 itrerations: 10300 Training Loss: 4.45953273773\n",
      "Epoch: 23 itrerations: 10400 Training Loss: 4.457010746\n",
      "Epoch: 23 itrerations: 10500 Training Loss: 4.44647407532\n",
      "Epoch: 23 itrerations: 10600 Training Loss: 4.44215297699\n",
      "Epoch: 23 itrerations: 10700 Training Loss: 4.44727516174\n",
      "Epoch: 24 itrerations: 0 Training Loss: 4.54319143295\n",
      "Epoch: 24 itrerations: 100 Training Loss: 4.47454118729\n",
      "Epoch: 24 itrerations: 200 Training Loss: 4.28406620026\n",
      "Epoch: 24 itrerations: 300 Training Loss: 4.24499130249\n",
      "Epoch: 24 itrerations: 400 Training Loss: 4.17366361618\n",
      "Epoch: 24 itrerations: 500 Training Loss: 3.85051584244\n",
      "Epoch: 24 itrerations: 600 Training Loss: 4.08274269104\n",
      "Epoch: 24 itrerations: 700 Training Loss: 4.11669063568\n",
      "Epoch: 24 itrerations: 800 Training Loss: 4.27548980713\n",
      "Epoch: 24 itrerations: 900 Training Loss: 4.2803440094\n",
      "Epoch: 24 itrerations: 1000 Training Loss: 4.24764108658\n",
      "Epoch: 24 itrerations: 1100 Training Loss: 4.2236161232\n",
      "Epoch: 24 itrerations: 1200 Training Loss: 4.29475069046\n",
      "Epoch: 24 itrerations: 1300 Training Loss: 4.32493305206\n",
      "Epoch: 24 itrerations: 1400 Training Loss: 4.4319653511\n",
      "Epoch: 24 itrerations: 1500 Training Loss: 4.45122337341\n",
      "Epoch: 24 itrerations: 1600 Training Loss: 4.45718002319\n",
      "Epoch: 24 itrerations: 1700 Training Loss: 4.48836898804\n",
      "Epoch: 24 itrerations: 1800 Training Loss: 4.49874448776\n",
      "Epoch: 24 itrerations: 1900 Training Loss: 4.54245185852\n",
      "Epoch: 24 itrerations: 2000 Training Loss: 4.55655574799\n",
      "Epoch: 24 itrerations: 2100 Training Loss: 4.55221176147\n",
      "Epoch: 24 itrerations: 2200 Training Loss: 4.54673862457\n",
      "Epoch: 24 itrerations: 2300 Training Loss: 4.52817726135\n",
      "Epoch: 24 itrerations: 2400 Training Loss: 4.49664402008\n",
      "Epoch: 24 itrerations: 2500 Training Loss: 4.47968006134\n",
      "Epoch: 24 itrerations: 2600 Training Loss: 4.48338890076\n",
      "Epoch: 24 itrerations: 2700 Training Loss: 4.52523565292\n",
      "Epoch: 24 itrerations: 2800 Training Loss: 4.48185396194\n",
      "Epoch: 24 itrerations: 2900 Training Loss: 4.46469163895\n",
      "Epoch: 24 itrerations: 3000 Training Loss: 4.50559282303\n",
      "Epoch: 24 itrerations: 3100 Training Loss: 4.48818063736\n",
      "Epoch: 24 itrerations: 3200 Training Loss: 4.49698019028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 itrerations: 3300 Training Loss: 4.47044658661\n",
      "Epoch: 24 itrerations: 3400 Training Loss: 4.4866194725\n",
      "Epoch: 24 itrerations: 3500 Training Loss: 4.50120735168\n",
      "Epoch: 24 itrerations: 3600 Training Loss: 4.51058387756\n",
      "Epoch: 24 itrerations: 3700 Training Loss: 4.50079584122\n",
      "Epoch: 24 itrerations: 3800 Training Loss: 4.51912784576\n",
      "Epoch: 24 itrerations: 3900 Training Loss: 4.50097799301\n",
      "Epoch: 24 itrerations: 4000 Training Loss: 4.50495243073\n",
      "Epoch: 24 itrerations: 4100 Training Loss: 4.49388694763\n",
      "Epoch: 24 itrerations: 4200 Training Loss: 4.49482679367\n",
      "Epoch: 24 itrerations: 4300 Training Loss: 4.48791837692\n",
      "Epoch: 24 itrerations: 4400 Training Loss: 4.49858474731\n",
      "Epoch: 24 itrerations: 4500 Training Loss: 4.51579427719\n",
      "Epoch: 24 itrerations: 4600 Training Loss: 4.50066661835\n",
      "Epoch: 24 itrerations: 4700 Training Loss: 4.50172138214\n",
      "Epoch: 24 itrerations: 4800 Training Loss: 4.47965669632\n",
      "Epoch: 24 itrerations: 4900 Training Loss: 4.47008514404\n",
      "Epoch: 24 itrerations: 5000 Training Loss: 4.44539546967\n",
      "Epoch: 24 itrerations: 5100 Training Loss: 4.41548967361\n",
      "Epoch: 24 itrerations: 5200 Training Loss: 4.41767930984\n",
      "Epoch: 24 itrerations: 5300 Training Loss: 4.40218353271\n",
      "Epoch: 24 itrerations: 5400 Training Loss: 4.41133022308\n",
      "Epoch: 24 itrerations: 5500 Training Loss: 4.41213512421\n",
      "Epoch: 24 itrerations: 5600 Training Loss: 4.41763877869\n",
      "Epoch: 24 itrerations: 5700 Training Loss: 4.4204249382\n",
      "Epoch: 24 itrerations: 5800 Training Loss: 4.42818164825\n",
      "Epoch: 24 itrerations: 5900 Training Loss: 4.43119955063\n",
      "Epoch: 24 itrerations: 6000 Training Loss: 4.42722511292\n",
      "Epoch: 24 itrerations: 6100 Training Loss: 4.42055416107\n",
      "Epoch: 24 itrerations: 6200 Training Loss: 4.43941831589\n",
      "Epoch: 24 itrerations: 6300 Training Loss: 4.43602705002\n",
      "Epoch: 24 itrerations: 6400 Training Loss: 4.43692588806\n",
      "Epoch: 24 itrerations: 6500 Training Loss: 4.43608283997\n",
      "Epoch: 24 itrerations: 6600 Training Loss: 4.4227013588\n",
      "Epoch: 24 itrerations: 6700 Training Loss: 4.42158699036\n",
      "Epoch: 24 itrerations: 6800 Training Loss: 4.43227434158\n",
      "Epoch: 24 itrerations: 6900 Training Loss: 4.44974279404\n",
      "Epoch: 24 itrerations: 7000 Training Loss: 4.46713256836\n",
      "Epoch: 24 itrerations: 7100 Training Loss: 4.47283792496\n",
      "Epoch: 24 itrerations: 7200 Training Loss: 4.46057748795\n",
      "Epoch: 24 itrerations: 7300 Training Loss: 4.46863412857\n",
      "Epoch: 24 itrerations: 7400 Training Loss: 4.45530700684\n",
      "Epoch: 24 itrerations: 7500 Training Loss: 4.45375013351\n",
      "Epoch: 24 itrerations: 7600 Training Loss: 4.44459295273\n",
      "Epoch: 24 itrerations: 7700 Training Loss: 4.429271698\n",
      "Epoch: 24 itrerations: 7800 Training Loss: 4.43196773529\n",
      "Epoch: 24 itrerations: 7900 Training Loss: 4.44093990326\n",
      "Epoch: 24 itrerations: 8000 Training Loss: 4.43699026108\n",
      "Epoch: 24 itrerations: 8100 Training Loss: 4.4314699173\n",
      "Epoch: 24 itrerations: 8200 Training Loss: 4.44156694412\n",
      "Epoch: 24 itrerations: 8300 Training Loss: 4.44056177139\n",
      "Epoch: 24 itrerations: 8400 Training Loss: 4.43627595901\n",
      "Epoch: 24 itrerations: 8500 Training Loss: 4.44468927383\n",
      "Epoch: 24 itrerations: 8600 Training Loss: 4.44069051743\n",
      "Epoch: 24 itrerations: 8700 Training Loss: 4.4325594902\n",
      "Epoch: 24 itrerations: 8800 Training Loss: 4.43524885178\n",
      "Epoch: 24 itrerations: 8900 Training Loss: 4.4329829216\n",
      "Epoch: 24 itrerations: 9000 Training Loss: 4.44305562973\n",
      "Epoch: 24 itrerations: 9100 Training Loss: 4.44972801208\n",
      "Epoch: 24 itrerations: 9200 Training Loss: 4.45005559921\n",
      "Epoch: 24 itrerations: 9300 Training Loss: 4.44954109192\n",
      "Epoch: 24 itrerations: 9400 Training Loss: 4.44313669205\n",
      "Epoch: 24 itrerations: 9500 Training Loss: 4.44258975983\n",
      "Epoch: 24 itrerations: 9600 Training Loss: 4.45675086975\n",
      "Epoch: 24 itrerations: 9700 Training Loss: 4.45682811737\n",
      "Epoch: 24 itrerations: 9800 Training Loss: 4.44920015335\n",
      "Epoch: 24 itrerations: 9900 Training Loss: 4.44840145111\n",
      "Epoch: 24 itrerations: 10000 Training Loss: 4.45984601974\n",
      "Epoch: 24 itrerations: 10100 Training Loss: 4.46025848389\n",
      "Epoch: 24 itrerations: 10200 Training Loss: 4.46450471878\n",
      "Epoch: 24 itrerations: 10300 Training Loss: 4.4636297226\n",
      "Epoch: 24 itrerations: 10400 Training Loss: 4.46160745621\n",
      "Epoch: 24 itrerations: 10500 Training Loss: 4.45415973663\n",
      "Epoch: 24 itrerations: 10600 Training Loss: 4.45340490341\n",
      "Epoch: 24 itrerations: 10700 Training Loss: 4.45522117615\n",
      "Epoch: 25 itrerations: 0 Training Loss: 3.53302764893\n",
      "Epoch: 25 itrerations: 100 Training Loss: 4.29480457306\n",
      "Epoch: 25 itrerations: 200 Training Loss: 4.15922021866\n",
      "Epoch: 25 itrerations: 300 Training Loss: 4.21483707428\n",
      "Epoch: 25 itrerations: 400 Training Loss: 4.32934808731\n",
      "Epoch: 25 itrerations: 500 Training Loss: 3.99343132973\n",
      "Epoch: 25 itrerations: 600 Training Loss: 4.15673685074\n",
      "Epoch: 25 itrerations: 700 Training Loss: 4.15165328979\n",
      "Epoch: 25 itrerations: 800 Training Loss: 4.33497858047\n",
      "Epoch: 25 itrerations: 900 Training Loss: 4.3499879837\n",
      "Epoch: 25 itrerations: 1000 Training Loss: 4.39084148407\n",
      "Epoch: 25 itrerations: 1100 Training Loss: 4.38466739655\n",
      "Epoch: 25 itrerations: 1200 Training Loss: 4.53229808807\n",
      "Epoch: 25 itrerations: 1300 Training Loss: 4.49685811996\n",
      "Epoch: 25 itrerations: 1400 Training Loss: 4.57108688354\n",
      "Epoch: 25 itrerations: 1500 Training Loss: 4.52916526794\n",
      "Epoch: 25 itrerations: 1600 Training Loss: 4.54116249084\n",
      "Epoch: 25 itrerations: 1700 Training Loss: 4.52902460098\n",
      "Epoch: 25 itrerations: 1800 Training Loss: 4.49313497543\n",
      "Epoch: 25 itrerations: 1900 Training Loss: 4.54226922989\n",
      "Epoch: 25 itrerations: 2000 Training Loss: 4.54031610489\n",
      "Epoch: 25 itrerations: 2100 Training Loss: 4.52257871628\n",
      "Epoch: 25 itrerations: 2200 Training Loss: 4.50201129913\n",
      "Epoch: 25 itrerations: 2300 Training Loss: 4.50838470459\n",
      "Epoch: 25 itrerations: 2400 Training Loss: 4.4815993309\n",
      "Epoch: 25 itrerations: 2500 Training Loss: 4.4633436203\n",
      "Epoch: 25 itrerations: 2600 Training Loss: 4.4615945816\n",
      "Epoch: 25 itrerations: 2700 Training Loss: 4.50282144547\n",
      "Epoch: 25 itrerations: 2800 Training Loss: 4.45954227448\n",
      "Epoch: 25 itrerations: 2900 Training Loss: 4.43428707123\n",
      "Epoch: 25 itrerations: 3000 Training Loss: 4.47904920578\n",
      "Epoch: 25 itrerations: 3100 Training Loss: 4.46933507919\n",
      "Epoch: 25 itrerations: 3200 Training Loss: 4.45712995529\n",
      "Epoch: 25 itrerations: 3300 Training Loss: 4.41537618637\n",
      "Epoch: 25 itrerations: 3400 Training Loss: 4.42640686035\n",
      "Epoch: 25 itrerations: 3500 Training Loss: 4.45854139328\n",
      "Epoch: 25 itrerations: 3600 Training Loss: 4.46467542648\n",
      "Epoch: 25 itrerations: 3700 Training Loss: 4.45524120331\n",
      "Epoch: 25 itrerations: 3800 Training Loss: 4.48209762573\n",
      "Epoch: 25 itrerations: 3900 Training Loss: 4.45200157166\n",
      "Epoch: 25 itrerations: 4000 Training Loss: 4.44792604446\n",
      "Epoch: 25 itrerations: 4100 Training Loss: 4.44319105148\n",
      "Epoch: 25 itrerations: 4200 Training Loss: 4.43646860123\n",
      "Epoch: 25 itrerations: 4300 Training Loss: 4.43351221085\n",
      "Epoch: 25 itrerations: 4400 Training Loss: 4.45210075378\n",
      "Epoch: 25 itrerations: 4500 Training Loss: 4.4526553154\n",
      "Epoch: 25 itrerations: 4600 Training Loss: 4.46081256866\n",
      "Epoch: 25 itrerations: 4700 Training Loss: 4.48093223572\n",
      "Epoch: 25 itrerations: 4800 Training Loss: 4.47851896286\n",
      "Epoch: 25 itrerations: 4900 Training Loss: 4.46683120728\n",
      "Epoch: 25 itrerations: 5000 Training Loss: 4.4393286705\n",
      "Epoch: 25 itrerations: 5100 Training Loss: 4.43016862869\n",
      "Epoch: 25 itrerations: 5200 Training Loss: 4.43924188614\n",
      "Epoch: 25 itrerations: 5300 Training Loss: 4.43764352798\n",
      "Epoch: 25 itrerations: 5400 Training Loss: 4.44602632523\n",
      "Epoch: 25 itrerations: 5500 Training Loss: 4.44211006165\n",
      "Epoch: 25 itrerations: 5600 Training Loss: 4.44988822937\n",
      "Epoch: 25 itrerations: 5700 Training Loss: 4.45886087418\n",
      "Epoch: 25 itrerations: 5800 Training Loss: 4.46656942368\n",
      "Epoch: 25 itrerations: 5900 Training Loss: 4.47387647629\n",
      "Epoch: 25 itrerations: 6000 Training Loss: 4.46149492264\n",
      "Epoch: 25 itrerations: 6100 Training Loss: 4.45240163803\n",
      "Epoch: 25 itrerations: 6200 Training Loss: 4.44787216187\n",
      "Epoch: 25 itrerations: 6300 Training Loss: 4.45431089401\n",
      "Epoch: 25 itrerations: 6400 Training Loss: 4.45448541641\n",
      "Epoch: 25 itrerations: 6500 Training Loss: 4.44791936874\n",
      "Epoch: 25 itrerations: 6600 Training Loss: 4.44013214111\n",
      "Epoch: 25 itrerations: 6700 Training Loss: 4.44547796249\n",
      "Epoch: 25 itrerations: 6800 Training Loss: 4.44982433319\n",
      "Epoch: 25 itrerations: 6900 Training Loss: 4.47045326233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 itrerations: 7000 Training Loss: 4.49250125885\n",
      "Epoch: 25 itrerations: 7100 Training Loss: 4.50654268265\n",
      "Epoch: 25 itrerations: 7200 Training Loss: 4.4878320694\n",
      "Epoch: 25 itrerations: 7300 Training Loss: 4.4888625145\n",
      "Epoch: 25 itrerations: 7400 Training Loss: 4.47304964066\n",
      "Epoch: 25 itrerations: 7500 Training Loss: 4.48544216156\n",
      "Epoch: 25 itrerations: 7600 Training Loss: 4.47754907608\n",
      "Epoch: 25 itrerations: 7700 Training Loss: 4.4649066925\n",
      "Epoch: 25 itrerations: 7800 Training Loss: 4.46676111221\n",
      "Epoch: 25 itrerations: 7900 Training Loss: 4.47497034073\n",
      "Epoch: 25 itrerations: 8000 Training Loss: 4.46876955032\n",
      "Epoch: 25 itrerations: 8100 Training Loss: 4.46091508865\n",
      "Epoch: 25 itrerations: 8200 Training Loss: 4.47140359879\n",
      "Epoch: 25 itrerations: 8300 Training Loss: 4.47580099106\n",
      "Epoch: 25 itrerations: 8400 Training Loss: 4.47506523132\n",
      "Epoch: 25 itrerations: 8500 Training Loss: 4.47109937668\n",
      "Epoch: 25 itrerations: 8600 Training Loss: 4.46075105667\n",
      "Epoch: 25 itrerations: 8700 Training Loss: 4.4593706131\n",
      "Epoch: 25 itrerations: 8800 Training Loss: 4.45507192612\n",
      "Epoch: 25 itrerations: 8900 Training Loss: 4.45657157898\n",
      "Epoch: 25 itrerations: 9000 Training Loss: 4.45890951157\n",
      "Epoch: 25 itrerations: 9100 Training Loss: 4.46000146866\n",
      "Epoch: 25 itrerations: 9200 Training Loss: 4.46803569794\n",
      "Epoch: 25 itrerations: 9300 Training Loss: 4.46494579315\n",
      "Epoch: 25 itrerations: 9400 Training Loss: 4.4523639679\n",
      "Epoch: 25 itrerations: 9500 Training Loss: 4.45137500763\n",
      "Epoch: 25 itrerations: 9600 Training Loss: 4.46389722824\n",
      "Epoch: 25 itrerations: 9700 Training Loss: 4.46080684662\n",
      "Epoch: 25 itrerations: 9800 Training Loss: 4.45939731598\n",
      "Epoch: 25 itrerations: 9900 Training Loss: 4.4533700943\n",
      "Epoch: 25 itrerations: 10000 Training Loss: 4.46484899521\n",
      "Epoch: 25 itrerations: 10100 Training Loss: 4.46551990509\n",
      "Epoch: 25 itrerations: 10200 Training Loss: 4.46244382858\n",
      "Epoch: 25 itrerations: 10300 Training Loss: 4.46388149261\n",
      "Epoch: 25 itrerations: 10400 Training Loss: 4.45967054367\n",
      "Epoch: 25 itrerations: 10500 Training Loss: 4.44941329956\n",
      "Epoch: 25 itrerations: 10600 Training Loss: 4.44333314896\n",
      "Epoch: 25 itrerations: 10700 Training Loss: 4.44489812851\n",
      "Epoch: 26 itrerations: 0 Training Loss: 0.822673797607\n",
      "Epoch: 26 itrerations: 100 Training Loss: 4.61685800552\n",
      "Epoch: 26 itrerations: 200 Training Loss: 4.58983755112\n",
      "Epoch: 26 itrerations: 300 Training Loss: 4.63166618347\n",
      "Epoch: 26 itrerations: 400 Training Loss: 4.75608158112\n",
      "Epoch: 26 itrerations: 500 Training Loss: 4.48169183731\n",
      "Epoch: 26 itrerations: 600 Training Loss: 4.57467842102\n",
      "Epoch: 26 itrerations: 700 Training Loss: 4.53629541397\n",
      "Epoch: 26 itrerations: 800 Training Loss: 4.61653089523\n",
      "Epoch: 26 itrerations: 900 Training Loss: 4.52610111237\n",
      "Epoch: 26 itrerations: 1000 Training Loss: 4.47092866898\n",
      "Epoch: 26 itrerations: 1100 Training Loss: 4.4786734581\n",
      "Epoch: 26 itrerations: 1200 Training Loss: 4.58709621429\n",
      "Epoch: 26 itrerations: 1300 Training Loss: 4.59692144394\n",
      "Epoch: 26 itrerations: 1400 Training Loss: 4.6376709938\n",
      "Epoch: 26 itrerations: 1500 Training Loss: 4.59587001801\n",
      "Epoch: 26 itrerations: 1600 Training Loss: 4.60715723038\n",
      "Epoch: 26 itrerations: 1700 Training Loss: 4.60140752792\n",
      "Epoch: 26 itrerations: 1800 Training Loss: 4.60116434097\n",
      "Epoch: 26 itrerations: 1900 Training Loss: 4.64081525803\n",
      "Epoch: 26 itrerations: 2000 Training Loss: 4.65080547333\n",
      "Epoch: 26 itrerations: 2100 Training Loss: 4.60777330399\n",
      "Epoch: 26 itrerations: 2200 Training Loss: 4.59760284424\n",
      "Epoch: 26 itrerations: 2300 Training Loss: 4.62134885788\n",
      "Epoch: 26 itrerations: 2400 Training Loss: 4.58129692078\n",
      "Epoch: 26 itrerations: 2500 Training Loss: 4.53698539734\n",
      "Epoch: 26 itrerations: 2600 Training Loss: 4.52113008499\n",
      "Epoch: 26 itrerations: 2700 Training Loss: 4.55383062363\n",
      "Epoch: 26 itrerations: 2800 Training Loss: 4.50640201569\n",
      "Epoch: 26 itrerations: 2900 Training Loss: 4.46354579926\n",
      "Epoch: 26 itrerations: 3000 Training Loss: 4.53530073166\n",
      "Epoch: 26 itrerations: 3100 Training Loss: 4.52650976181\n",
      "Epoch: 26 itrerations: 3200 Training Loss: 4.5245847702\n",
      "Epoch: 26 itrerations: 3300 Training Loss: 4.49857664108\n",
      "Epoch: 26 itrerations: 3400 Training Loss: 4.49019002914\n",
      "Epoch: 26 itrerations: 3500 Training Loss: 4.51478481293\n",
      "Epoch: 26 itrerations: 3600 Training Loss: 4.51711988449\n",
      "Epoch: 26 itrerations: 3700 Training Loss: 4.50100803375\n",
      "Epoch: 26 itrerations: 3800 Training Loss: 4.51994037628\n",
      "Epoch: 26 itrerations: 3900 Training Loss: 4.50867366791\n",
      "Epoch: 26 itrerations: 4000 Training Loss: 4.49189853668\n",
      "Epoch: 26 itrerations: 4100 Training Loss: 4.47506856918\n",
      "Epoch: 26 itrerations: 4200 Training Loss: 4.46551847458\n",
      "Epoch: 26 itrerations: 4300 Training Loss: 4.46074056625\n",
      "Epoch: 26 itrerations: 4400 Training Loss: 4.48054456711\n",
      "Epoch: 26 itrerations: 4500 Training Loss: 4.47493076324\n",
      "Epoch: 26 itrerations: 4600 Training Loss: 4.46495676041\n",
      "Epoch: 26 itrerations: 4700 Training Loss: 4.47121191025\n",
      "Epoch: 26 itrerations: 4800 Training Loss: 4.46064281464\n",
      "Epoch: 26 itrerations: 4900 Training Loss: 4.45388698578\n",
      "Epoch: 26 itrerations: 5000 Training Loss: 4.42608070374\n",
      "Epoch: 26 itrerations: 5100 Training Loss: 4.40330457687\n",
      "Epoch: 26 itrerations: 5200 Training Loss: 4.4071187973\n",
      "Epoch: 26 itrerations: 5300 Training Loss: 4.41380882263\n",
      "Epoch: 26 itrerations: 5400 Training Loss: 4.41759252548\n",
      "Epoch: 26 itrerations: 5500 Training Loss: 4.40362071991\n",
      "Epoch: 26 itrerations: 5600 Training Loss: 4.40862417221\n",
      "Epoch: 26 itrerations: 5700 Training Loss: 4.41507530212\n",
      "Epoch: 26 itrerations: 5800 Training Loss: 4.42645835876\n",
      "Epoch: 26 itrerations: 5900 Training Loss: 4.42965841293\n",
      "Epoch: 26 itrerations: 6000 Training Loss: 4.42230129242\n",
      "Epoch: 26 itrerations: 6100 Training Loss: 4.41240310669\n",
      "Epoch: 26 itrerations: 6200 Training Loss: 4.40967273712\n",
      "Epoch: 26 itrerations: 6300 Training Loss: 4.41847658157\n",
      "Epoch: 26 itrerations: 6400 Training Loss: 4.41612577438\n",
      "Epoch: 26 itrerations: 6500 Training Loss: 4.41035699844\n",
      "Epoch: 26 itrerations: 6600 Training Loss: 4.40413093567\n",
      "Epoch: 26 itrerations: 6700 Training Loss: 4.41228914261\n",
      "Epoch: 26 itrerations: 6800 Training Loss: 4.42029619217\n",
      "Epoch: 26 itrerations: 6900 Training Loss: 4.42985010147\n",
      "Epoch: 26 itrerations: 7000 Training Loss: 4.44013834\n",
      "Epoch: 26 itrerations: 7100 Training Loss: 4.44876337051\n",
      "Epoch: 26 itrerations: 7200 Training Loss: 4.44203948975\n",
      "Epoch: 26 itrerations: 7300 Training Loss: 4.44341945648\n",
      "Epoch: 26 itrerations: 7400 Training Loss: 4.43749284744\n",
      "Epoch: 26 itrerations: 7500 Training Loss: 4.44289922714\n",
      "Epoch: 26 itrerations: 7600 Training Loss: 4.4379029274\n",
      "Epoch: 26 itrerations: 7700 Training Loss: 4.42381477356\n",
      "Epoch: 26 itrerations: 7800 Training Loss: 4.42743730545\n",
      "Epoch: 26 itrerations: 7900 Training Loss: 4.43428421021\n",
      "Epoch: 26 itrerations: 8000 Training Loss: 4.42578744888\n",
      "Epoch: 26 itrerations: 8100 Training Loss: 4.42539548874\n",
      "Epoch: 26 itrerations: 8200 Training Loss: 4.42778158188\n",
      "Epoch: 26 itrerations: 8300 Training Loss: 4.43873548508\n",
      "Epoch: 26 itrerations: 8400 Training Loss: 4.43177986145\n",
      "Epoch: 26 itrerations: 8500 Training Loss: 4.43521499634\n",
      "Epoch: 26 itrerations: 8600 Training Loss: 4.43196487427\n",
      "Epoch: 26 itrerations: 8700 Training Loss: 4.43136167526\n",
      "Epoch: 26 itrerations: 8800 Training Loss: 4.42746829987\n",
      "Epoch: 26 itrerations: 8900 Training Loss: 4.42223072052\n",
      "Epoch: 26 itrerations: 9000 Training Loss: 4.43207502365\n",
      "Epoch: 26 itrerations: 9100 Training Loss: 4.43090200424\n",
      "Epoch: 26 itrerations: 9200 Training Loss: 4.43517971039\n",
      "Epoch: 26 itrerations: 9300 Training Loss: 4.43036985397\n",
      "Epoch: 26 itrerations: 9400 Training Loss: 4.42284059525\n",
      "Epoch: 26 itrerations: 9500 Training Loss: 4.42871570587\n",
      "Epoch: 26 itrerations: 9600 Training Loss: 4.44143867493\n",
      "Epoch: 26 itrerations: 9700 Training Loss: 4.4493765831\n",
      "Epoch: 26 itrerations: 9800 Training Loss: 4.44934606552\n",
      "Epoch: 26 itrerations: 9900 Training Loss: 4.44928455353\n",
      "Epoch: 26 itrerations: 10000 Training Loss: 4.4643740654\n",
      "Epoch: 26 itrerations: 10100 Training Loss: 4.46262693405\n",
      "Epoch: 26 itrerations: 10200 Training Loss: 4.46224880219\n",
      "Epoch: 26 itrerations: 10300 Training Loss: 4.46129655838\n",
      "Epoch: 26 itrerations: 10400 Training Loss: 4.45853090286\n",
      "Epoch: 26 itrerations: 10500 Training Loss: 4.44971847534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 itrerations: 10600 Training Loss: 4.45297670364\n",
      "Epoch: 26 itrerations: 10700 Training Loss: 4.4532251358\n",
      "Epoch: 27 itrerations: 0 Training Loss: 5.54472303391\n",
      "Epoch: 27 itrerations: 100 Training Loss: 3.70464539528\n",
      "Epoch: 27 itrerations: 200 Training Loss: 3.75048494339\n",
      "Epoch: 27 itrerations: 300 Training Loss: 3.83890128136\n",
      "Epoch: 27 itrerations: 400 Training Loss: 3.94729661942\n",
      "Epoch: 27 itrerations: 500 Training Loss: 3.75710678101\n",
      "Epoch: 27 itrerations: 600 Training Loss: 3.94628667831\n",
      "Epoch: 27 itrerations: 700 Training Loss: 4.01854610443\n",
      "Epoch: 27 itrerations: 800 Training Loss: 4.24343347549\n",
      "Epoch: 27 itrerations: 900 Training Loss: 4.2369966507\n",
      "Epoch: 27 itrerations: 1000 Training Loss: 4.32564783096\n",
      "Epoch: 27 itrerations: 1100 Training Loss: 4.29063224792\n",
      "Epoch: 27 itrerations: 1200 Training Loss: 4.39057207108\n",
      "Epoch: 27 itrerations: 1300 Training Loss: 4.38301181793\n",
      "Epoch: 27 itrerations: 1400 Training Loss: 4.47319412231\n",
      "Epoch: 27 itrerations: 1500 Training Loss: 4.45753335953\n",
      "Epoch: 27 itrerations: 1600 Training Loss: 4.4573264122\n",
      "Epoch: 27 itrerations: 1700 Training Loss: 4.45184755325\n",
      "Epoch: 27 itrerations: 1800 Training Loss: 4.41715240479\n",
      "Epoch: 27 itrerations: 1900 Training Loss: 4.46287155151\n",
      "Epoch: 27 itrerations: 2000 Training Loss: 4.48838281631\n",
      "Epoch: 27 itrerations: 2100 Training Loss: 4.45030546188\n",
      "Epoch: 27 itrerations: 2200 Training Loss: 4.43974399567\n",
      "Epoch: 27 itrerations: 2300 Training Loss: 4.43879699707\n",
      "Epoch: 27 itrerations: 2400 Training Loss: 4.41489362717\n",
      "Epoch: 27 itrerations: 2500 Training Loss: 4.38775491714\n",
      "Epoch: 27 itrerations: 2600 Training Loss: 4.42544746399\n",
      "Epoch: 27 itrerations: 2700 Training Loss: 4.45622062683\n",
      "Epoch: 27 itrerations: 2800 Training Loss: 4.40974664688\n",
      "Epoch: 27 itrerations: 2900 Training Loss: 4.37739133835\n",
      "Epoch: 27 itrerations: 3000 Training Loss: 4.4189658165\n",
      "Epoch: 27 itrerations: 3100 Training Loss: 4.39390563965\n",
      "Epoch: 27 itrerations: 3200 Training Loss: 4.4003777504\n",
      "Epoch: 27 itrerations: 3300 Training Loss: 4.36922168732\n",
      "Epoch: 27 itrerations: 3400 Training Loss: 4.37876272202\n",
      "Epoch: 27 itrerations: 3500 Training Loss: 4.42312574387\n",
      "Epoch: 27 itrerations: 3600 Training Loss: 4.43132925034\n",
      "Epoch: 27 itrerations: 3700 Training Loss: 4.44070386887\n",
      "Epoch: 27 itrerations: 3800 Training Loss: 4.46256828308\n",
      "Epoch: 27 itrerations: 3900 Training Loss: 4.46286249161\n",
      "Epoch: 27 itrerations: 4000 Training Loss: 4.45160245895\n",
      "Epoch: 27 itrerations: 4100 Training Loss: 4.44839763641\n",
      "Epoch: 27 itrerations: 4200 Training Loss: 4.43971252441\n",
      "Epoch: 27 itrerations: 4300 Training Loss: 4.43754053116\n",
      "Epoch: 27 itrerations: 4400 Training Loss: 4.45922470093\n",
      "Epoch: 27 itrerations: 4500 Training Loss: 4.45197868347\n",
      "Epoch: 27 itrerations: 4600 Training Loss: 4.43901872635\n",
      "Epoch: 27 itrerations: 4700 Training Loss: 4.4422454834\n",
      "Epoch: 27 itrerations: 4800 Training Loss: 4.43083238602\n",
      "Epoch: 27 itrerations: 4900 Training Loss: 4.41507911682\n",
      "Epoch: 27 itrerations: 5000 Training Loss: 4.38955783844\n",
      "Epoch: 27 itrerations: 5100 Training Loss: 4.39600372314\n",
      "Epoch: 27 itrerations: 5200 Training Loss: 4.39986848831\n",
      "Epoch: 27 itrerations: 5300 Training Loss: 4.39007091522\n",
      "Epoch: 27 itrerations: 5400 Training Loss: 4.41505479813\n",
      "Epoch: 27 itrerations: 5500 Training Loss: 4.39838457108\n",
      "Epoch: 27 itrerations: 5600 Training Loss: 4.39521217346\n",
      "Epoch: 27 itrerations: 5700 Training Loss: 4.38567209244\n",
      "Epoch: 27 itrerations: 5800 Training Loss: 4.38080739975\n",
      "Epoch: 27 itrerations: 5900 Training Loss: 4.38936805725\n",
      "Epoch: 27 itrerations: 6000 Training Loss: 4.38263225555\n",
      "Epoch: 27 itrerations: 6100 Training Loss: 4.37110710144\n",
      "Epoch: 27 itrerations: 6200 Training Loss: 4.38345336914\n",
      "Epoch: 27 itrerations: 6300 Training Loss: 4.38064813614\n",
      "Epoch: 27 itrerations: 6400 Training Loss: 4.38103103638\n",
      "Epoch: 27 itrerations: 6500 Training Loss: 4.37492895126\n",
      "Epoch: 27 itrerations: 6600 Training Loss: 4.36886692047\n",
      "Epoch: 27 itrerations: 6700 Training Loss: 4.38402557373\n",
      "Epoch: 27 itrerations: 6800 Training Loss: 4.39036560059\n",
      "Epoch: 27 itrerations: 6900 Training Loss: 4.39612674713\n",
      "Epoch: 27 itrerations: 7000 Training Loss: 4.42050981522\n",
      "Epoch: 27 itrerations: 7100 Training Loss: 4.43260240555\n",
      "Epoch: 27 itrerations: 7200 Training Loss: 4.41832780838\n",
      "Epoch: 27 itrerations: 7300 Training Loss: 4.41805934906\n",
      "Epoch: 27 itrerations: 7400 Training Loss: 4.40886640549\n",
      "Epoch: 27 itrerations: 7500 Training Loss: 4.40875911713\n",
      "Epoch: 27 itrerations: 7600 Training Loss: 4.40370082855\n",
      "Epoch: 27 itrerations: 7700 Training Loss: 4.39290332794\n",
      "Epoch: 27 itrerations: 7800 Training Loss: 4.39861154556\n",
      "Epoch: 27 itrerations: 7900 Training Loss: 4.40634202957\n",
      "Epoch: 27 itrerations: 8000 Training Loss: 4.40147972107\n",
      "Epoch: 27 itrerations: 8100 Training Loss: 4.39534854889\n",
      "Epoch: 27 itrerations: 8200 Training Loss: 4.40335798264\n",
      "Epoch: 27 itrerations: 8300 Training Loss: 4.40585184097\n",
      "Epoch: 27 itrerations: 8400 Training Loss: 4.41166639328\n",
      "Epoch: 27 itrerations: 8500 Training Loss: 4.41746997833\n",
      "Epoch: 27 itrerations: 8600 Training Loss: 4.4167637825\n",
      "Epoch: 27 itrerations: 8700 Training Loss: 4.42369508743\n",
      "Epoch: 27 itrerations: 8800 Training Loss: 4.42943906784\n",
      "Epoch: 27 itrerations: 8900 Training Loss: 4.42971086502\n",
      "Epoch: 27 itrerations: 9000 Training Loss: 4.4434967041\n",
      "Epoch: 27 itrerations: 9100 Training Loss: 4.44711446762\n",
      "Epoch: 27 itrerations: 9200 Training Loss: 4.4465675354\n",
      "Epoch: 27 itrerations: 9300 Training Loss: 4.44567966461\n",
      "Epoch: 27 itrerations: 9400 Training Loss: 4.4382815361\n",
      "Epoch: 27 itrerations: 9500 Training Loss: 4.4344367981\n",
      "Epoch: 27 itrerations: 9600 Training Loss: 4.44603204727\n",
      "Epoch: 27 itrerations: 9700 Training Loss: 4.44790410995\n",
      "Epoch: 27 itrerations: 9800 Training Loss: 4.44049549103\n",
      "Epoch: 27 itrerations: 9900 Training Loss: 4.43611812592\n",
      "Epoch: 27 itrerations: 10000 Training Loss: 4.44657039642\n",
      "Epoch: 27 itrerations: 10100 Training Loss: 4.44358205795\n",
      "Epoch: 27 itrerations: 10200 Training Loss: 4.44386816025\n",
      "Epoch: 27 itrerations: 10300 Training Loss: 4.44700908661\n",
      "Epoch: 27 itrerations: 10400 Training Loss: 4.44530534744\n",
      "Epoch: 27 itrerations: 10500 Training Loss: 4.43467378616\n",
      "Epoch: 27 itrerations: 10600 Training Loss: 4.44014501572\n",
      "Epoch: 27 itrerations: 10700 Training Loss: 4.43788909912\n",
      "Epoch: 28 itrerations: 0 Training Loss: 2.57841515541\n",
      "Epoch: 28 itrerations: 100 Training Loss: 4.3510093689\n",
      "Epoch: 28 itrerations: 200 Training Loss: 4.10079622269\n",
      "Epoch: 28 itrerations: 300 Training Loss: 3.9609131813\n",
      "Epoch: 28 itrerations: 400 Training Loss: 4.08975028992\n",
      "Epoch: 28 itrerations: 500 Training Loss: 3.84478855133\n",
      "Epoch: 28 itrerations: 600 Training Loss: 4.2474398613\n",
      "Epoch: 28 itrerations: 700 Training Loss: 4.2455496788\n",
      "Epoch: 28 itrerations: 800 Training Loss: 4.37376070023\n",
      "Epoch: 28 itrerations: 900 Training Loss: 4.39543104172\n",
      "Epoch: 28 itrerations: 1000 Training Loss: 4.33488035202\n",
      "Epoch: 28 itrerations: 1100 Training Loss: 4.32623195648\n",
      "Epoch: 28 itrerations: 1200 Training Loss: 4.44141435623\n",
      "Epoch: 28 itrerations: 1300 Training Loss: 4.4022102356\n",
      "Epoch: 28 itrerations: 1400 Training Loss: 4.50895023346\n",
      "Epoch: 28 itrerations: 1500 Training Loss: 4.47715044022\n",
      "Epoch: 28 itrerations: 1600 Training Loss: 4.48114156723\n",
      "Epoch: 28 itrerations: 1700 Training Loss: 4.47616815567\n",
      "Epoch: 28 itrerations: 1800 Training Loss: 4.44709444046\n",
      "Epoch: 28 itrerations: 1900 Training Loss: 4.46370029449\n",
      "Epoch: 28 itrerations: 2000 Training Loss: 4.45876789093\n",
      "Epoch: 28 itrerations: 2100 Training Loss: 4.4244556427\n",
      "Epoch: 28 itrerations: 2200 Training Loss: 4.38803100586\n",
      "Epoch: 28 itrerations: 2300 Training Loss: 4.41180562973\n",
      "Epoch: 28 itrerations: 2400 Training Loss: 4.37822151184\n",
      "Epoch: 28 itrerations: 2500 Training Loss: 4.35300683975\n",
      "Epoch: 28 itrerations: 2600 Training Loss: 4.35416507721\n",
      "Epoch: 28 itrerations: 2700 Training Loss: 4.38376617432\n",
      "Epoch: 28 itrerations: 2800 Training Loss: 4.35488891602\n",
      "Epoch: 28 itrerations: 2900 Training Loss: 4.3274102211\n",
      "Epoch: 28 itrerations: 3000 Training Loss: 4.38364028931\n",
      "Epoch: 28 itrerations: 3100 Training Loss: 4.38702917099\n",
      "Epoch: 28 itrerations: 3200 Training Loss: 4.3919801712\n",
      "Epoch: 28 itrerations: 3300 Training Loss: 4.36622524261\n",
      "Epoch: 28 itrerations: 3400 Training Loss: 4.39358472824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 itrerations: 3500 Training Loss: 4.4321141243\n",
      "Epoch: 28 itrerations: 3600 Training Loss: 4.46691942215\n",
      "Epoch: 28 itrerations: 3700 Training Loss: 4.45220422745\n",
      "Epoch: 28 itrerations: 3800 Training Loss: 4.46946239471\n",
      "Epoch: 28 itrerations: 3900 Training Loss: 4.4528260231\n",
      "Epoch: 28 itrerations: 4000 Training Loss: 4.45083332062\n",
      "Epoch: 28 itrerations: 4100 Training Loss: 4.44117689133\n",
      "Epoch: 28 itrerations: 4200 Training Loss: 4.42847490311\n",
      "Epoch: 28 itrerations: 4300 Training Loss: 4.43505096436\n",
      "Epoch: 28 itrerations: 4400 Training Loss: 4.4617357254\n",
      "Epoch: 28 itrerations: 4500 Training Loss: 4.46386003494\n",
      "Epoch: 28 itrerations: 4600 Training Loss: 4.45263671875\n",
      "Epoch: 28 itrerations: 4700 Training Loss: 4.45518636703\n",
      "Epoch: 28 itrerations: 4800 Training Loss: 4.440990448\n",
      "Epoch: 28 itrerations: 4900 Training Loss: 4.42354059219\n",
      "Epoch: 28 itrerations: 5000 Training Loss: 4.40133714676\n",
      "Epoch: 28 itrerations: 5100 Training Loss: 4.37972831726\n",
      "Epoch: 28 itrerations: 5200 Training Loss: 4.38722848892\n",
      "Epoch: 28 itrerations: 5300 Training Loss: 4.39034986496\n",
      "Epoch: 28 itrerations: 5400 Training Loss: 4.41380882263\n",
      "Epoch: 28 itrerations: 5500 Training Loss: 4.40541553497\n",
      "Epoch: 28 itrerations: 5600 Training Loss: 4.41097164154\n",
      "Epoch: 28 itrerations: 5700 Training Loss: 4.41968584061\n",
      "Epoch: 28 itrerations: 5800 Training Loss: 4.41700935364\n",
      "Epoch: 28 itrerations: 5900 Training Loss: 4.4088139534\n",
      "Epoch: 28 itrerations: 6000 Training Loss: 4.40535640717\n",
      "Epoch: 28 itrerations: 6100 Training Loss: 4.39906644821\n",
      "Epoch: 28 itrerations: 6200 Training Loss: 4.39256906509\n",
      "Epoch: 28 itrerations: 6300 Training Loss: 4.38890886307\n",
      "Epoch: 28 itrerations: 6400 Training Loss: 4.38351202011\n",
      "Epoch: 28 itrerations: 6500 Training Loss: 4.3744187355\n",
      "Epoch: 28 itrerations: 6600 Training Loss: 4.37265968323\n",
      "Epoch: 28 itrerations: 6700 Training Loss: 4.38315153122\n",
      "Epoch: 28 itrerations: 6800 Training Loss: 4.38640880585\n",
      "Epoch: 28 itrerations: 6900 Training Loss: 4.40234708786\n",
      "Epoch: 28 itrerations: 7000 Training Loss: 4.43038272858\n",
      "Epoch: 28 itrerations: 7100 Training Loss: 4.43831396103\n",
      "Epoch: 28 itrerations: 7200 Training Loss: 4.43691110611\n",
      "Epoch: 28 itrerations: 7300 Training Loss: 4.43085193634\n",
      "Epoch: 28 itrerations: 7400 Training Loss: 4.42436695099\n",
      "Epoch: 28 itrerations: 7500 Training Loss: 4.42772197723\n",
      "Epoch: 28 itrerations: 7600 Training Loss: 4.4174451828\n",
      "Epoch: 28 itrerations: 7700 Training Loss: 4.41025304794\n",
      "Epoch: 28 itrerations: 7800 Training Loss: 4.40922784805\n",
      "Epoch: 28 itrerations: 7900 Training Loss: 4.41673994064\n",
      "Epoch: 28 itrerations: 8000 Training Loss: 4.40903186798\n",
      "Epoch: 28 itrerations: 8100 Training Loss: 4.40090847015\n",
      "Epoch: 28 itrerations: 8200 Training Loss: 4.41303300858\n",
      "Epoch: 28 itrerations: 8300 Training Loss: 4.41960477829\n",
      "Epoch: 28 itrerations: 8400 Training Loss: 4.42026996613\n",
      "Epoch: 28 itrerations: 8500 Training Loss: 4.41612100601\n",
      "Epoch: 28 itrerations: 8600 Training Loss: 4.41337156296\n",
      "Epoch: 28 itrerations: 8700 Training Loss: 4.41120433807\n",
      "Epoch: 28 itrerations: 8800 Training Loss: 4.41651296616\n",
      "Epoch: 28 itrerations: 8900 Training Loss: 4.41551065445\n",
      "Epoch: 28 itrerations: 9000 Training Loss: 4.41573905945\n",
      "Epoch: 28 itrerations: 9100 Training Loss: 4.41340875626\n",
      "Epoch: 28 itrerations: 9200 Training Loss: 4.41765546799\n",
      "Epoch: 28 itrerations: 9300 Training Loss: 4.41243648529\n",
      "Epoch: 28 itrerations: 9400 Training Loss: 4.39859724045\n",
      "Epoch: 28 itrerations: 9500 Training Loss: 4.39698553085\n",
      "Epoch: 28 itrerations: 9600 Training Loss: 4.414686203\n",
      "Epoch: 28 itrerations: 9700 Training Loss: 4.41894245148\n",
      "Epoch: 28 itrerations: 9800 Training Loss: 4.41567802429\n",
      "Epoch: 28 itrerations: 9900 Training Loss: 4.41231060028\n",
      "Epoch: 28 itrerations: 10000 Training Loss: 4.42004156113\n",
      "Epoch: 28 itrerations: 10100 Training Loss: 4.42040777206\n",
      "Epoch: 28 itrerations: 10200 Training Loss: 4.41991138458\n",
      "Epoch: 28 itrerations: 10300 Training Loss: 4.42104625702\n",
      "Epoch: 28 itrerations: 10400 Training Loss: 4.42010736465\n",
      "Epoch: 28 itrerations: 10500 Training Loss: 4.41082382202\n",
      "Epoch: 28 itrerations: 10600 Training Loss: 4.41350984573\n",
      "Epoch: 28 itrerations: 10700 Training Loss: 4.4143576622\n",
      "Epoch: 29 itrerations: 0 Training Loss: 5.39899110794\n",
      "Epoch: 29 itrerations: 100 Training Loss: 4.22910356522\n",
      "Epoch: 29 itrerations: 200 Training Loss: 4.25286483765\n",
      "Epoch: 29 itrerations: 300 Training Loss: 4.31444549561\n",
      "Epoch: 29 itrerations: 400 Training Loss: 4.46574735641\n",
      "Epoch: 29 itrerations: 500 Training Loss: 4.13641738892\n",
      "Epoch: 29 itrerations: 600 Training Loss: 4.39024734497\n",
      "Epoch: 29 itrerations: 700 Training Loss: 4.31920099258\n",
      "Epoch: 29 itrerations: 800 Training Loss: 4.519572258\n",
      "Epoch: 29 itrerations: 900 Training Loss: 4.4729552269\n",
      "Epoch: 29 itrerations: 1000 Training Loss: 4.48828792572\n",
      "Epoch: 29 itrerations: 1100 Training Loss: 4.45163822174\n",
      "Epoch: 29 itrerations: 1200 Training Loss: 4.54008150101\n",
      "Epoch: 29 itrerations: 1300 Training Loss: 4.52973842621\n",
      "Epoch: 29 itrerations: 1400 Training Loss: 4.68806791306\n",
      "Epoch: 29 itrerations: 1500 Training Loss: 4.70783615112\n",
      "Epoch: 29 itrerations: 1600 Training Loss: 4.69220781326\n",
      "Epoch: 29 itrerations: 1700 Training Loss: 4.67711162567\n",
      "Epoch: 29 itrerations: 1800 Training Loss: 4.64532518387\n",
      "Epoch: 29 itrerations: 1900 Training Loss: 4.63517570496\n",
      "Epoch: 29 itrerations: 2000 Training Loss: 4.61167383194\n",
      "Epoch: 29 itrerations: 2100 Training Loss: 4.58863639832\n",
      "Epoch: 29 itrerations: 2200 Training Loss: 4.5334854126\n",
      "Epoch: 29 itrerations: 2300 Training Loss: 4.57073497772\n",
      "Epoch: 29 itrerations: 2400 Training Loss: 4.54283475876\n",
      "Epoch: 29 itrerations: 2500 Training Loss: 4.50802946091\n",
      "Epoch: 29 itrerations: 2600 Training Loss: 4.48180770874\n",
      "Epoch: 29 itrerations: 2700 Training Loss: 4.51245498657\n",
      "Epoch: 29 itrerations: 2800 Training Loss: 4.48420333862\n",
      "Epoch: 29 itrerations: 2900 Training Loss: 4.44973659515\n",
      "Epoch: 29 itrerations: 3000 Training Loss: 4.49893856049\n",
      "Epoch: 29 itrerations: 3100 Training Loss: 4.47992372513\n",
      "Epoch: 29 itrerations: 3200 Training Loss: 4.49159955978\n",
      "Epoch: 29 itrerations: 3300 Training Loss: 4.46069431305\n",
      "Epoch: 29 itrerations: 3400 Training Loss: 4.45909881592\n",
      "Epoch: 29 itrerations: 3500 Training Loss: 4.47115039825\n",
      "Epoch: 29 itrerations: 3600 Training Loss: 4.46763658524\n",
      "Epoch: 29 itrerations: 3700 Training Loss: 4.45067214966\n",
      "Epoch: 29 itrerations: 3800 Training Loss: 4.46928358078\n",
      "Epoch: 29 itrerations: 3900 Training Loss: 4.44733285904\n",
      "Epoch: 29 itrerations: 4000 Training Loss: 4.45002126694\n",
      "Epoch: 29 itrerations: 4100 Training Loss: 4.4352812767\n",
      "Epoch: 29 itrerations: 4200 Training Loss: 4.42221736908\n",
      "Epoch: 29 itrerations: 4300 Training Loss: 4.42416763306\n",
      "Epoch: 29 itrerations: 4400 Training Loss: 4.44888591766\n",
      "Epoch: 29 itrerations: 4500 Training Loss: 4.44749736786\n",
      "Epoch: 29 itrerations: 4600 Training Loss: 4.44574117661\n",
      "Epoch: 29 itrerations: 4700 Training Loss: 4.4542131424\n",
      "Epoch: 29 itrerations: 4800 Training Loss: 4.44882106781\n",
      "Epoch: 29 itrerations: 4900 Training Loss: 4.44245433807\n",
      "Epoch: 29 itrerations: 5000 Training Loss: 4.42285060883\n",
      "Epoch: 29 itrerations: 5100 Training Loss: 4.41187620163\n",
      "Epoch: 29 itrerations: 5200 Training Loss: 4.41388654709\n",
      "Epoch: 29 itrerations: 5300 Training Loss: 4.41546106339\n",
      "Epoch: 29 itrerations: 5400 Training Loss: 4.43061828613\n",
      "Epoch: 29 itrerations: 5500 Training Loss: 4.42285442352\n",
      "Epoch: 29 itrerations: 5600 Training Loss: 4.43114280701\n",
      "Epoch: 29 itrerations: 5700 Training Loss: 4.43166303635\n",
      "Epoch: 29 itrerations: 5800 Training Loss: 4.43606615067\n",
      "Epoch: 29 itrerations: 5900 Training Loss: 4.44374847412\n",
      "Epoch: 29 itrerations: 6000 Training Loss: 4.43845653534\n",
      "Epoch: 29 itrerations: 6100 Training Loss: 4.42306280136\n",
      "Epoch: 29 itrerations: 6200 Training Loss: 4.42215299606\n",
      "Epoch: 29 itrerations: 6300 Training Loss: 4.42348337173\n",
      "Epoch: 29 itrerations: 6400 Training Loss: 4.41691684723\n",
      "Epoch: 29 itrerations: 6500 Training Loss: 4.4084444046\n",
      "Epoch: 29 itrerations: 6600 Training Loss: 4.40313911438\n",
      "Epoch: 29 itrerations: 6700 Training Loss: 4.41367197037\n",
      "Epoch: 29 itrerations: 6800 Training Loss: 4.42786502838\n",
      "Epoch: 29 itrerations: 6900 Training Loss: 4.43328619003\n",
      "Epoch: 29 itrerations: 7000 Training Loss: 4.4604678154\n",
      "Epoch: 29 itrerations: 7100 Training Loss: 4.47217321396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 itrerations: 7200 Training Loss: 4.46303796768\n",
      "Epoch: 29 itrerations: 7300 Training Loss: 4.46416950226\n",
      "Epoch: 29 itrerations: 7400 Training Loss: 4.45043420792\n",
      "Epoch: 29 itrerations: 7500 Training Loss: 4.45579862595\n",
      "Epoch: 29 itrerations: 7600 Training Loss: 4.44420623779\n",
      "Epoch: 29 itrerations: 7700 Training Loss: 4.43938589096\n",
      "Epoch: 29 itrerations: 7800 Training Loss: 4.44003486633\n",
      "Epoch: 29 itrerations: 7900 Training Loss: 4.44606971741\n",
      "Epoch: 29 itrerations: 8000 Training Loss: 4.44628190994\n",
      "Epoch: 29 itrerations: 8100 Training Loss: 4.44193220139\n",
      "Epoch: 29 itrerations: 8200 Training Loss: 4.44644260406\n",
      "Epoch: 29 itrerations: 8300 Training Loss: 4.45700407028\n",
      "Epoch: 29 itrerations: 8400 Training Loss: 4.45428276062\n",
      "Epoch: 29 itrerations: 8500 Training Loss: 4.45539808273\n",
      "Epoch: 29 itrerations: 8600 Training Loss: 4.45321416855\n",
      "Epoch: 29 itrerations: 8700 Training Loss: 4.44972610474\n",
      "Epoch: 29 itrerations: 8800 Training Loss: 4.44654941559\n",
      "Epoch: 29 itrerations: 8900 Training Loss: 4.44366455078\n",
      "Epoch: 29 itrerations: 9000 Training Loss: 4.45086669922\n",
      "Epoch: 29 itrerations: 9100 Training Loss: 4.44987487793\n",
      "Epoch: 29 itrerations: 9200 Training Loss: 4.45558643341\n",
      "Epoch: 29 itrerations: 9300 Training Loss: 4.4558930397\n",
      "Epoch: 29 itrerations: 9400 Training Loss: 4.44754457474\n",
      "Epoch: 29 itrerations: 9500 Training Loss: 4.44559144974\n",
      "Epoch: 29 itrerations: 9600 Training Loss: 4.45868396759\n",
      "Epoch: 29 itrerations: 9700 Training Loss: 4.46190929413\n",
      "Epoch: 29 itrerations: 9800 Training Loss: 4.45859575272\n",
      "Epoch: 29 itrerations: 9900 Training Loss: 4.45585775375\n",
      "Epoch: 29 itrerations: 10000 Training Loss: 4.455930233\n",
      "Epoch: 29 itrerations: 10100 Training Loss: 4.45246648788\n",
      "Epoch: 29 itrerations: 10200 Training Loss: 4.45085954666\n",
      "Epoch: 29 itrerations: 10300 Training Loss: 4.45426940918\n",
      "Epoch: 29 itrerations: 10400 Training Loss: 4.45082807541\n",
      "Epoch: 29 itrerations: 10500 Training Loss: 4.44121313095\n",
      "Epoch: 29 itrerations: 10600 Training Loss: 4.44398355484\n",
      "Epoch: 29 itrerations: 10700 Training Loss: 4.44447469711\n",
      "Epoch: 30 itrerations: 0 Training Loss: 9.71549129486\n",
      "Epoch: 30 itrerations: 100 Training Loss: 4.17664289474\n",
      "Epoch: 30 itrerations: 200 Training Loss: 3.74420261383\n",
      "Epoch: 30 itrerations: 300 Training Loss: 3.95057177544\n",
      "Epoch: 30 itrerations: 400 Training Loss: 3.93302965164\n",
      "Epoch: 30 itrerations: 500 Training Loss: 3.75525546074\n",
      "Epoch: 30 itrerations: 600 Training Loss: 4.01660585403\n",
      "Epoch: 30 itrerations: 700 Training Loss: 4.04939365387\n",
      "Epoch: 30 itrerations: 800 Training Loss: 4.27389669418\n",
      "Epoch: 30 itrerations: 900 Training Loss: 4.37520313263\n",
      "Epoch: 30 itrerations: 1000 Training Loss: 4.3858332634\n",
      "Epoch: 30 itrerations: 1100 Training Loss: 4.37792873383\n",
      "Epoch: 30 itrerations: 1200 Training Loss: 4.47265863419\n",
      "Epoch: 30 itrerations: 1300 Training Loss: 4.43336868286\n",
      "Epoch: 30 itrerations: 1400 Training Loss: 4.50248861313\n",
      "Epoch: 30 itrerations: 1500 Training Loss: 4.50436639786\n",
      "Epoch: 30 itrerations: 1600 Training Loss: 4.50135660172\n",
      "Epoch: 30 itrerations: 1700 Training Loss: 4.50736141205\n",
      "Epoch: 30 itrerations: 1800 Training Loss: 4.52949142456\n",
      "Epoch: 30 itrerations: 1900 Training Loss: 4.56577348709\n",
      "Epoch: 30 itrerations: 2000 Training Loss: 4.58515739441\n",
      "Epoch: 30 itrerations: 2100 Training Loss: 4.54473352432\n",
      "Epoch: 30 itrerations: 2200 Training Loss: 4.49028205872\n",
      "Epoch: 30 itrerations: 2300 Training Loss: 4.51525068283\n",
      "Epoch: 30 itrerations: 2400 Training Loss: 4.48733663559\n",
      "Epoch: 30 itrerations: 2500 Training Loss: 4.46561956406\n",
      "Epoch: 30 itrerations: 2600 Training Loss: 4.46699714661\n",
      "Epoch: 30 itrerations: 2700 Training Loss: 4.48223114014\n",
      "Epoch: 30 itrerations: 2800 Training Loss: 4.42813444138\n",
      "Epoch: 30 itrerations: 2900 Training Loss: 4.39696025848\n",
      "Epoch: 30 itrerations: 3000 Training Loss: 4.43306922913\n",
      "Epoch: 30 itrerations: 3100 Training Loss: 4.42621278763\n",
      "Epoch: 30 itrerations: 3200 Training Loss: 4.43105173111\n",
      "Epoch: 30 itrerations: 3300 Training Loss: 4.41700315475\n",
      "Epoch: 30 itrerations: 3400 Training Loss: 4.40746498108\n",
      "Epoch: 30 itrerations: 3500 Training Loss: 4.43128585815\n",
      "Epoch: 30 itrerations: 3600 Training Loss: 4.43709039688\n",
      "Epoch: 30 itrerations: 3700 Training Loss: 4.41952133179\n",
      "Epoch: 30 itrerations: 3800 Training Loss: 4.44103527069\n",
      "Epoch: 30 itrerations: 3900 Training Loss: 4.44796085358\n",
      "Epoch: 30 itrerations: 4000 Training Loss: 4.4503698349\n",
      "Epoch: 30 itrerations: 4100 Training Loss: 4.45230770111\n",
      "Epoch: 30 itrerations: 4200 Training Loss: 4.43891143799\n",
      "Epoch: 30 itrerations: 4300 Training Loss: 4.43732643127\n",
      "Epoch: 30 itrerations: 4400 Training Loss: 4.46078395844\n",
      "Epoch: 30 itrerations: 4500 Training Loss: 4.46118402481\n",
      "Epoch: 30 itrerations: 4600 Training Loss: 4.46799850464\n",
      "Epoch: 30 itrerations: 4700 Training Loss: 4.45822095871\n",
      "Epoch: 30 itrerations: 4800 Training Loss: 4.43964958191\n",
      "Epoch: 30 itrerations: 4900 Training Loss: 4.4437122345\n",
      "Epoch: 30 itrerations: 5000 Training Loss: 4.43425750732\n",
      "Epoch: 30 itrerations: 5100 Training Loss: 4.41769170761\n",
      "Epoch: 30 itrerations: 5200 Training Loss: 4.41323328018\n",
      "Epoch: 30 itrerations: 5300 Training Loss: 4.40867805481\n",
      "Epoch: 30 itrerations: 5400 Training Loss: 4.42859888077\n",
      "Epoch: 30 itrerations: 5500 Training Loss: 4.42169332504\n",
      "Epoch: 30 itrerations: 5600 Training Loss: 4.41889715195\n",
      "Epoch: 30 itrerations: 5700 Training Loss: 4.40974569321\n",
      "Epoch: 30 itrerations: 5800 Training Loss: 4.40506124496\n",
      "Epoch: 30 itrerations: 5900 Training Loss: 4.40306854248\n",
      "Epoch: 30 itrerations: 6000 Training Loss: 4.39666891098\n",
      "Epoch: 30 itrerations: 6100 Training Loss: 4.38182830811\n",
      "Epoch: 30 itrerations: 6200 Training Loss: 4.38328933716\n",
      "Epoch: 30 itrerations: 6300 Training Loss: 4.3798995018\n",
      "Epoch: 30 itrerations: 6400 Training Loss: 4.38182544708\n",
      "Epoch: 30 itrerations: 6500 Training Loss: 4.39129924774\n",
      "Epoch: 30 itrerations: 6600 Training Loss: 4.38627338409\n",
      "Epoch: 30 itrerations: 6700 Training Loss: 4.38408899307\n",
      "Epoch: 30 itrerations: 6800 Training Loss: 4.39012527466\n",
      "Epoch: 30 itrerations: 6900 Training Loss: 4.40719223022\n",
      "Epoch: 30 itrerations: 7000 Training Loss: 4.42149209976\n",
      "Epoch: 30 itrerations: 7100 Training Loss: 4.442507267\n",
      "Epoch: 30 itrerations: 7200 Training Loss: 4.4301199913\n",
      "Epoch: 30 itrerations: 7300 Training Loss: 4.43189716339\n",
      "Epoch: 30 itrerations: 7400 Training Loss: 4.42671251297\n",
      "Epoch: 30 itrerations: 7500 Training Loss: 4.43697977066\n",
      "Epoch: 30 itrerations: 7600 Training Loss: 4.43113660812\n",
      "Epoch: 30 itrerations: 7700 Training Loss: 4.41485071182\n",
      "Epoch: 30 itrerations: 7800 Training Loss: 4.41670322418\n",
      "Epoch: 30 itrerations: 7900 Training Loss: 4.42258214951\n",
      "Epoch: 30 itrerations: 8000 Training Loss: 4.42157936096\n",
      "Epoch: 30 itrerations: 8100 Training Loss: 4.41254425049\n",
      "Epoch: 30 itrerations: 8200 Training Loss: 4.41820907593\n",
      "Epoch: 30 itrerations: 8300 Training Loss: 4.42762422562\n",
      "Epoch: 30 itrerations: 8400 Training Loss: 4.43397474289\n",
      "Epoch: 30 itrerations: 8500 Training Loss: 4.44082212448\n",
      "Epoch: 30 itrerations: 8600 Training Loss: 4.44055509567\n",
      "Epoch: 30 itrerations: 8700 Training Loss: 4.4361000061\n",
      "Epoch: 30 itrerations: 8800 Training Loss: 4.42592716217\n",
      "Epoch: 30 itrerations: 8900 Training Loss: 4.4237947464\n",
      "Epoch: 30 itrerations: 9000 Training Loss: 4.4348487854\n",
      "Epoch: 30 itrerations: 9100 Training Loss: 4.43835020065\n",
      "Epoch: 30 itrerations: 9200 Training Loss: 4.44162893295\n",
      "Epoch: 30 itrerations: 9300 Training Loss: 4.43419075012\n",
      "Epoch: 30 itrerations: 9400 Training Loss: 4.42246055603\n",
      "Epoch: 30 itrerations: 9500 Training Loss: 4.41883277893\n",
      "Epoch: 30 itrerations: 9600 Training Loss: 4.43084812164\n",
      "Epoch: 30 itrerations: 9700 Training Loss: 4.43000078201\n",
      "Epoch: 30 itrerations: 9800 Training Loss: 4.42593050003\n",
      "Epoch: 30 itrerations: 9900 Training Loss: 4.42493581772\n",
      "Epoch: 30 itrerations: 10000 Training Loss: 4.4315700531\n",
      "Epoch: 30 itrerations: 10100 Training Loss: 4.43225383759\n",
      "Epoch: 30 itrerations: 10200 Training Loss: 4.43220567703\n",
      "Epoch: 30 itrerations: 10300 Training Loss: 4.43159103394\n",
      "Epoch: 30 itrerations: 10400 Training Loss: 4.42839622498\n",
      "Epoch: 30 itrerations: 10500 Training Loss: 4.42314481735\n",
      "Epoch: 30 itrerations: 10600 Training Loss: 4.42556524277\n",
      "Epoch: 30 itrerations: 10700 Training Loss: 4.43165254593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 itrerations: 0 Training Loss: 0.934552848339\n",
      "Epoch: 31 itrerations: 100 Training Loss: 3.49889802933\n",
      "Epoch: 31 itrerations: 200 Training Loss: 3.36347579956\n",
      "Epoch: 31 itrerations: 300 Training Loss: 3.67081618309\n",
      "Epoch: 31 itrerations: 400 Training Loss: 3.79548382759\n",
      "Epoch: 31 itrerations: 500 Training Loss: 3.65388655663\n",
      "Epoch: 31 itrerations: 600 Training Loss: 3.94477701187\n",
      "Epoch: 31 itrerations: 700 Training Loss: 3.99297118187\n",
      "Epoch: 31 itrerations: 800 Training Loss: 4.20854711533\n",
      "Epoch: 31 itrerations: 900 Training Loss: 4.26414251328\n",
      "Epoch: 31 itrerations: 1000 Training Loss: 4.29364490509\n",
      "Epoch: 31 itrerations: 1100 Training Loss: 4.29250192642\n",
      "Epoch: 31 itrerations: 1200 Training Loss: 4.3943066597\n",
      "Epoch: 31 itrerations: 1300 Training Loss: 4.34878015518\n",
      "Epoch: 31 itrerations: 1400 Training Loss: 4.42867422104\n",
      "Epoch: 31 itrerations: 1500 Training Loss: 4.41074323654\n",
      "Epoch: 31 itrerations: 1600 Training Loss: 4.40977621078\n",
      "Epoch: 31 itrerations: 1700 Training Loss: 4.40026426315\n",
      "Epoch: 31 itrerations: 1800 Training Loss: 4.41034460068\n",
      "Epoch: 31 itrerations: 1900 Training Loss: 4.46114206314\n",
      "Epoch: 31 itrerations: 2000 Training Loss: 4.49715662003\n",
      "Epoch: 31 itrerations: 2100 Training Loss: 4.46560239792\n",
      "Epoch: 31 itrerations: 2200 Training Loss: 4.42788171768\n",
      "Epoch: 31 itrerations: 2300 Training Loss: 4.42640018463\n",
      "Epoch: 31 itrerations: 2400 Training Loss: 4.39257669449\n",
      "Epoch: 31 itrerations: 2500 Training Loss: 4.34231328964\n",
      "Epoch: 31 itrerations: 2600 Training Loss: 4.35123491287\n",
      "Epoch: 31 itrerations: 2700 Training Loss: 4.39593267441\n",
      "Epoch: 31 itrerations: 2800 Training Loss: 4.339594841\n",
      "Epoch: 31 itrerations: 2900 Training Loss: 4.30272483826\n",
      "Epoch: 31 itrerations: 3000 Training Loss: 4.35954427719\n",
      "Epoch: 31 itrerations: 3100 Training Loss: 4.34475660324\n",
      "Epoch: 31 itrerations: 3200 Training Loss: 4.35104990005\n",
      "Epoch: 31 itrerations: 3300 Training Loss: 4.33523035049\n",
      "Epoch: 31 itrerations: 3400 Training Loss: 4.33033084869\n",
      "Epoch: 31 itrerations: 3500 Training Loss: 4.35314178467\n",
      "Epoch: 31 itrerations: 3600 Training Loss: 4.36909198761\n",
      "Epoch: 31 itrerations: 3700 Training Loss: 4.36937761307\n",
      "Epoch: 31 itrerations: 3800 Training Loss: 4.37843132019\n",
      "Epoch: 31 itrerations: 3900 Training Loss: 4.36097145081\n",
      "Epoch: 31 itrerations: 4000 Training Loss: 4.36962032318\n",
      "Epoch: 31 itrerations: 4100 Training Loss: 4.37067079544\n",
      "Epoch: 31 itrerations: 4200 Training Loss: 4.36265468597\n",
      "Epoch: 31 itrerations: 4300 Training Loss: 4.36161613464\n",
      "Epoch: 31 itrerations: 4400 Training Loss: 4.38016414642\n",
      "Epoch: 31 itrerations: 4500 Training Loss: 4.37950944901\n",
      "Epoch: 31 itrerations: 4600 Training Loss: 4.37875938416\n",
      "Epoch: 31 itrerations: 4700 Training Loss: 4.37779283524\n",
      "Epoch: 31 itrerations: 4800 Training Loss: 4.3746213913\n",
      "Epoch: 31 itrerations: 4900 Training Loss: 4.35990667343\n",
      "Epoch: 31 itrerations: 5000 Training Loss: 4.33885145187\n",
      "Epoch: 31 itrerations: 5100 Training Loss: 4.3284778595\n",
      "Epoch: 31 itrerations: 5200 Training Loss: 4.32149744034\n",
      "Epoch: 31 itrerations: 5300 Training Loss: 4.30806303024\n",
      "Epoch: 31 itrerations: 5400 Training Loss: 4.32252216339\n",
      "Epoch: 31 itrerations: 5500 Training Loss: 4.32488536835\n",
      "Epoch: 31 itrerations: 5600 Training Loss: 4.32628202438\n",
      "Epoch: 31 itrerations: 5700 Training Loss: 4.324614048\n",
      "Epoch: 31 itrerations: 5800 Training Loss: 4.31961488724\n",
      "Epoch: 31 itrerations: 5900 Training Loss: 4.33227777481\n",
      "Epoch: 31 itrerations: 6000 Training Loss: 4.32537984848\n",
      "Epoch: 31 itrerations: 6100 Training Loss: 4.32865715027\n",
      "Epoch: 31 itrerations: 6200 Training Loss: 4.33164453506\n",
      "Epoch: 31 itrerations: 6300 Training Loss: 4.33200836182\n",
      "Epoch: 31 itrerations: 6400 Training Loss: 4.32756853104\n",
      "Epoch: 31 itrerations: 6500 Training Loss: 4.32189893723\n",
      "Epoch: 31 itrerations: 6600 Training Loss: 4.31445932388\n",
      "Epoch: 31 itrerations: 6700 Training Loss: 4.3241891861\n",
      "Epoch: 31 itrerations: 6800 Training Loss: 4.33951282501\n",
      "Epoch: 31 itrerations: 6900 Training Loss: 4.35122203827\n",
      "Epoch: 31 itrerations: 7000 Training Loss: 4.3753194809\n",
      "Epoch: 31 itrerations: 7100 Training Loss: 4.38638496399\n",
      "Epoch: 31 itrerations: 7200 Training Loss: 4.3723077774\n",
      "Epoch: 31 itrerations: 7300 Training Loss: 4.35947608948\n",
      "Epoch: 31 itrerations: 7400 Training Loss: 4.35204696655\n",
      "Epoch: 31 itrerations: 7500 Training Loss: 4.35115623474\n",
      "Epoch: 31 itrerations: 7600 Training Loss: 4.34652519226\n",
      "Epoch: 31 itrerations: 7700 Training Loss: 4.34256601334\n",
      "Epoch: 31 itrerations: 7800 Training Loss: 4.34427642822\n",
      "Epoch: 31 itrerations: 7900 Training Loss: 4.35412073135\n",
      "Epoch: 31 itrerations: 8000 Training Loss: 4.35317993164\n",
      "Epoch: 31 itrerations: 8100 Training Loss: 4.34544849396\n",
      "Epoch: 31 itrerations: 8200 Training Loss: 4.35311460495\n",
      "Epoch: 31 itrerations: 8300 Training Loss: 4.35936355591\n",
      "Epoch: 31 itrerations: 8400 Training Loss: 4.36088132858\n",
      "Epoch: 31 itrerations: 8500 Training Loss: 4.36762619019\n",
      "Epoch: 31 itrerations: 8600 Training Loss: 4.36556577682\n",
      "Epoch: 31 itrerations: 8700 Training Loss: 4.36289548874\n",
      "Epoch: 31 itrerations: 8800 Training Loss: 4.36390829086\n",
      "Epoch: 31 itrerations: 8900 Training Loss: 4.36653661728\n",
      "Epoch: 31 itrerations: 9000 Training Loss: 4.37518501282\n",
      "Epoch: 31 itrerations: 9100 Training Loss: 4.37090539932\n",
      "Epoch: 31 itrerations: 9200 Training Loss: 4.37691354752\n",
      "Epoch: 31 itrerations: 9300 Training Loss: 4.37376785278\n",
      "Epoch: 31 itrerations: 9400 Training Loss: 4.36721515656\n",
      "Epoch: 31 itrerations: 9500 Training Loss: 4.36916208267\n",
      "Epoch: 31 itrerations: 9600 Training Loss: 4.38550329208\n",
      "Epoch: 31 itrerations: 9700 Training Loss: 4.38222408295\n",
      "Epoch: 31 itrerations: 9800 Training Loss: 4.37681722641\n",
      "Epoch: 31 itrerations: 9900 Training Loss: 4.37755441666\n",
      "Epoch: 31 itrerations: 10000 Training Loss: 4.38640022278\n",
      "Epoch: 31 itrerations: 10100 Training Loss: 4.38175010681\n",
      "Epoch: 31 itrerations: 10200 Training Loss: 4.37933301926\n",
      "Epoch: 31 itrerations: 10300 Training Loss: 4.37972021103\n",
      "Epoch: 31 itrerations: 10400 Training Loss: 4.3756685257\n",
      "Epoch: 31 itrerations: 10500 Training Loss: 4.36776256561\n",
      "Epoch: 31 itrerations: 10600 Training Loss: 4.36655378342\n",
      "Epoch: 31 itrerations: 10700 Training Loss: 4.37346553802\n",
      "Epoch: 32 itrerations: 0 Training Loss: 2.26461625099\n",
      "Epoch: 32 itrerations: 100 Training Loss: 4.68220901489\n",
      "Epoch: 32 itrerations: 200 Training Loss: 3.96392941475\n",
      "Epoch: 32 itrerations: 300 Training Loss: 4.02535390854\n",
      "Epoch: 32 itrerations: 400 Training Loss: 4.04177618027\n",
      "Epoch: 32 itrerations: 500 Training Loss: 3.71088671684\n",
      "Epoch: 32 itrerations: 600 Training Loss: 3.9799284935\n",
      "Epoch: 32 itrerations: 700 Training Loss: 4.05723524094\n",
      "Epoch: 32 itrerations: 800 Training Loss: 4.23065376282\n",
      "Epoch: 32 itrerations: 900 Training Loss: 4.22225427628\n",
      "Epoch: 32 itrerations: 1000 Training Loss: 4.23078680038\n",
      "Epoch: 32 itrerations: 1100 Training Loss: 4.20040655136\n",
      "Epoch: 32 itrerations: 1200 Training Loss: 4.34383296967\n",
      "Epoch: 32 itrerations: 1300 Training Loss: 4.35836601257\n",
      "Epoch: 32 itrerations: 1400 Training Loss: 4.45908212662\n",
      "Epoch: 32 itrerations: 1500 Training Loss: 4.46728277206\n",
      "Epoch: 32 itrerations: 1600 Training Loss: 4.46811199188\n",
      "Epoch: 32 itrerations: 1700 Training Loss: 4.4764175415\n",
      "Epoch: 32 itrerations: 1800 Training Loss: 4.50206565857\n",
      "Epoch: 32 itrerations: 1900 Training Loss: 4.52446269989\n",
      "Epoch: 32 itrerations: 2000 Training Loss: 4.53617429733\n",
      "Epoch: 32 itrerations: 2100 Training Loss: 4.49278640747\n",
      "Epoch: 32 itrerations: 2200 Training Loss: 4.47305011749\n",
      "Epoch: 32 itrerations: 2300 Training Loss: 4.49490976334\n",
      "Epoch: 32 itrerations: 2400 Training Loss: 4.46048688889\n",
      "Epoch: 32 itrerations: 2500 Training Loss: 4.43158531189\n",
      "Epoch: 32 itrerations: 2600 Training Loss: 4.41718244553\n",
      "Epoch: 32 itrerations: 2700 Training Loss: 4.44156599045\n",
      "Epoch: 32 itrerations: 2800 Training Loss: 4.39493656158\n",
      "Epoch: 32 itrerations: 2900 Training Loss: 4.35985517502\n",
      "Epoch: 32 itrerations: 3000 Training Loss: 4.39229488373\n",
      "Epoch: 32 itrerations: 3100 Training Loss: 4.37789583206\n",
      "Epoch: 32 itrerations: 3200 Training Loss: 4.36690568924\n",
      "Epoch: 32 itrerations: 3300 Training Loss: 4.34211969376\n",
      "Epoch: 32 itrerations: 3400 Training Loss: 4.33746051788\n",
      "Epoch: 32 itrerations: 3500 Training Loss: 4.36325645447\n",
      "Epoch: 32 itrerations: 3600 Training Loss: 4.38825035095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 itrerations: 3700 Training Loss: 4.39010000229\n",
      "Epoch: 32 itrerations: 3800 Training Loss: 4.41721534729\n",
      "Epoch: 32 itrerations: 3900 Training Loss: 4.39861631393\n",
      "Epoch: 32 itrerations: 4000 Training Loss: 4.41284227371\n",
      "Epoch: 32 itrerations: 4100 Training Loss: 4.39086389542\n",
      "Epoch: 32 itrerations: 4200 Training Loss: 4.38065433502\n",
      "Epoch: 32 itrerations: 4300 Training Loss: 4.38930749893\n",
      "Epoch: 32 itrerations: 4400 Training Loss: 4.39480209351\n",
      "Epoch: 32 itrerations: 4500 Training Loss: 4.39608669281\n",
      "Epoch: 32 itrerations: 4600 Training Loss: 4.39032268524\n",
      "Epoch: 32 itrerations: 4700 Training Loss: 4.39396047592\n",
      "Epoch: 32 itrerations: 4800 Training Loss: 4.37987995148\n",
      "Epoch: 32 itrerations: 4900 Training Loss: 4.37122583389\n",
      "Epoch: 32 itrerations: 5000 Training Loss: 4.35071516037\n",
      "Epoch: 32 itrerations: 5100 Training Loss: 4.34061479568\n",
      "Epoch: 32 itrerations: 5200 Training Loss: 4.34320020676\n",
      "Epoch: 32 itrerations: 5300 Training Loss: 4.33574867249\n",
      "Epoch: 32 itrerations: 5400 Training Loss: 4.36882781982\n",
      "Epoch: 32 itrerations: 5500 Training Loss: 4.36005592346\n",
      "Epoch: 32 itrerations: 5600 Training Loss: 4.37168359756\n",
      "Epoch: 32 itrerations: 5700 Training Loss: 4.36660909653\n",
      "Epoch: 32 itrerations: 5800 Training Loss: 4.37379598618\n",
      "Epoch: 32 itrerations: 5900 Training Loss: 4.38114833832\n",
      "Epoch: 32 itrerations: 6000 Training Loss: 4.37954998016\n",
      "Epoch: 32 itrerations: 6100 Training Loss: 4.36330366135\n",
      "Epoch: 32 itrerations: 6200 Training Loss: 4.38495492935\n",
      "Epoch: 32 itrerations: 6300 Training Loss: 4.38890171051\n",
      "Epoch: 32 itrerations: 6400 Training Loss: 4.39044809341\n",
      "Epoch: 32 itrerations: 6500 Training Loss: 4.38229417801\n",
      "Epoch: 32 itrerations: 6600 Training Loss: 4.38254022598\n",
      "Epoch: 32 itrerations: 6700 Training Loss: 4.3846321106\n",
      "Epoch: 32 itrerations: 6800 Training Loss: 4.39044094086\n",
      "Epoch: 32 itrerations: 6900 Training Loss: 4.40915775299\n",
      "Epoch: 32 itrerations: 7000 Training Loss: 4.42596435547\n",
      "Epoch: 32 itrerations: 7100 Training Loss: 4.4280257225\n",
      "Epoch: 32 itrerations: 7200 Training Loss: 4.41648101807\n",
      "Epoch: 32 itrerations: 7300 Training Loss: 4.43139410019\n",
      "Epoch: 32 itrerations: 7400 Training Loss: 4.42165565491\n",
      "Epoch: 32 itrerations: 7500 Training Loss: 4.42591381073\n",
      "Epoch: 32 itrerations: 7600 Training Loss: 4.4215555191\n",
      "Epoch: 32 itrerations: 7700 Training Loss: 4.40858745575\n",
      "Epoch: 32 itrerations: 7800 Training Loss: 4.4022269249\n",
      "Epoch: 32 itrerations: 7900 Training Loss: 4.40895223618\n",
      "Epoch: 32 itrerations: 8000 Training Loss: 4.40471124649\n",
      "Epoch: 32 itrerations: 8100 Training Loss: 4.39800357819\n",
      "Epoch: 32 itrerations: 8200 Training Loss: 4.39962291718\n",
      "Epoch: 32 itrerations: 8300 Training Loss: 4.40766143799\n",
      "Epoch: 32 itrerations: 8400 Training Loss: 4.4106054306\n",
      "Epoch: 32 itrerations: 8500 Training Loss: 4.40983104706\n",
      "Epoch: 32 itrerations: 8600 Training Loss: 4.40227890015\n",
      "Epoch: 32 itrerations: 8700 Training Loss: 4.40264177322\n",
      "Epoch: 32 itrerations: 8800 Training Loss: 4.39585542679\n",
      "Epoch: 32 itrerations: 8900 Training Loss: 4.38244485855\n",
      "Epoch: 32 itrerations: 9000 Training Loss: 4.3898434639\n",
      "Epoch: 32 itrerations: 9100 Training Loss: 4.3911857605\n",
      "Epoch: 32 itrerations: 9200 Training Loss: 4.39981174469\n",
      "Epoch: 32 itrerations: 9300 Training Loss: 4.39559936523\n",
      "Epoch: 32 itrerations: 9400 Training Loss: 4.39180469513\n",
      "Epoch: 32 itrerations: 9500 Training Loss: 4.39019489288\n",
      "Epoch: 32 itrerations: 9600 Training Loss: 4.40444278717\n",
      "Epoch: 32 itrerations: 9700 Training Loss: 4.4056968689\n",
      "Epoch: 32 itrerations: 9800 Training Loss: 4.40201473236\n",
      "Epoch: 32 itrerations: 9900 Training Loss: 4.40149641037\n",
      "Epoch: 32 itrerations: 10000 Training Loss: 4.42020463943\n",
      "Epoch: 32 itrerations: 10100 Training Loss: 4.42075586319\n",
      "Epoch: 32 itrerations: 10200 Training Loss: 4.42020368576\n",
      "Epoch: 32 itrerations: 10300 Training Loss: 4.42267513275\n",
      "Epoch: 32 itrerations: 10400 Training Loss: 4.42041301727\n",
      "Epoch: 32 itrerations: 10500 Training Loss: 4.41067075729\n",
      "Epoch: 32 itrerations: 10600 Training Loss: 4.41755533218\n",
      "Epoch: 32 itrerations: 10700 Training Loss: 4.42078733444\n",
      "Epoch: 33 itrerations: 0 Training Loss: 9.11949253082\n",
      "Epoch: 33 itrerations: 100 Training Loss: 4.24496221542\n",
      "Epoch: 33 itrerations: 200 Training Loss: 4.10866308212\n",
      "Epoch: 33 itrerations: 300 Training Loss: 4.23121213913\n",
      "Epoch: 33 itrerations: 400 Training Loss: 4.18735218048\n",
      "Epoch: 33 itrerations: 500 Training Loss: 3.9024002552\n",
      "Epoch: 33 itrerations: 600 Training Loss: 4.17015838623\n",
      "Epoch: 33 itrerations: 700 Training Loss: 4.26440620422\n",
      "Epoch: 33 itrerations: 800 Training Loss: 4.39134693146\n",
      "Epoch: 33 itrerations: 900 Training Loss: 4.39989376068\n",
      "Epoch: 33 itrerations: 1000 Training Loss: 4.38457536697\n",
      "Epoch: 33 itrerations: 1100 Training Loss: 4.37072181702\n",
      "Epoch: 33 itrerations: 1200 Training Loss: 4.4964723587\n",
      "Epoch: 33 itrerations: 1300 Training Loss: 4.49537754059\n",
      "Epoch: 33 itrerations: 1400 Training Loss: 4.5812048912\n",
      "Epoch: 33 itrerations: 1500 Training Loss: 4.56030511856\n",
      "Epoch: 33 itrerations: 1600 Training Loss: 4.54812812805\n",
      "Epoch: 33 itrerations: 1700 Training Loss: 4.54578399658\n",
      "Epoch: 33 itrerations: 1800 Training Loss: 4.51600313187\n",
      "Epoch: 33 itrerations: 1900 Training Loss: 4.56441736221\n",
      "Epoch: 33 itrerations: 2000 Training Loss: 4.56163358688\n",
      "Epoch: 33 itrerations: 2100 Training Loss: 4.56249952316\n",
      "Epoch: 33 itrerations: 2200 Training Loss: 4.52230834961\n",
      "Epoch: 33 itrerations: 2300 Training Loss: 4.51960659027\n",
      "Epoch: 33 itrerations: 2400 Training Loss: 4.46402549744\n",
      "Epoch: 33 itrerations: 2500 Training Loss: 4.44361305237\n",
      "Epoch: 33 itrerations: 2600 Training Loss: 4.41205453873\n",
      "Epoch: 33 itrerations: 2700 Training Loss: 4.41029548645\n",
      "Epoch: 33 itrerations: 2800 Training Loss: 4.39503097534\n",
      "Epoch: 33 itrerations: 2900 Training Loss: 4.36347150803\n",
      "Epoch: 33 itrerations: 3000 Training Loss: 4.40549325943\n",
      "Epoch: 33 itrerations: 3100 Training Loss: 4.39257621765\n",
      "Epoch: 33 itrerations: 3200 Training Loss: 4.40768623352\n",
      "Epoch: 33 itrerations: 3300 Training Loss: 4.39405727386\n",
      "Epoch: 33 itrerations: 3400 Training Loss: 4.38815784454\n",
      "Epoch: 33 itrerations: 3500 Training Loss: 4.41496801376\n",
      "Epoch: 33 itrerations: 3600 Training Loss: 4.42546033859\n",
      "Epoch: 33 itrerations: 3700 Training Loss: 4.41523075104\n",
      "Epoch: 33 itrerations: 3800 Training Loss: 4.4213848114\n",
      "Epoch: 33 itrerations: 3900 Training Loss: 4.41097974777\n",
      "Epoch: 33 itrerations: 4000 Training Loss: 4.40631151199\n",
      "Epoch: 33 itrerations: 4100 Training Loss: 4.38640928268\n",
      "Epoch: 33 itrerations: 4200 Training Loss: 4.37175512314\n",
      "Epoch: 33 itrerations: 4300 Training Loss: 4.37843847275\n",
      "Epoch: 33 itrerations: 4400 Training Loss: 4.41243648529\n",
      "Epoch: 33 itrerations: 4500 Training Loss: 4.41057443619\n",
      "Epoch: 33 itrerations: 4600 Training Loss: 4.41007566452\n",
      "Epoch: 33 itrerations: 4700 Training Loss: 4.41360616684\n",
      "Epoch: 33 itrerations: 4800 Training Loss: 4.40208673477\n",
      "Epoch: 33 itrerations: 4900 Training Loss: 4.38648986816\n",
      "Epoch: 33 itrerations: 5000 Training Loss: 4.36795759201\n",
      "Epoch: 33 itrerations: 5100 Training Loss: 4.36907529831\n",
      "Epoch: 33 itrerations: 5200 Training Loss: 4.38009023666\n",
      "Epoch: 33 itrerations: 5300 Training Loss: 4.38089513779\n",
      "Epoch: 33 itrerations: 5400 Training Loss: 4.39007616043\n",
      "Epoch: 33 itrerations: 5500 Training Loss: 4.38698577881\n",
      "Epoch: 33 itrerations: 5600 Training Loss: 4.39846754074\n",
      "Epoch: 33 itrerations: 5700 Training Loss: 4.40631055832\n",
      "Epoch: 33 itrerations: 5800 Training Loss: 4.4140162468\n",
      "Epoch: 33 itrerations: 5900 Training Loss: 4.4116153717\n",
      "Epoch: 33 itrerations: 6000 Training Loss: 4.41066837311\n",
      "Epoch: 33 itrerations: 6100 Training Loss: 4.40558624268\n",
      "Epoch: 33 itrerations: 6200 Training Loss: 4.41236448288\n",
      "Epoch: 33 itrerations: 6300 Training Loss: 4.41135263443\n",
      "Epoch: 33 itrerations: 6400 Training Loss: 4.41393232346\n",
      "Epoch: 33 itrerations: 6500 Training Loss: 4.40625190735\n",
      "Epoch: 33 itrerations: 6600 Training Loss: 4.39504957199\n",
      "Epoch: 33 itrerations: 6700 Training Loss: 4.40586805344\n",
      "Epoch: 33 itrerations: 6800 Training Loss: 4.41224575043\n",
      "Epoch: 33 itrerations: 6900 Training Loss: 4.42494297028\n",
      "Epoch: 33 itrerations: 7000 Training Loss: 4.44618463516\n",
      "Epoch: 33 itrerations: 7100 Training Loss: 4.45577001572\n",
      "Epoch: 33 itrerations: 7200 Training Loss: 4.44288492203\n",
      "Epoch: 33 itrerations: 7300 Training Loss: 4.45000982285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 itrerations: 7400 Training Loss: 4.45019769669\n",
      "Epoch: 33 itrerations: 7500 Training Loss: 4.44587993622\n",
      "Epoch: 33 itrerations: 7600 Training Loss: 4.43772268295\n",
      "Epoch: 33 itrerations: 7700 Training Loss: 4.421189785\n",
      "Epoch: 33 itrerations: 7800 Training Loss: 4.42775821686\n",
      "Epoch: 33 itrerations: 7900 Training Loss: 4.43719053268\n",
      "Epoch: 33 itrerations: 8000 Training Loss: 4.42913866043\n",
      "Epoch: 33 itrerations: 8100 Training Loss: 4.41707897186\n",
      "Epoch: 33 itrerations: 8200 Training Loss: 4.42401313782\n",
      "Epoch: 33 itrerations: 8300 Training Loss: 4.43421030045\n",
      "Epoch: 33 itrerations: 8400 Training Loss: 4.43342542648\n",
      "Epoch: 33 itrerations: 8500 Training Loss: 4.43515491486\n",
      "Epoch: 33 itrerations: 8600 Training Loss: 4.42533874512\n",
      "Epoch: 33 itrerations: 8700 Training Loss: 4.42730569839\n",
      "Epoch: 33 itrerations: 8800 Training Loss: 4.42358398438\n",
      "Epoch: 33 itrerations: 8900 Training Loss: 4.41776323318\n",
      "Epoch: 33 itrerations: 9000 Training Loss: 4.42102575302\n",
      "Epoch: 33 itrerations: 9100 Training Loss: 4.42212867737\n",
      "Epoch: 33 itrerations: 9200 Training Loss: 4.42198467255\n",
      "Epoch: 33 itrerations: 9300 Training Loss: 4.41928863525\n",
      "Epoch: 33 itrerations: 9400 Training Loss: 4.40667152405\n",
      "Epoch: 33 itrerations: 9500 Training Loss: 4.40769481659\n",
      "Epoch: 33 itrerations: 9600 Training Loss: 4.41431045532\n",
      "Epoch: 33 itrerations: 9700 Training Loss: 4.41735124588\n",
      "Epoch: 33 itrerations: 9800 Training Loss: 4.4182972908\n",
      "Epoch: 33 itrerations: 9900 Training Loss: 4.41662025452\n",
      "Epoch: 33 itrerations: 10000 Training Loss: 4.43600893021\n",
      "Epoch: 33 itrerations: 10100 Training Loss: 4.43666267395\n",
      "Epoch: 33 itrerations: 10200 Training Loss: 4.43083715439\n",
      "Epoch: 33 itrerations: 10300 Training Loss: 4.42892742157\n",
      "Epoch: 33 itrerations: 10400 Training Loss: 4.42573785782\n",
      "Epoch: 33 itrerations: 10500 Training Loss: 4.41885900497\n",
      "Epoch: 33 itrerations: 10600 Training Loss: 4.42112541199\n",
      "Epoch: 33 itrerations: 10700 Training Loss: 4.43502569199\n",
      "Epoch: 34 itrerations: 0 Training Loss: 0.860717952251\n",
      "Epoch: 34 itrerations: 100 Training Loss: 4.11955213547\n",
      "Epoch: 34 itrerations: 200 Training Loss: 3.85024642944\n",
      "Epoch: 34 itrerations: 300 Training Loss: 4.12264299393\n",
      "Epoch: 34 itrerations: 400 Training Loss: 4.16518449783\n",
      "Epoch: 34 itrerations: 500 Training Loss: 3.78793287277\n",
      "Epoch: 34 itrerations: 600 Training Loss: 4.01286554337\n",
      "Epoch: 34 itrerations: 700 Training Loss: 4.05040407181\n",
      "Epoch: 34 itrerations: 800 Training Loss: 4.3000998497\n",
      "Epoch: 34 itrerations: 900 Training Loss: 4.31539535522\n",
      "Epoch: 34 itrerations: 1000 Training Loss: 4.33321332932\n",
      "Epoch: 34 itrerations: 1100 Training Loss: 4.28428697586\n",
      "Epoch: 34 itrerations: 1200 Training Loss: 4.39823961258\n",
      "Epoch: 34 itrerations: 1300 Training Loss: 4.40345859528\n",
      "Epoch: 34 itrerations: 1400 Training Loss: 4.44799566269\n",
      "Epoch: 34 itrerations: 1500 Training Loss: 4.44671773911\n",
      "Epoch: 34 itrerations: 1600 Training Loss: 4.49063634872\n",
      "Epoch: 34 itrerations: 1700 Training Loss: 4.52627468109\n",
      "Epoch: 34 itrerations: 1800 Training Loss: 4.52335500717\n",
      "Epoch: 34 itrerations: 1900 Training Loss: 4.54815101624\n",
      "Epoch: 34 itrerations: 2000 Training Loss: 4.57954645157\n",
      "Epoch: 34 itrerations: 2100 Training Loss: 4.55154514313\n",
      "Epoch: 34 itrerations: 2200 Training Loss: 4.52400827408\n",
      "Epoch: 34 itrerations: 2300 Training Loss: 4.51086235046\n",
      "Epoch: 34 itrerations: 2400 Training Loss: 4.48679685593\n",
      "Epoch: 34 itrerations: 2500 Training Loss: 4.45862960815\n",
      "Epoch: 34 itrerations: 2600 Training Loss: 4.44771337509\n",
      "Epoch: 34 itrerations: 2700 Training Loss: 4.47203207016\n",
      "Epoch: 34 itrerations: 2800 Training Loss: 4.43436717987\n",
      "Epoch: 34 itrerations: 2900 Training Loss: 4.40367412567\n",
      "Epoch: 34 itrerations: 3000 Training Loss: 4.46480035782\n",
      "Epoch: 34 itrerations: 3100 Training Loss: 4.45941638947\n",
      "Epoch: 34 itrerations: 3200 Training Loss: 4.46760845184\n",
      "Epoch: 34 itrerations: 3300 Training Loss: 4.4314494133\n",
      "Epoch: 34 itrerations: 3400 Training Loss: 4.43596458435\n",
      "Epoch: 34 itrerations: 3500 Training Loss: 4.459836483\n",
      "Epoch: 34 itrerations: 3600 Training Loss: 4.46446084976\n",
      "Epoch: 34 itrerations: 3700 Training Loss: 4.45596647263\n",
      "Epoch: 34 itrerations: 3800 Training Loss: 4.46628046036\n",
      "Epoch: 34 itrerations: 3900 Training Loss: 4.46077013016\n",
      "Epoch: 34 itrerations: 4000 Training Loss: 4.44677686691\n",
      "Epoch: 34 itrerations: 4100 Training Loss: 4.43523645401\n",
      "Epoch: 34 itrerations: 4200 Training Loss: 4.41924285889\n",
      "Epoch: 34 itrerations: 4300 Training Loss: 4.4137172699\n",
      "Epoch: 34 itrerations: 4400 Training Loss: 4.42359447479\n",
      "Epoch: 34 itrerations: 4500 Training Loss: 4.42437887192\n",
      "Epoch: 34 itrerations: 4600 Training Loss: 4.41990852356\n",
      "Epoch: 34 itrerations: 4700 Training Loss: 4.41466856003\n",
      "Epoch: 34 itrerations: 4800 Training Loss: 4.39405059814\n",
      "Epoch: 34 itrerations: 4900 Training Loss: 4.3833322525\n",
      "Epoch: 34 itrerations: 5000 Training Loss: 4.35557985306\n",
      "Epoch: 34 itrerations: 5100 Training Loss: 4.33558130264\n",
      "Epoch: 34 itrerations: 5200 Training Loss: 4.33520317078\n",
      "Epoch: 34 itrerations: 5300 Training Loss: 4.33530426025\n",
      "Epoch: 34 itrerations: 5400 Training Loss: 4.35238170624\n",
      "Epoch: 34 itrerations: 5500 Training Loss: 4.3528418541\n",
      "Epoch: 34 itrerations: 5600 Training Loss: 4.36013936996\n",
      "Epoch: 34 itrerations: 5700 Training Loss: 4.34827518463\n",
      "Epoch: 34 itrerations: 5800 Training Loss: 4.34167480469\n",
      "Epoch: 34 itrerations: 5900 Training Loss: 4.34586620331\n",
      "Epoch: 34 itrerations: 6000 Training Loss: 4.33890295029\n",
      "Epoch: 34 itrerations: 6100 Training Loss: 4.32888412476\n",
      "Epoch: 34 itrerations: 6200 Training Loss: 4.32523059845\n",
      "Epoch: 34 itrerations: 6300 Training Loss: 4.32723522186\n",
      "Epoch: 34 itrerations: 6400 Training Loss: 4.34002065659\n",
      "Epoch: 34 itrerations: 6500 Training Loss: 4.32973766327\n",
      "Epoch: 34 itrerations: 6600 Training Loss: 4.32621669769\n",
      "Epoch: 34 itrerations: 6700 Training Loss: 4.32603740692\n",
      "Epoch: 34 itrerations: 6800 Training Loss: 4.34319353104\n",
      "Epoch: 34 itrerations: 6900 Training Loss: 4.35949373245\n",
      "Epoch: 34 itrerations: 7000 Training Loss: 4.39091110229\n",
      "Epoch: 34 itrerations: 7100 Training Loss: 4.40105581284\n",
      "Epoch: 34 itrerations: 7200 Training Loss: 4.38776826859\n",
      "Epoch: 34 itrerations: 7300 Training Loss: 4.38450288773\n",
      "Epoch: 34 itrerations: 7400 Training Loss: 4.38254404068\n",
      "Epoch: 34 itrerations: 7500 Training Loss: 4.38155031204\n",
      "Epoch: 34 itrerations: 7600 Training Loss: 4.37445354462\n",
      "Epoch: 34 itrerations: 7700 Training Loss: 4.36184930801\n",
      "Epoch: 34 itrerations: 7800 Training Loss: 4.36969709396\n",
      "Epoch: 34 itrerations: 7900 Training Loss: 4.37193822861\n",
      "Epoch: 34 itrerations: 8000 Training Loss: 4.36570215225\n",
      "Epoch: 34 itrerations: 8100 Training Loss: 4.35864591599\n",
      "Epoch: 34 itrerations: 8200 Training Loss: 4.36776924133\n",
      "Epoch: 34 itrerations: 8300 Training Loss: 4.37688064575\n",
      "Epoch: 34 itrerations: 8400 Training Loss: 4.38368368149\n",
      "Epoch: 34 itrerations: 8500 Training Loss: 4.38909435272\n",
      "Epoch: 34 itrerations: 8600 Training Loss: 4.38654375076\n",
      "Epoch: 34 itrerations: 8700 Training Loss: 4.38777351379\n",
      "Epoch: 34 itrerations: 8800 Training Loss: 4.38465642929\n",
      "Epoch: 34 itrerations: 8900 Training Loss: 4.38287210464\n",
      "Epoch: 34 itrerations: 9000 Training Loss: 4.38882064819\n",
      "Epoch: 34 itrerations: 9100 Training Loss: 4.39035367966\n",
      "Epoch: 34 itrerations: 9200 Training Loss: 4.39536571503\n",
      "Epoch: 34 itrerations: 9300 Training Loss: 4.3935379982\n",
      "Epoch: 34 itrerations: 9400 Training Loss: 4.3860578537\n",
      "Epoch: 34 itrerations: 9500 Training Loss: 4.38534736633\n",
      "Epoch: 34 itrerations: 9600 Training Loss: 4.39630842209\n",
      "Epoch: 34 itrerations: 9700 Training Loss: 4.39518165588\n",
      "Epoch: 34 itrerations: 9800 Training Loss: 4.39126205444\n",
      "Epoch: 34 itrerations: 9900 Training Loss: 4.38487768173\n",
      "Epoch: 34 itrerations: 10000 Training Loss: 4.40268802643\n",
      "Epoch: 34 itrerations: 10100 Training Loss: 4.40394496918\n",
      "Epoch: 34 itrerations: 10200 Training Loss: 4.40587377548\n",
      "Epoch: 34 itrerations: 10300 Training Loss: 4.40885686874\n",
      "Epoch: 34 itrerations: 10400 Training Loss: 4.40768623352\n",
      "Epoch: 34 itrerations: 10500 Training Loss: 4.40092182159\n",
      "Epoch: 34 itrerations: 10600 Training Loss: 4.40464448929\n",
      "Epoch: 34 itrerations: 10700 Training Loss: 4.40492582321\n",
      "Epoch: 35 itrerations: 0 Training Loss: 4.56981801987\n",
      "Epoch: 35 itrerations: 100 Training Loss: 4.30783176422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 itrerations: 200 Training Loss: 4.08950471878\n",
      "Epoch: 35 itrerations: 300 Training Loss: 3.97192382812\n",
      "Epoch: 35 itrerations: 400 Training Loss: 3.93447780609\n",
      "Epoch: 35 itrerations: 500 Training Loss: 3.71896004677\n",
      "Epoch: 35 itrerations: 600 Training Loss: 3.85884213448\n",
      "Epoch: 35 itrerations: 700 Training Loss: 3.93347120285\n",
      "Epoch: 35 itrerations: 800 Training Loss: 4.12416172028\n",
      "Epoch: 35 itrerations: 900 Training Loss: 4.12940073013\n",
      "Epoch: 35 itrerations: 1000 Training Loss: 4.18291187286\n",
      "Epoch: 35 itrerations: 1100 Training Loss: 4.16477823257\n",
      "Epoch: 35 itrerations: 1200 Training Loss: 4.35380887985\n",
      "Epoch: 35 itrerations: 1300 Training Loss: 4.33960771561\n",
      "Epoch: 35 itrerations: 1400 Training Loss: 4.43954324722\n",
      "Epoch: 35 itrerations: 1500 Training Loss: 4.39325666428\n",
      "Epoch: 35 itrerations: 1600 Training Loss: 4.38364076614\n",
      "Epoch: 35 itrerations: 1700 Training Loss: 4.41470003128\n",
      "Epoch: 35 itrerations: 1800 Training Loss: 4.40141868591\n",
      "Epoch: 35 itrerations: 1900 Training Loss: 4.42080688477\n",
      "Epoch: 35 itrerations: 2000 Training Loss: 4.43586540222\n",
      "Epoch: 35 itrerations: 2100 Training Loss: 4.38887739182\n",
      "Epoch: 35 itrerations: 2200 Training Loss: 4.37238693237\n",
      "Epoch: 35 itrerations: 2300 Training Loss: 4.36943864822\n",
      "Epoch: 35 itrerations: 2400 Training Loss: 4.35679149628\n",
      "Epoch: 35 itrerations: 2500 Training Loss: 4.3475651741\n",
      "Epoch: 35 itrerations: 2600 Training Loss: 4.30684995651\n",
      "Epoch: 35 itrerations: 2700 Training Loss: 4.33906078339\n",
      "Epoch: 35 itrerations: 2800 Training Loss: 4.29214906693\n",
      "Epoch: 35 itrerations: 2900 Training Loss: 4.25353813171\n",
      "Epoch: 35 itrerations: 3000 Training Loss: 4.32894945145\n",
      "Epoch: 35 itrerations: 3100 Training Loss: 4.31019210815\n",
      "Epoch: 35 itrerations: 3200 Training Loss: 4.32028913498\n",
      "Epoch: 35 itrerations: 3300 Training Loss: 4.2898774147\n",
      "Epoch: 35 itrerations: 3400 Training Loss: 4.31067800522\n",
      "Epoch: 35 itrerations: 3500 Training Loss: 4.33756971359\n",
      "Epoch: 35 itrerations: 3600 Training Loss: 4.35207509995\n",
      "Epoch: 35 itrerations: 3700 Training Loss: 4.34278392792\n",
      "Epoch: 35 itrerations: 3800 Training Loss: 4.3630900383\n",
      "Epoch: 35 itrerations: 3900 Training Loss: 4.34254026413\n",
      "Epoch: 35 itrerations: 4000 Training Loss: 4.35738229752\n",
      "Epoch: 35 itrerations: 4100 Training Loss: 4.36189651489\n",
      "Epoch: 35 itrerations: 4200 Training Loss: 4.34593963623\n",
      "Epoch: 35 itrerations: 4300 Training Loss: 4.33127737045\n",
      "Epoch: 35 itrerations: 4400 Training Loss: 4.36498260498\n",
      "Epoch: 35 itrerations: 4500 Training Loss: 4.36134624481\n",
      "Epoch: 35 itrerations: 4600 Training Loss: 4.35404825211\n",
      "Epoch: 35 itrerations: 4700 Training Loss: 4.35781002045\n",
      "Epoch: 35 itrerations: 4800 Training Loss: 4.35159111023\n",
      "Epoch: 35 itrerations: 4900 Training Loss: 4.35316944122\n",
      "Epoch: 35 itrerations: 5000 Training Loss: 4.33117961884\n",
      "Epoch: 35 itrerations: 5100 Training Loss: 4.32074975967\n",
      "Epoch: 35 itrerations: 5200 Training Loss: 4.32171916962\n",
      "Epoch: 35 itrerations: 5300 Training Loss: 4.31715917587\n",
      "Epoch: 35 itrerations: 5400 Training Loss: 4.33726167679\n",
      "Epoch: 35 itrerations: 5500 Training Loss: 4.3253698349\n",
      "Epoch: 35 itrerations: 5600 Training Loss: 4.32381725311\n",
      "Epoch: 35 itrerations: 5700 Training Loss: 4.32655477524\n",
      "Epoch: 35 itrerations: 5800 Training Loss: 4.33074855804\n",
      "Epoch: 35 itrerations: 5900 Training Loss: 4.34695768356\n",
      "Epoch: 35 itrerations: 6000 Training Loss: 4.34317731857\n",
      "Epoch: 35 itrerations: 6100 Training Loss: 4.33155536652\n",
      "Epoch: 35 itrerations: 6200 Training Loss: 4.32780885696\n",
      "Epoch: 35 itrerations: 6300 Training Loss: 4.33866977692\n",
      "Epoch: 35 itrerations: 6400 Training Loss: 4.33526754379\n",
      "Epoch: 35 itrerations: 6500 Training Loss: 4.3261847496\n",
      "Epoch: 35 itrerations: 6600 Training Loss: 4.31571674347\n",
      "Epoch: 35 itrerations: 6700 Training Loss: 4.33346605301\n",
      "Epoch: 35 itrerations: 6800 Training Loss: 4.34676122665\n",
      "Epoch: 35 itrerations: 6900 Training Loss: 4.36192321777\n",
      "Epoch: 35 itrerations: 7000 Training Loss: 4.38801050186\n",
      "Epoch: 35 itrerations: 7100 Training Loss: 4.38694381714\n",
      "Epoch: 35 itrerations: 7200 Training Loss: 4.36408948898\n",
      "Epoch: 35 itrerations: 7300 Training Loss: 4.37073850632\n",
      "Epoch: 35 itrerations: 7400 Training Loss: 4.36853218079\n",
      "Epoch: 35 itrerations: 7500 Training Loss: 4.37002182007\n",
      "Epoch: 35 itrerations: 7600 Training Loss: 4.3685593605\n",
      "Epoch: 35 itrerations: 7700 Training Loss: 4.35942316055\n",
      "Epoch: 35 itrerations: 7800 Training Loss: 4.35741233826\n",
      "Epoch: 35 itrerations: 7900 Training Loss: 4.36603403091\n",
      "Epoch: 35 itrerations: 8000 Training Loss: 4.3599281311\n",
      "Epoch: 35 itrerations: 8100 Training Loss: 4.35502481461\n",
      "Epoch: 35 itrerations: 8200 Training Loss: 4.35751819611\n",
      "Epoch: 35 itrerations: 8300 Training Loss: 4.36432504654\n",
      "Epoch: 35 itrerations: 8400 Training Loss: 4.36519098282\n",
      "Epoch: 35 itrerations: 8500 Training Loss: 4.36759138107\n",
      "Epoch: 35 itrerations: 8600 Training Loss: 4.36016654968\n",
      "Epoch: 35 itrerations: 8700 Training Loss: 4.36196517944\n",
      "Epoch: 35 itrerations: 8800 Training Loss: 4.3611536026\n",
      "Epoch: 35 itrerations: 8900 Training Loss: 4.35654973984\n",
      "Epoch: 35 itrerations: 9000 Training Loss: 4.36792707443\n",
      "Epoch: 35 itrerations: 9100 Training Loss: 4.37314367294\n",
      "Epoch: 35 itrerations: 9200 Training Loss: 4.36858701706\n",
      "Epoch: 35 itrerations: 9300 Training Loss: 4.36638689041\n",
      "Epoch: 35 itrerations: 9400 Training Loss: 4.35849380493\n",
      "Epoch: 35 itrerations: 9500 Training Loss: 4.35809373856\n",
      "Epoch: 35 itrerations: 9600 Training Loss: 4.37052965164\n",
      "Epoch: 35 itrerations: 9700 Training Loss: 4.37390899658\n",
      "Epoch: 35 itrerations: 9800 Training Loss: 4.37054634094\n",
      "Epoch: 35 itrerations: 9900 Training Loss: 4.36949729919\n",
      "Epoch: 35 itrerations: 10000 Training Loss: 4.3753156662\n",
      "Epoch: 35 itrerations: 10100 Training Loss: 4.37572622299\n",
      "Epoch: 35 itrerations: 10200 Training Loss: 4.37630844116\n",
      "Epoch: 35 itrerations: 10300 Training Loss: 4.38098478317\n",
      "Epoch: 35 itrerations: 10400 Training Loss: 4.37966156006\n",
      "Epoch: 35 itrerations: 10500 Training Loss: 4.37154626846\n",
      "Epoch: 35 itrerations: 10600 Training Loss: 4.36664056778\n",
      "Epoch: 35 itrerations: 10700 Training Loss: 4.36551141739\n",
      "Epoch: 36 itrerations: 0 Training Loss: 7.46549701691\n",
      "Epoch: 36 itrerations: 100 Training Loss: 4.8781504631\n",
      "Epoch: 36 itrerations: 200 Training Loss: 4.17298316956\n",
      "Epoch: 36 itrerations: 300 Training Loss: 3.97281813622\n",
      "Epoch: 36 itrerations: 400 Training Loss: 4.09504032135\n",
      "Epoch: 36 itrerations: 500 Training Loss: 3.87250638008\n",
      "Epoch: 36 itrerations: 600 Training Loss: 4.05995941162\n",
      "Epoch: 36 itrerations: 700 Training Loss: 4.05552005768\n",
      "Epoch: 36 itrerations: 800 Training Loss: 4.31548881531\n",
      "Epoch: 36 itrerations: 900 Training Loss: 4.27010536194\n",
      "Epoch: 36 itrerations: 1000 Training Loss: 4.28589344025\n",
      "Epoch: 36 itrerations: 1100 Training Loss: 4.27889633179\n",
      "Epoch: 36 itrerations: 1200 Training Loss: 4.36189556122\n",
      "Epoch: 36 itrerations: 1300 Training Loss: 4.2966094017\n",
      "Epoch: 36 itrerations: 1400 Training Loss: 4.40755844116\n",
      "Epoch: 36 itrerations: 1500 Training Loss: 4.42375612259\n",
      "Epoch: 36 itrerations: 1600 Training Loss: 4.44088029861\n",
      "Epoch: 36 itrerations: 1700 Training Loss: 4.45432138443\n",
      "Epoch: 36 itrerations: 1800 Training Loss: 4.42762613297\n",
      "Epoch: 36 itrerations: 1900 Training Loss: 4.46372365952\n",
      "Epoch: 36 itrerations: 2000 Training Loss: 4.49308156967\n",
      "Epoch: 36 itrerations: 2100 Training Loss: 4.46256875992\n",
      "Epoch: 36 itrerations: 2200 Training Loss: 4.44381046295\n",
      "Epoch: 36 itrerations: 2300 Training Loss: 4.45349836349\n",
      "Epoch: 36 itrerations: 2400 Training Loss: 4.40342950821\n",
      "Epoch: 36 itrerations: 2500 Training Loss: 4.37751913071\n",
      "Epoch: 36 itrerations: 2600 Training Loss: 4.3824672699\n",
      "Epoch: 36 itrerations: 2700 Training Loss: 4.4261507988\n",
      "Epoch: 36 itrerations: 2800 Training Loss: 4.37491512299\n",
      "Epoch: 36 itrerations: 2900 Training Loss: 4.34740686417\n",
      "Epoch: 36 itrerations: 3000 Training Loss: 4.40705156326\n",
      "Epoch: 36 itrerations: 3100 Training Loss: 4.38039588928\n",
      "Epoch: 36 itrerations: 3200 Training Loss: 4.38217020035\n",
      "Epoch: 36 itrerations: 3300 Training Loss: 4.35705280304\n",
      "Epoch: 36 itrerations: 3400 Training Loss: 4.35228013992\n",
      "Epoch: 36 itrerations: 3500 Training Loss: 4.38883304596\n",
      "Epoch: 36 itrerations: 3600 Training Loss: 4.39810991287\n",
      "Epoch: 36 itrerations: 3700 Training Loss: 4.38237476349\n",
      "Epoch: 36 itrerations: 3800 Training Loss: 4.38828039169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 itrerations: 3900 Training Loss: 4.36476802826\n",
      "Epoch: 36 itrerations: 4000 Training Loss: 4.36568212509\n",
      "Epoch: 36 itrerations: 4100 Training Loss: 4.3666806221\n",
      "Epoch: 36 itrerations: 4200 Training Loss: 4.34552955627\n",
      "Epoch: 36 itrerations: 4300 Training Loss: 4.36720561981\n",
      "Epoch: 36 itrerations: 4400 Training Loss: 4.38058423996\n",
      "Epoch: 36 itrerations: 4500 Training Loss: 4.37849521637\n",
      "Epoch: 36 itrerations: 4600 Training Loss: 4.36150550842\n",
      "Epoch: 36 itrerations: 4700 Training Loss: 4.36801528931\n",
      "Epoch: 36 itrerations: 4800 Training Loss: 4.35674524307\n",
      "Epoch: 36 itrerations: 4900 Training Loss: 4.3509812355\n",
      "Epoch: 36 itrerations: 5000 Training Loss: 4.33710622787\n",
      "Epoch: 36 itrerations: 5100 Training Loss: 4.32454013824\n",
      "Epoch: 36 itrerations: 5200 Training Loss: 4.33805084229\n",
      "Epoch: 36 itrerations: 5300 Training Loss: 4.33451080322\n",
      "Epoch: 36 itrerations: 5400 Training Loss: 4.36120414734\n",
      "Epoch: 36 itrerations: 5500 Training Loss: 4.35106706619\n",
      "Epoch: 36 itrerations: 5600 Training Loss: 4.35173606873\n",
      "Epoch: 36 itrerations: 5700 Training Loss: 4.34122657776\n",
      "Epoch: 36 itrerations: 5800 Training Loss: 4.33882427216\n",
      "Epoch: 36 itrerations: 5900 Training Loss: 4.33898162842\n",
      "Epoch: 36 itrerations: 6000 Training Loss: 4.33405637741\n",
      "Epoch: 36 itrerations: 6100 Training Loss: 4.32347011566\n",
      "Epoch: 36 itrerations: 6200 Training Loss: 4.3323931694\n",
      "Epoch: 36 itrerations: 6300 Training Loss: 4.33909463882\n",
      "Epoch: 36 itrerations: 6400 Training Loss: 4.34272766113\n",
      "Epoch: 36 itrerations: 6500 Training Loss: 4.33328294754\n",
      "Epoch: 36 itrerations: 6600 Training Loss: 4.33344554901\n",
      "Epoch: 36 itrerations: 6700 Training Loss: 4.34425067902\n",
      "Epoch: 36 itrerations: 6800 Training Loss: 4.35495519638\n",
      "Epoch: 36 itrerations: 6900 Training Loss: 4.35998296738\n",
      "Epoch: 36 itrerations: 7000 Training Loss: 4.3811044693\n",
      "Epoch: 36 itrerations: 7100 Training Loss: 4.39615535736\n",
      "Epoch: 36 itrerations: 7200 Training Loss: 4.38618612289\n",
      "Epoch: 36 itrerations: 7300 Training Loss: 4.38292455673\n",
      "Epoch: 36 itrerations: 7400 Training Loss: 4.37651205063\n",
      "Epoch: 36 itrerations: 7500 Training Loss: 4.37811613083\n",
      "Epoch: 36 itrerations: 7600 Training Loss: 4.3654088974\n",
      "Epoch: 36 itrerations: 7700 Training Loss: 4.35376691818\n",
      "Epoch: 36 itrerations: 7800 Training Loss: 4.36384963989\n",
      "Epoch: 36 itrerations: 7900 Training Loss: 4.38102817535\n",
      "Epoch: 36 itrerations: 8000 Training Loss: 4.3760766983\n",
      "Epoch: 36 itrerations: 8100 Training Loss: 4.37010478973\n",
      "Epoch: 36 itrerations: 8200 Training Loss: 4.37704801559\n",
      "Epoch: 36 itrerations: 8300 Training Loss: 4.38261270523\n",
      "Epoch: 36 itrerations: 8400 Training Loss: 4.38363742828\n",
      "Epoch: 36 itrerations: 8500 Training Loss: 4.38270998001\n",
      "Epoch: 36 itrerations: 8600 Training Loss: 4.37729597092\n",
      "Epoch: 36 itrerations: 8700 Training Loss: 4.37902450562\n",
      "Epoch: 36 itrerations: 8800 Training Loss: 4.37581682205\n",
      "Epoch: 36 itrerations: 8900 Training Loss: 4.37812948227\n",
      "Epoch: 36 itrerations: 9000 Training Loss: 4.38748121262\n",
      "Epoch: 36 itrerations: 9100 Training Loss: 4.38574028015\n",
      "Epoch: 36 itrerations: 9200 Training Loss: 4.39010620117\n",
      "Epoch: 36 itrerations: 9300 Training Loss: 4.38248109818\n",
      "Epoch: 36 itrerations: 9400 Training Loss: 4.37565279007\n",
      "Epoch: 36 itrerations: 9500 Training Loss: 4.37833499908\n",
      "Epoch: 36 itrerations: 9600 Training Loss: 4.3870472908\n",
      "Epoch: 36 itrerations: 9700 Training Loss: 4.3853764534\n",
      "Epoch: 36 itrerations: 9800 Training Loss: 4.38699245453\n",
      "Epoch: 36 itrerations: 9900 Training Loss: 4.38583087921\n",
      "Epoch: 36 itrerations: 10000 Training Loss: 4.39901542664\n",
      "Epoch: 36 itrerations: 10100 Training Loss: 4.40148925781\n",
      "Epoch: 36 itrerations: 10200 Training Loss: 4.39400434494\n",
      "Epoch: 36 itrerations: 10300 Training Loss: 4.39698266983\n",
      "Epoch: 36 itrerations: 10400 Training Loss: 4.39365816116\n",
      "Epoch: 36 itrerations: 10500 Training Loss: 4.38435173035\n",
      "Epoch: 36 itrerations: 10600 Training Loss: 4.38644218445\n",
      "Epoch: 36 itrerations: 10700 Training Loss: 4.38838672638\n",
      "Epoch: 37 itrerations: 0 Training Loss: 9.33365917206\n",
      "Epoch: 37 itrerations: 100 Training Loss: 4.25258541107\n",
      "Epoch: 37 itrerations: 200 Training Loss: 4.09009122849\n",
      "Epoch: 37 itrerations: 300 Training Loss: 4.00755548477\n",
      "Epoch: 37 itrerations: 400 Training Loss: 4.11579465866\n",
      "Epoch: 37 itrerations: 500 Training Loss: 3.84546279907\n",
      "Epoch: 37 itrerations: 600 Training Loss: 4.02767324448\n",
      "Epoch: 37 itrerations: 700 Training Loss: 4.12041902542\n",
      "Epoch: 37 itrerations: 800 Training Loss: 4.35628843307\n",
      "Epoch: 37 itrerations: 900 Training Loss: 4.28553676605\n",
      "Epoch: 37 itrerations: 1000 Training Loss: 4.36480522156\n",
      "Epoch: 37 itrerations: 1100 Training Loss: 4.33826780319\n",
      "Epoch: 37 itrerations: 1200 Training Loss: 4.49120378494\n",
      "Epoch: 37 itrerations: 1300 Training Loss: 4.43295288086\n",
      "Epoch: 37 itrerations: 1400 Training Loss: 4.49844074249\n",
      "Epoch: 37 itrerations: 1500 Training Loss: 4.53031682968\n",
      "Epoch: 37 itrerations: 1600 Training Loss: 4.54765224457\n",
      "Epoch: 37 itrerations: 1700 Training Loss: 4.57503414154\n",
      "Epoch: 37 itrerations: 1800 Training Loss: 4.55348062515\n",
      "Epoch: 37 itrerations: 1900 Training Loss: 4.58896017075\n",
      "Epoch: 37 itrerations: 2000 Training Loss: 4.60633563995\n",
      "Epoch: 37 itrerations: 2100 Training Loss: 4.56107759476\n",
      "Epoch: 37 itrerations: 2200 Training Loss: 4.53060054779\n",
      "Epoch: 37 itrerations: 2300 Training Loss: 4.52191877365\n",
      "Epoch: 37 itrerations: 2400 Training Loss: 4.49185943604\n",
      "Epoch: 37 itrerations: 2500 Training Loss: 4.4872469902\n",
      "Epoch: 37 itrerations: 2600 Training Loss: 4.48409509659\n",
      "Epoch: 37 itrerations: 2700 Training Loss: 4.47888994217\n",
      "Epoch: 37 itrerations: 2800 Training Loss: 4.42772197723\n",
      "Epoch: 37 itrerations: 2900 Training Loss: 4.3944735527\n",
      "Epoch: 37 itrerations: 3000 Training Loss: 4.44424200058\n",
      "Epoch: 37 itrerations: 3100 Training Loss: 4.43414068222\n",
      "Epoch: 37 itrerations: 3200 Training Loss: 4.4351272583\n",
      "Epoch: 37 itrerations: 3300 Training Loss: 4.40572166443\n",
      "Epoch: 37 itrerations: 3400 Training Loss: 4.40311861038\n",
      "Epoch: 37 itrerations: 3500 Training Loss: 4.41712093353\n",
      "Epoch: 37 itrerations: 3600 Training Loss: 4.4235701561\n",
      "Epoch: 37 itrerations: 3700 Training Loss: 4.41996383667\n",
      "Epoch: 37 itrerations: 3800 Training Loss: 4.4518494606\n",
      "Epoch: 37 itrerations: 3900 Training Loss: 4.44751834869\n",
      "Epoch: 37 itrerations: 4000 Training Loss: 4.45159626007\n",
      "Epoch: 37 itrerations: 4100 Training Loss: 4.44532585144\n",
      "Epoch: 37 itrerations: 4200 Training Loss: 4.42647743225\n",
      "Epoch: 37 itrerations: 4300 Training Loss: 4.43781042099\n",
      "Epoch: 37 itrerations: 4400 Training Loss: 4.44270992279\n",
      "Epoch: 37 itrerations: 4500 Training Loss: 4.43605661392\n",
      "Epoch: 37 itrerations: 4600 Training Loss: 4.42322063446\n",
      "Epoch: 37 itrerations: 4700 Training Loss: 4.42595481873\n",
      "Epoch: 37 itrerations: 4800 Training Loss: 4.40482473373\n",
      "Epoch: 37 itrerations: 4900 Training Loss: 4.3923830986\n",
      "Epoch: 37 itrerations: 5000 Training Loss: 4.36207532883\n",
      "Epoch: 37 itrerations: 5100 Training Loss: 4.3475728035\n",
      "Epoch: 37 itrerations: 5200 Training Loss: 4.35542964935\n",
      "Epoch: 37 itrerations: 5300 Training Loss: 4.35838127136\n",
      "Epoch: 37 itrerations: 5400 Training Loss: 4.3840880394\n",
      "Epoch: 37 itrerations: 5500 Training Loss: 4.37187480927\n",
      "Epoch: 37 itrerations: 5600 Training Loss: 4.36414813995\n",
      "Epoch: 37 itrerations: 5700 Training Loss: 4.36638116837\n",
      "Epoch: 37 itrerations: 5800 Training Loss: 4.37598896027\n",
      "Epoch: 37 itrerations: 5900 Training Loss: 4.38243341446\n",
      "Epoch: 37 itrerations: 6000 Training Loss: 4.37264490128\n",
      "Epoch: 37 itrerations: 6100 Training Loss: 4.36047458649\n",
      "Epoch: 37 itrerations: 6200 Training Loss: 4.35841417313\n",
      "Epoch: 37 itrerations: 6300 Training Loss: 4.35520887375\n",
      "Epoch: 37 itrerations: 6400 Training Loss: 4.35727453232\n",
      "Epoch: 37 itrerations: 6500 Training Loss: 4.35200357437\n",
      "Epoch: 37 itrerations: 6600 Training Loss: 4.34367990494\n",
      "Epoch: 37 itrerations: 6700 Training Loss: 4.34025526047\n",
      "Epoch: 37 itrerations: 6800 Training Loss: 4.34341049194\n",
      "Epoch: 37 itrerations: 6900 Training Loss: 4.36078262329\n",
      "Epoch: 37 itrerations: 7000 Training Loss: 4.37235689163\n",
      "Epoch: 37 itrerations: 7100 Training Loss: 4.37455844879\n",
      "Epoch: 37 itrerations: 7200 Training Loss: 4.36615037918\n",
      "Epoch: 37 itrerations: 7300 Training Loss: 4.37285709381\n",
      "Epoch: 37 itrerations: 7400 Training Loss: 4.36170911789\n",
      "Epoch: 37 itrerations: 7500 Training Loss: 4.3571228981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 itrerations: 7600 Training Loss: 4.3438577652\n",
      "Epoch: 37 itrerations: 7700 Training Loss: 4.33431005478\n",
      "Epoch: 37 itrerations: 7800 Training Loss: 4.33326244354\n",
      "Epoch: 37 itrerations: 7900 Training Loss: 4.34708547592\n",
      "Epoch: 37 itrerations: 8000 Training Loss: 4.34809923172\n",
      "Epoch: 37 itrerations: 8100 Training Loss: 4.34198522568\n",
      "Epoch: 37 itrerations: 8200 Training Loss: 4.34812784195\n",
      "Epoch: 37 itrerations: 8300 Training Loss: 4.35514545441\n",
      "Epoch: 37 itrerations: 8400 Training Loss: 4.35965538025\n",
      "Epoch: 37 itrerations: 8500 Training Loss: 4.35833978653\n",
      "Epoch: 37 itrerations: 8600 Training Loss: 4.35866737366\n",
      "Epoch: 37 itrerations: 8700 Training Loss: 4.34717607498\n",
      "Epoch: 37 itrerations: 8800 Training Loss: 4.34271812439\n",
      "Epoch: 37 itrerations: 8900 Training Loss: 4.34236812592\n",
      "Epoch: 37 itrerations: 9000 Training Loss: 4.3511929512\n",
      "Epoch: 37 itrerations: 9100 Training Loss: 4.35365867615\n",
      "Epoch: 37 itrerations: 9200 Training Loss: 4.35888147354\n",
      "Epoch: 37 itrerations: 9300 Training Loss: 4.35986280441\n",
      "Epoch: 37 itrerations: 9400 Training Loss: 4.35280847549\n",
      "Epoch: 37 itrerations: 9500 Training Loss: 4.35444355011\n",
      "Epoch: 37 itrerations: 9600 Training Loss: 4.36403465271\n",
      "Epoch: 37 itrerations: 9700 Training Loss: 4.36706209183\n",
      "Epoch: 37 itrerations: 9800 Training Loss: 4.37177705765\n",
      "Epoch: 37 itrerations: 9900 Training Loss: 4.37387084961\n",
      "Epoch: 37 itrerations: 10000 Training Loss: 4.38461637497\n",
      "Epoch: 37 itrerations: 10100 Training Loss: 4.38035106659\n",
      "Epoch: 37 itrerations: 10200 Training Loss: 4.37790060043\n",
      "Epoch: 37 itrerations: 10300 Training Loss: 4.38055753708\n",
      "Epoch: 37 itrerations: 10400 Training Loss: 4.38339471817\n",
      "Epoch: 37 itrerations: 10500 Training Loss: 4.37373828888\n",
      "Epoch: 37 itrerations: 10600 Training Loss: 4.37841272354\n",
      "Epoch: 37 itrerations: 10700 Training Loss: 4.37529087067\n",
      "Epoch: 38 itrerations: 0 Training Loss: 5.29931259155\n",
      "Epoch: 38 itrerations: 100 Training Loss: 4.22865867615\n",
      "Epoch: 38 itrerations: 200 Training Loss: 3.97858428955\n",
      "Epoch: 38 itrerations: 300 Training Loss: 4.01857328415\n",
      "Epoch: 38 itrerations: 400 Training Loss: 4.2677321434\n",
      "Epoch: 38 itrerations: 500 Training Loss: 3.95627045631\n",
      "Epoch: 38 itrerations: 600 Training Loss: 4.19182395935\n",
      "Epoch: 38 itrerations: 700 Training Loss: 4.27252006531\n",
      "Epoch: 38 itrerations: 800 Training Loss: 4.51122570038\n",
      "Epoch: 38 itrerations: 900 Training Loss: 4.48254346848\n",
      "Epoch: 38 itrerations: 1000 Training Loss: 4.54423570633\n",
      "Epoch: 38 itrerations: 1100 Training Loss: 4.48959922791\n",
      "Epoch: 38 itrerations: 1200 Training Loss: 4.52465391159\n",
      "Epoch: 38 itrerations: 1300 Training Loss: 4.5027384758\n",
      "Epoch: 38 itrerations: 1400 Training Loss: 4.58653640747\n",
      "Epoch: 38 itrerations: 1500 Training Loss: 4.5213227272\n",
      "Epoch: 38 itrerations: 1600 Training Loss: 4.51277685165\n",
      "Epoch: 38 itrerations: 1700 Training Loss: 4.50943660736\n",
      "Epoch: 38 itrerations: 1800 Training Loss: 4.48177480698\n",
      "Epoch: 38 itrerations: 1900 Training Loss: 4.5324716568\n",
      "Epoch: 38 itrerations: 2000 Training Loss: 4.56685829163\n",
      "Epoch: 38 itrerations: 2100 Training Loss: 4.52989244461\n",
      "Epoch: 38 itrerations: 2200 Training Loss: 4.49335193634\n",
      "Epoch: 38 itrerations: 2300 Training Loss: 4.53057861328\n",
      "Epoch: 38 itrerations: 2400 Training Loss: 4.48406505585\n",
      "Epoch: 38 itrerations: 2500 Training Loss: 4.4563574791\n",
      "Epoch: 38 itrerations: 2600 Training Loss: 4.47480821609\n",
      "Epoch: 38 itrerations: 2700 Training Loss: 4.48495578766\n",
      "Epoch: 38 itrerations: 2800 Training Loss: 4.44333076477\n",
      "Epoch: 38 itrerations: 2900 Training Loss: 4.4229388237\n",
      "Epoch: 38 itrerations: 3000 Training Loss: 4.45317459106\n",
      "Epoch: 38 itrerations: 3100 Training Loss: 4.43120622635\n",
      "Epoch: 38 itrerations: 3200 Training Loss: 4.44607543945\n",
      "Epoch: 38 itrerations: 3300 Training Loss: 4.40376758575\n",
      "Epoch: 38 itrerations: 3400 Training Loss: 4.3991560936\n",
      "Epoch: 38 itrerations: 3500 Training Loss: 4.44307136536\n",
      "Epoch: 38 itrerations: 3600 Training Loss: 4.46508598328\n",
      "Epoch: 38 itrerations: 3700 Training Loss: 4.46110343933\n",
      "Epoch: 38 itrerations: 3800 Training Loss: 4.47057819366\n",
      "Epoch: 38 itrerations: 3900 Training Loss: 4.45442676544\n",
      "Epoch: 38 itrerations: 4000 Training Loss: 4.44949054718\n",
      "Epoch: 38 itrerations: 4100 Training Loss: 4.43898153305\n",
      "Epoch: 38 itrerations: 4200 Training Loss: 4.43114376068\n",
      "Epoch: 38 itrerations: 4300 Training Loss: 4.4318652153\n",
      "Epoch: 38 itrerations: 4400 Training Loss: 4.44473838806\n",
      "Epoch: 38 itrerations: 4500 Training Loss: 4.43391370773\n",
      "Epoch: 38 itrerations: 4600 Training Loss: 4.42564296722\n",
      "Epoch: 38 itrerations: 4700 Training Loss: 4.43683290482\n",
      "Epoch: 38 itrerations: 4800 Training Loss: 4.43729972839\n",
      "Epoch: 38 itrerations: 4900 Training Loss: 4.42515230179\n",
      "Epoch: 38 itrerations: 5000 Training Loss: 4.40248060226\n",
      "Epoch: 38 itrerations: 5100 Training Loss: 4.3942990303\n",
      "Epoch: 38 itrerations: 5200 Training Loss: 4.39239358902\n",
      "Epoch: 38 itrerations: 5300 Training Loss: 4.38937473297\n",
      "Epoch: 38 itrerations: 5400 Training Loss: 4.40436792374\n",
      "Epoch: 38 itrerations: 5500 Training Loss: 4.40077638626\n",
      "Epoch: 38 itrerations: 5600 Training Loss: 4.38704824448\n",
      "Epoch: 38 itrerations: 5700 Training Loss: 4.3877248764\n",
      "Epoch: 38 itrerations: 5800 Training Loss: 4.38871717453\n",
      "Epoch: 38 itrerations: 5900 Training Loss: 4.39531612396\n",
      "Epoch: 38 itrerations: 6000 Training Loss: 4.39417600632\n",
      "Epoch: 38 itrerations: 6100 Training Loss: 4.38033294678\n",
      "Epoch: 38 itrerations: 6200 Training Loss: 4.38425207138\n",
      "Epoch: 38 itrerations: 6300 Training Loss: 4.38662576675\n",
      "Epoch: 38 itrerations: 6400 Training Loss: 4.38821220398\n",
      "Epoch: 38 itrerations: 6500 Training Loss: 4.38614463806\n",
      "Epoch: 38 itrerations: 6600 Training Loss: 4.37369918823\n",
      "Epoch: 38 itrerations: 6700 Training Loss: 4.37131214142\n",
      "Epoch: 38 itrerations: 6800 Training Loss: 4.37971878052\n",
      "Epoch: 38 itrerations: 6900 Training Loss: 4.39475345612\n",
      "Epoch: 38 itrerations: 7000 Training Loss: 4.41861009598\n",
      "Epoch: 38 itrerations: 7100 Training Loss: 4.42887783051\n",
      "Epoch: 38 itrerations: 7200 Training Loss: 4.41201686859\n",
      "Epoch: 38 itrerations: 7300 Training Loss: 4.40550947189\n",
      "Epoch: 38 itrerations: 7400 Training Loss: 4.39505910873\n",
      "Epoch: 38 itrerations: 7500 Training Loss: 4.39225482941\n",
      "Epoch: 38 itrerations: 7600 Training Loss: 4.38173151016\n",
      "Epoch: 38 itrerations: 7700 Training Loss: 4.36945772171\n",
      "Epoch: 38 itrerations: 7800 Training Loss: 4.36920547485\n",
      "Epoch: 38 itrerations: 7900 Training Loss: 4.37662172318\n",
      "Epoch: 38 itrerations: 8000 Training Loss: 4.37645864487\n",
      "Epoch: 38 itrerations: 8100 Training Loss: 4.37543296814\n",
      "Epoch: 38 itrerations: 8200 Training Loss: 4.380423069\n",
      "Epoch: 38 itrerations: 8300 Training Loss: 4.37757015228\n",
      "Epoch: 38 itrerations: 8400 Training Loss: 4.36920022964\n",
      "Epoch: 38 itrerations: 8500 Training Loss: 4.36745738983\n",
      "Epoch: 38 itrerations: 8600 Training Loss: 4.36551189423\n",
      "Epoch: 38 itrerations: 8700 Training Loss: 4.36290693283\n",
      "Epoch: 38 itrerations: 8800 Training Loss: 4.36614990234\n",
      "Epoch: 38 itrerations: 8900 Training Loss: 4.36283254623\n",
      "Epoch: 38 itrerations: 9000 Training Loss: 4.37649822235\n",
      "Epoch: 38 itrerations: 9100 Training Loss: 4.37424325943\n",
      "Epoch: 38 itrerations: 9200 Training Loss: 4.37242174149\n",
      "Epoch: 38 itrerations: 9300 Training Loss: 4.36986351013\n",
      "Epoch: 38 itrerations: 9400 Training Loss: 4.35915708542\n",
      "Epoch: 38 itrerations: 9500 Training Loss: 4.35929775238\n",
      "Epoch: 38 itrerations: 9600 Training Loss: 4.36651134491\n",
      "Epoch: 38 itrerations: 9700 Training Loss: 4.3689699173\n",
      "Epoch: 38 itrerations: 9800 Training Loss: 4.36949300766\n",
      "Epoch: 38 itrerations: 9900 Training Loss: 4.36944246292\n",
      "Epoch: 38 itrerations: 10000 Training Loss: 4.37938928604\n",
      "Epoch: 38 itrerations: 10100 Training Loss: 4.38586235046\n",
      "Epoch: 38 itrerations: 10200 Training Loss: 4.38599205017\n",
      "Epoch: 38 itrerations: 10300 Training Loss: 4.3859629631\n",
      "Epoch: 38 itrerations: 10400 Training Loss: 4.38152694702\n",
      "Epoch: 38 itrerations: 10500 Training Loss: 4.37296247482\n",
      "Epoch: 38 itrerations: 10600 Training Loss: 4.37158441544\n",
      "Epoch: 38 itrerations: 10700 Training Loss: 4.37257194519\n",
      "Epoch: 39 itrerations: 0 Training Loss: 9.263007164\n",
      "Epoch: 39 itrerations: 100 Training Loss: 4.57342433929\n",
      "Epoch: 39 itrerations: 200 Training Loss: 4.03928852081\n",
      "Epoch: 39 itrerations: 300 Training Loss: 4.15658712387\n",
      "Epoch: 39 itrerations: 400 Training Loss: 4.11797857285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 itrerations: 500 Training Loss: 3.82380151749\n",
      "Epoch: 39 itrerations: 600 Training Loss: 4.02410268784\n",
      "Epoch: 39 itrerations: 700 Training Loss: 4.08908367157\n",
      "Epoch: 39 itrerations: 800 Training Loss: 4.25087404251\n",
      "Epoch: 39 itrerations: 900 Training Loss: 4.24062633514\n",
      "Epoch: 39 itrerations: 1000 Training Loss: 4.25100231171\n",
      "Epoch: 39 itrerations: 1100 Training Loss: 4.20902585983\n",
      "Epoch: 39 itrerations: 1200 Training Loss: 4.31062746048\n",
      "Epoch: 39 itrerations: 1300 Training Loss: 4.27609014511\n",
      "Epoch: 39 itrerations: 1400 Training Loss: 4.3406662941\n",
      "Epoch: 39 itrerations: 1500 Training Loss: 4.31934690475\n",
      "Epoch: 39 itrerations: 1600 Training Loss: 4.40127754211\n",
      "Epoch: 39 itrerations: 1700 Training Loss: 4.41787481308\n",
      "Epoch: 39 itrerations: 1800 Training Loss: 4.40080690384\n",
      "Epoch: 39 itrerations: 1900 Training Loss: 4.45050191879\n",
      "Epoch: 39 itrerations: 2000 Training Loss: 4.47575426102\n",
      "Epoch: 39 itrerations: 2100 Training Loss: 4.46186256409\n",
      "Epoch: 39 itrerations: 2200 Training Loss: 4.41008520126\n",
      "Epoch: 39 itrerations: 2300 Training Loss: 4.42087316513\n",
      "Epoch: 39 itrerations: 2400 Training Loss: 4.37792491913\n",
      "Epoch: 39 itrerations: 2500 Training Loss: 4.35852956772\n",
      "Epoch: 39 itrerations: 2600 Training Loss: 4.35481786728\n",
      "Epoch: 39 itrerations: 2700 Training Loss: 4.3752040863\n",
      "Epoch: 39 itrerations: 2800 Training Loss: 4.34226131439\n",
      "Epoch: 39 itrerations: 2900 Training Loss: 4.32072925568\n",
      "Epoch: 39 itrerations: 3000 Training Loss: 4.38372993469\n",
      "Epoch: 39 itrerations: 3100 Training Loss: 4.35993242264\n",
      "Epoch: 39 itrerations: 3200 Training Loss: 4.36307239532\n",
      "Epoch: 39 itrerations: 3300 Training Loss: 4.34680175781\n",
      "Epoch: 39 itrerations: 3400 Training Loss: 4.33909225464\n",
      "Epoch: 39 itrerations: 3500 Training Loss: 4.38037395477\n",
      "Epoch: 39 itrerations: 3600 Training Loss: 4.3844909668\n",
      "Epoch: 39 itrerations: 3700 Training Loss: 4.36727428436\n",
      "Epoch: 39 itrerations: 3800 Training Loss: 4.38527011871\n",
      "Epoch: 39 itrerations: 3900 Training Loss: 4.3635725975\n",
      "Epoch: 39 itrerations: 4000 Training Loss: 4.3620839119\n",
      "Epoch: 39 itrerations: 4100 Training Loss: 4.35521316528\n",
      "Epoch: 39 itrerations: 4200 Training Loss: 4.34385490417\n",
      "Epoch: 39 itrerations: 4300 Training Loss: 4.33772706985\n",
      "Epoch: 39 itrerations: 4400 Training Loss: 4.34124660492\n",
      "Epoch: 39 itrerations: 4500 Training Loss: 4.34136009216\n",
      "Epoch: 39 itrerations: 4600 Training Loss: 4.34475374222\n",
      "Epoch: 39 itrerations: 4700 Training Loss: 4.34945344925\n",
      "Epoch: 39 itrerations: 4800 Training Loss: 4.33978128433\n",
      "Epoch: 39 itrerations: 4900 Training Loss: 4.3352766037\n",
      "Epoch: 39 itrerations: 5000 Training Loss: 4.31127309799\n",
      "Epoch: 39 itrerations: 5100 Training Loss: 4.29417705536\n",
      "Epoch: 39 itrerations: 5200 Training Loss: 4.28802394867\n",
      "Epoch: 39 itrerations: 5300 Training Loss: 4.28641414642\n",
      "Epoch: 39 itrerations: 5400 Training Loss: 4.2990436554\n",
      "Epoch: 39 itrerations: 5500 Training Loss: 4.29256010056\n",
      "Epoch: 39 itrerations: 5600 Training Loss: 4.29795694351\n",
      "Epoch: 39 itrerations: 5700 Training Loss: 4.29146146774\n",
      "Epoch: 39 itrerations: 5800 Training Loss: 4.29572868347\n",
      "Epoch: 39 itrerations: 5900 Training Loss: 4.29768848419\n",
      "Epoch: 39 itrerations: 6000 Training Loss: 4.29421949387\n",
      "Epoch: 39 itrerations: 6100 Training Loss: 4.2914276123\n",
      "Epoch: 39 itrerations: 6200 Training Loss: 4.28788614273\n",
      "Epoch: 39 itrerations: 6300 Training Loss: 4.29059123993\n",
      "Epoch: 39 itrerations: 6400 Training Loss: 4.28803682327\n",
      "Epoch: 39 itrerations: 6500 Training Loss: 4.27931261063\n",
      "Epoch: 39 itrerations: 6600 Training Loss: 4.27855920792\n",
      "Epoch: 39 itrerations: 6700 Training Loss: 4.28064346313\n",
      "Epoch: 39 itrerations: 6800 Training Loss: 4.28674936295\n",
      "Epoch: 39 itrerations: 6900 Training Loss: 4.3100271225\n",
      "Epoch: 39 itrerations: 7000 Training Loss: 4.33983325958\n",
      "Epoch: 39 itrerations: 7100 Training Loss: 4.34224557877\n",
      "Epoch: 39 itrerations: 7200 Training Loss: 4.32818746567\n",
      "Epoch: 39 itrerations: 7300 Training Loss: 4.33084917068\n",
      "Epoch: 39 itrerations: 7400 Training Loss: 4.32481002808\n",
      "Epoch: 39 itrerations: 7500 Training Loss: 4.31938171387\n",
      "Epoch: 39 itrerations: 7600 Training Loss: 4.31707191467\n",
      "Epoch: 39 itrerations: 7700 Training Loss: 4.30303192139\n",
      "Epoch: 39 itrerations: 7800 Training Loss: 4.29921245575\n",
      "Epoch: 39 itrerations: 7900 Training Loss: 4.3042473793\n",
      "Epoch: 39 itrerations: 8000 Training Loss: 4.30140113831\n",
      "Epoch: 39 itrerations: 8100 Training Loss: 4.29226493835\n",
      "Epoch: 39 itrerations: 8200 Training Loss: 4.30638170242\n",
      "Epoch: 39 itrerations: 8300 Training Loss: 4.31417274475\n",
      "Epoch: 39 itrerations: 8400 Training Loss: 4.30745649338\n",
      "Epoch: 39 itrerations: 8500 Training Loss: 4.3096575737\n",
      "Epoch: 39 itrerations: 8600 Training Loss: 4.30203723907\n",
      "Epoch: 39 itrerations: 8700 Training Loss: 4.29597997665\n",
      "Epoch: 39 itrerations: 8800 Training Loss: 4.30323076248\n",
      "Epoch: 39 itrerations: 8900 Training Loss: 4.3009595871\n",
      "Epoch: 39 itrerations: 9000 Training Loss: 4.30113124847\n",
      "Epoch: 39 itrerations: 9100 Training Loss: 4.30435943604\n",
      "Epoch: 39 itrerations: 9200 Training Loss: 4.30773639679\n",
      "Epoch: 39 itrerations: 9300 Training Loss: 4.30654096603\n",
      "Epoch: 39 itrerations: 9400 Training Loss: 4.29551887512\n",
      "Epoch: 39 itrerations: 9500 Training Loss: 4.29522323608\n",
      "Epoch: 39 itrerations: 9600 Training Loss: 4.30318546295\n",
      "Epoch: 39 itrerations: 9700 Training Loss: 4.30513477325\n",
      "Epoch: 39 itrerations: 9800 Training Loss: 4.30465555191\n",
      "Epoch: 39 itrerations: 9900 Training Loss: 4.30603599548\n",
      "Epoch: 39 itrerations: 10000 Training Loss: 4.3124127388\n",
      "Epoch: 39 itrerations: 10100 Training Loss: 4.30946207047\n",
      "Epoch: 39 itrerations: 10200 Training Loss: 4.30351877213\n",
      "Epoch: 39 itrerations: 10300 Training Loss: 4.30946588516\n",
      "Epoch: 39 itrerations: 10400 Training Loss: 4.30774736404\n",
      "Epoch: 39 itrerations: 10500 Training Loss: 4.30089998245\n",
      "Epoch: 39 itrerations: 10600 Training Loss: 4.31047058105\n",
      "Epoch: 39 itrerations: 10700 Training Loss: 4.30899763107\n",
      "Epoch: 40 itrerations: 0 Training Loss: 4.82582044601\n",
      "Epoch: 40 itrerations: 100 Training Loss: 3.93974375725\n",
      "Epoch: 40 itrerations: 200 Training Loss: 3.9306371212\n",
      "Epoch: 40 itrerations: 300 Training Loss: 4.0099902153\n",
      "Epoch: 40 itrerations: 400 Training Loss: 4.08171081543\n",
      "Epoch: 40 itrerations: 500 Training Loss: 3.76605892181\n",
      "Epoch: 40 itrerations: 600 Training Loss: 3.960105896\n",
      "Epoch: 40 itrerations: 700 Training Loss: 4.05999326706\n",
      "Epoch: 40 itrerations: 800 Training Loss: 4.23871994019\n",
      "Epoch: 40 itrerations: 900 Training Loss: 4.27440166473\n",
      "Epoch: 40 itrerations: 1000 Training Loss: 4.30798196793\n",
      "Epoch: 40 itrerations: 1100 Training Loss: 4.30748081207\n",
      "Epoch: 40 itrerations: 1200 Training Loss: 4.46989154816\n",
      "Epoch: 40 itrerations: 1300 Training Loss: 4.45313549042\n",
      "Epoch: 40 itrerations: 1400 Training Loss: 4.49618148804\n",
      "Epoch: 40 itrerations: 1500 Training Loss: 4.48954296112\n",
      "Epoch: 40 itrerations: 1600 Training Loss: 4.51962089539\n",
      "Epoch: 40 itrerations: 1700 Training Loss: 4.51711082458\n",
      "Epoch: 40 itrerations: 1800 Training Loss: 4.53121805191\n",
      "Epoch: 40 itrerations: 1900 Training Loss: 4.54872131348\n",
      "Epoch: 40 itrerations: 2000 Training Loss: 4.56559371948\n",
      "Epoch: 40 itrerations: 2100 Training Loss: 4.52813386917\n",
      "Epoch: 40 itrerations: 2200 Training Loss: 4.48447370529\n",
      "Epoch: 40 itrerations: 2300 Training Loss: 4.5177230835\n",
      "Epoch: 40 itrerations: 2400 Training Loss: 4.47899389267\n",
      "Epoch: 40 itrerations: 2500 Training Loss: 4.45727109909\n",
      "Epoch: 40 itrerations: 2600 Training Loss: 4.44158983231\n",
      "Epoch: 40 itrerations: 2700 Training Loss: 4.4557056427\n",
      "Epoch: 40 itrerations: 2800 Training Loss: 4.42545890808\n",
      "Epoch: 40 itrerations: 2900 Training Loss: 4.4033241272\n",
      "Epoch: 40 itrerations: 3000 Training Loss: 4.43824911118\n",
      "Epoch: 40 itrerations: 3100 Training Loss: 4.41447925568\n",
      "Epoch: 40 itrerations: 3200 Training Loss: 4.4269490242\n",
      "Epoch: 40 itrerations: 3300 Training Loss: 4.40261411667\n",
      "Epoch: 40 itrerations: 3400 Training Loss: 4.41349697113\n",
      "Epoch: 40 itrerations: 3500 Training Loss: 4.41900873184\n",
      "Epoch: 40 itrerations: 3600 Training Loss: 4.41248655319\n",
      "Epoch: 40 itrerations: 3700 Training Loss: 4.40150785446\n",
      "Epoch: 40 itrerations: 3800 Training Loss: 4.40907287598\n",
      "Epoch: 40 itrerations: 3900 Training Loss: 4.4083366394\n",
      "Epoch: 40 itrerations: 4000 Training Loss: 4.41711807251\n",
      "Epoch: 40 itrerations: 4100 Training Loss: 4.42128372192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 itrerations: 4200 Training Loss: 4.39574861526\n",
      "Epoch: 40 itrerations: 4300 Training Loss: 4.40009307861\n",
      "Epoch: 40 itrerations: 4400 Training Loss: 4.40100860596\n",
      "Epoch: 40 itrerations: 4500 Training Loss: 4.39820814133\n",
      "Epoch: 40 itrerations: 4600 Training Loss: 4.38251924515\n",
      "Epoch: 40 itrerations: 4700 Training Loss: 4.383872509\n",
      "Epoch: 40 itrerations: 4800 Training Loss: 4.37283420563\n",
      "Epoch: 40 itrerations: 4900 Training Loss: 4.36609554291\n",
      "Epoch: 40 itrerations: 5000 Training Loss: 4.34509706497\n",
      "Epoch: 40 itrerations: 5100 Training Loss: 4.33467006683\n",
      "Epoch: 40 itrerations: 5200 Training Loss: 4.33222723007\n",
      "Epoch: 40 itrerations: 5300 Training Loss: 4.33159589767\n",
      "Epoch: 40 itrerations: 5400 Training Loss: 4.34589099884\n",
      "Epoch: 40 itrerations: 5500 Training Loss: 4.34232139587\n",
      "Epoch: 40 itrerations: 5600 Training Loss: 4.3421125412\n",
      "Epoch: 40 itrerations: 5700 Training Loss: 4.34595775604\n",
      "Epoch: 40 itrerations: 5800 Training Loss: 4.35068798065\n",
      "Epoch: 40 itrerations: 5900 Training Loss: 4.35165166855\n",
      "Epoch: 40 itrerations: 6000 Training Loss: 4.34706068039\n",
      "Epoch: 40 itrerations: 6100 Training Loss: 4.34178972244\n",
      "Epoch: 40 itrerations: 6200 Training Loss: 4.33956670761\n",
      "Epoch: 40 itrerations: 6300 Training Loss: 4.34226322174\n",
      "Epoch: 40 itrerations: 6400 Training Loss: 4.34064483643\n",
      "Epoch: 40 itrerations: 6500 Training Loss: 4.33639431\n",
      "Epoch: 40 itrerations: 6600 Training Loss: 4.32781410217\n",
      "Epoch: 40 itrerations: 6700 Training Loss: 4.32951641083\n",
      "Epoch: 40 itrerations: 6800 Training Loss: 4.33845424652\n",
      "Epoch: 40 itrerations: 6900 Training Loss: 4.34264802933\n",
      "Epoch: 40 itrerations: 7000 Training Loss: 4.35564947128\n",
      "Epoch: 40 itrerations: 7100 Training Loss: 4.36119747162\n",
      "Epoch: 40 itrerations: 7200 Training Loss: 4.35909175873\n",
      "Epoch: 40 itrerations: 7300 Training Loss: 4.35980606079\n",
      "Epoch: 40 itrerations: 7400 Training Loss: 4.35751390457\n",
      "Epoch: 40 itrerations: 7500 Training Loss: 4.35128307343\n",
      "Epoch: 40 itrerations: 7600 Training Loss: 4.34098434448\n",
      "Epoch: 40 itrerations: 7700 Training Loss: 4.32485628128\n",
      "Epoch: 40 itrerations: 7800 Training Loss: 4.32666683197\n",
      "Epoch: 40 itrerations: 7900 Training Loss: 4.33944177628\n",
      "Epoch: 40 itrerations: 8000 Training Loss: 4.3360414505\n",
      "Epoch: 40 itrerations: 8100 Training Loss: 4.33793401718\n",
      "Epoch: 40 itrerations: 8200 Training Loss: 4.34200716019\n",
      "Epoch: 40 itrerations: 8300 Training Loss: 4.34572076797\n",
      "Epoch: 40 itrerations: 8400 Training Loss: 4.34873247147\n",
      "Epoch: 40 itrerations: 8500 Training Loss: 4.34747314453\n",
      "Epoch: 40 itrerations: 8600 Training Loss: 4.34684467316\n",
      "Epoch: 40 itrerations: 8700 Training Loss: 4.35536432266\n",
      "Epoch: 40 itrerations: 8800 Training Loss: 4.35108566284\n",
      "Epoch: 40 itrerations: 8900 Training Loss: 4.34320735931\n",
      "Epoch: 40 itrerations: 9000 Training Loss: 4.35220575333\n",
      "Epoch: 40 itrerations: 9100 Training Loss: 4.3521437645\n",
      "Epoch: 40 itrerations: 9200 Training Loss: 4.35764980316\n",
      "Epoch: 40 itrerations: 9300 Training Loss: 4.34919214249\n",
      "Epoch: 40 itrerations: 9400 Training Loss: 4.33864021301\n",
      "Epoch: 40 itrerations: 9500 Training Loss: 4.33757686615\n",
      "Epoch: 40 itrerations: 9600 Training Loss: 4.35163640976\n",
      "Epoch: 40 itrerations: 9700 Training Loss: 4.34896183014\n",
      "Epoch: 40 itrerations: 9800 Training Loss: 4.34532165527\n",
      "Epoch: 40 itrerations: 9900 Training Loss: 4.34581947327\n",
      "Epoch: 40 itrerations: 10000 Training Loss: 4.36307859421\n",
      "Epoch: 40 itrerations: 10100 Training Loss: 4.35674476624\n",
      "Epoch: 40 itrerations: 10200 Training Loss: 4.35639572144\n",
      "Epoch: 40 itrerations: 10300 Training Loss: 4.35190248489\n",
      "Epoch: 40 itrerations: 10400 Training Loss: 4.34685754776\n",
      "Epoch: 40 itrerations: 10500 Training Loss: 4.33858251572\n",
      "Epoch: 40 itrerations: 10600 Training Loss: 4.33747625351\n",
      "Epoch: 40 itrerations: 10700 Training Loss: 4.33596944809\n",
      "Epoch: 41 itrerations: 0 Training Loss: 0.572692036629\n",
      "Epoch: 41 itrerations: 100 Training Loss: 4.212454319\n",
      "Epoch: 41 itrerations: 200 Training Loss: 3.88516283035\n",
      "Epoch: 41 itrerations: 300 Training Loss: 4.02203798294\n",
      "Epoch: 41 itrerations: 400 Training Loss: 4.10221004486\n",
      "Epoch: 41 itrerations: 500 Training Loss: 3.82740092278\n",
      "Epoch: 41 itrerations: 600 Training Loss: 3.92739319801\n",
      "Epoch: 41 itrerations: 700 Training Loss: 4.10509872437\n",
      "Epoch: 41 itrerations: 800 Training Loss: 4.28862714767\n",
      "Epoch: 41 itrerations: 900 Training Loss: 4.27076816559\n",
      "Epoch: 41 itrerations: 1000 Training Loss: 4.27894353867\n",
      "Epoch: 41 itrerations: 1100 Training Loss: 4.22937631607\n",
      "Epoch: 41 itrerations: 1200 Training Loss: 4.30521440506\n",
      "Epoch: 41 itrerations: 1300 Training Loss: 4.25003814697\n",
      "Epoch: 41 itrerations: 1400 Training Loss: 4.36004543304\n",
      "Epoch: 41 itrerations: 1500 Training Loss: 4.32736968994\n",
      "Epoch: 41 itrerations: 1600 Training Loss: 4.35700416565\n",
      "Epoch: 41 itrerations: 1700 Training Loss: 4.37605285645\n",
      "Epoch: 41 itrerations: 1800 Training Loss: 4.35195302963\n",
      "Epoch: 41 itrerations: 1900 Training Loss: 4.41256809235\n",
      "Epoch: 41 itrerations: 2000 Training Loss: 4.44258642197\n",
      "Epoch: 41 itrerations: 2100 Training Loss: 4.40615653992\n",
      "Epoch: 41 itrerations: 2200 Training Loss: 4.36701011658\n",
      "Epoch: 41 itrerations: 2300 Training Loss: 4.35419034958\n",
      "Epoch: 41 itrerations: 2400 Training Loss: 4.32451295853\n",
      "Epoch: 41 itrerations: 2500 Training Loss: 4.3027009964\n",
      "Epoch: 41 itrerations: 2600 Training Loss: 4.28811693192\n",
      "Epoch: 41 itrerations: 2700 Training Loss: 4.32724046707\n",
      "Epoch: 41 itrerations: 2800 Training Loss: 4.28885126114\n",
      "Epoch: 41 itrerations: 2900 Training Loss: 4.2655172348\n",
      "Epoch: 41 itrerations: 3000 Training Loss: 4.33659887314\n",
      "Epoch: 41 itrerations: 3100 Training Loss: 4.32119131088\n",
      "Epoch: 41 itrerations: 3200 Training Loss: 4.33521127701\n",
      "Epoch: 41 itrerations: 3300 Training Loss: 4.29947090149\n",
      "Epoch: 41 itrerations: 3400 Training Loss: 4.30026674271\n",
      "Epoch: 41 itrerations: 3500 Training Loss: 4.31675815582\n",
      "Epoch: 41 itrerations: 3600 Training Loss: 4.33172273636\n",
      "Epoch: 41 itrerations: 3700 Training Loss: 4.31853246689\n",
      "Epoch: 41 itrerations: 3800 Training Loss: 4.34645700455\n",
      "Epoch: 41 itrerations: 3900 Training Loss: 4.31942415237\n",
      "Epoch: 41 itrerations: 4000 Training Loss: 4.30531024933\n",
      "Epoch: 41 itrerations: 4100 Training Loss: 4.31131267548\n",
      "Epoch: 41 itrerations: 4200 Training Loss: 4.29385757446\n",
      "Epoch: 41 itrerations: 4300 Training Loss: 4.30279397964\n",
      "Epoch: 41 itrerations: 4400 Training Loss: 4.31482028961\n",
      "Epoch: 41 itrerations: 4500 Training Loss: 4.31795597076\n",
      "Epoch: 41 itrerations: 4600 Training Loss: 4.31892776489\n",
      "Epoch: 41 itrerations: 4700 Training Loss: 4.30929613113\n",
      "Epoch: 41 itrerations: 4800 Training Loss: 4.2927479744\n",
      "Epoch: 41 itrerations: 4900 Training Loss: 4.28337192535\n",
      "Epoch: 41 itrerations: 5000 Training Loss: 4.25775384903\n",
      "Epoch: 41 itrerations: 5100 Training Loss: 4.25268745422\n",
      "Epoch: 41 itrerations: 5200 Training Loss: 4.25551605225\n",
      "Epoch: 41 itrerations: 5300 Training Loss: 4.24735069275\n",
      "Epoch: 41 itrerations: 5400 Training Loss: 4.27295160294\n",
      "Epoch: 41 itrerations: 5500 Training Loss: 4.27300024033\n",
      "Epoch: 41 itrerations: 5600 Training Loss: 4.27695798874\n",
      "Epoch: 41 itrerations: 5700 Training Loss: 4.27192926407\n",
      "Epoch: 41 itrerations: 5800 Training Loss: 4.27215385437\n",
      "Epoch: 41 itrerations: 5900 Training Loss: 4.27263402939\n",
      "Epoch: 41 itrerations: 6000 Training Loss: 4.26614570618\n",
      "Epoch: 41 itrerations: 6100 Training Loss: 4.26161050797\n",
      "Epoch: 41 itrerations: 6200 Training Loss: 4.2682390213\n",
      "Epoch: 41 itrerations: 6300 Training Loss: 4.27546167374\n",
      "Epoch: 41 itrerations: 6400 Training Loss: 4.27194929123\n",
      "Epoch: 41 itrerations: 6500 Training Loss: 4.27396583557\n",
      "Epoch: 41 itrerations: 6600 Training Loss: 4.26303434372\n",
      "Epoch: 41 itrerations: 6700 Training Loss: 4.27206325531\n",
      "Epoch: 41 itrerations: 6800 Training Loss: 4.28117895126\n",
      "Epoch: 41 itrerations: 6900 Training Loss: 4.28615760803\n",
      "Epoch: 41 itrerations: 7000 Training Loss: 4.30148887634\n",
      "Epoch: 41 itrerations: 7100 Training Loss: 4.30803442001\n",
      "Epoch: 41 itrerations: 7200 Training Loss: 4.3070602417\n",
      "Epoch: 41 itrerations: 7300 Training Loss: 4.30231714249\n",
      "Epoch: 41 itrerations: 7400 Training Loss: 4.29755544662\n",
      "Epoch: 41 itrerations: 7500 Training Loss: 4.30206871033\n",
      "Epoch: 41 itrerations: 7600 Training Loss: 4.29918622971\n",
      "Epoch: 41 itrerations: 7700 Training Loss: 4.28673124313\n",
      "Epoch: 41 itrerations: 7800 Training Loss: 4.28784513474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 itrerations: 7900 Training Loss: 4.30973482132\n",
      "Epoch: 41 itrerations: 8000 Training Loss: 4.30812692642\n",
      "Epoch: 41 itrerations: 8100 Training Loss: 4.30748414993\n",
      "Epoch: 41 itrerations: 8200 Training Loss: 4.31254911423\n",
      "Epoch: 41 itrerations: 8300 Training Loss: 4.31443929672\n",
      "Epoch: 41 itrerations: 8400 Training Loss: 4.31525278091\n",
      "Epoch: 41 itrerations: 8500 Training Loss: 4.31413936615\n",
      "Epoch: 41 itrerations: 8600 Training Loss: 4.30486488342\n",
      "Epoch: 41 itrerations: 8700 Training Loss: 4.30482292175\n",
      "Epoch: 41 itrerations: 8800 Training Loss: 4.30879735947\n",
      "Epoch: 41 itrerations: 8900 Training Loss: 4.30043554306\n",
      "Epoch: 41 itrerations: 9000 Training Loss: 4.30586433411\n",
      "Epoch: 41 itrerations: 9100 Training Loss: 4.30766057968\n",
      "Epoch: 41 itrerations: 9200 Training Loss: 4.31114006042\n",
      "Epoch: 41 itrerations: 9300 Training Loss: 4.31025457382\n",
      "Epoch: 41 itrerations: 9400 Training Loss: 4.29559326172\n",
      "Epoch: 41 itrerations: 9500 Training Loss: 4.29640960693\n",
      "Epoch: 41 itrerations: 9600 Training Loss: 4.30964612961\n",
      "Epoch: 41 itrerations: 9700 Training Loss: 4.31052780151\n",
      "Epoch: 41 itrerations: 9800 Training Loss: 4.30485582352\n",
      "Epoch: 41 itrerations: 9900 Training Loss: 4.30481529236\n",
      "Epoch: 41 itrerations: 10000 Training Loss: 4.31353569031\n",
      "Epoch: 41 itrerations: 10100 Training Loss: 4.31514072418\n",
      "Epoch: 41 itrerations: 10200 Training Loss: 4.31144380569\n",
      "Epoch: 41 itrerations: 10300 Training Loss: 4.31132364273\n",
      "Epoch: 41 itrerations: 10400 Training Loss: 4.31327962875\n",
      "Epoch: 41 itrerations: 10500 Training Loss: 4.30542182922\n",
      "Epoch: 41 itrerations: 10600 Training Loss: 4.30539560318\n",
      "Epoch: 41 itrerations: 10700 Training Loss: 4.31328392029\n",
      "Epoch: 42 itrerations: 0 Training Loss: 5.54102277756\n",
      "Epoch: 42 itrerations: 100 Training Loss: 4.39291667938\n",
      "Epoch: 42 itrerations: 200 Training Loss: 4.18531560898\n",
      "Epoch: 42 itrerations: 300 Training Loss: 4.328312397\n",
      "Epoch: 42 itrerations: 400 Training Loss: 4.36862230301\n",
      "Epoch: 42 itrerations: 500 Training Loss: 4.06932640076\n",
      "Epoch: 42 itrerations: 600 Training Loss: 4.17174100876\n",
      "Epoch: 42 itrerations: 700 Training Loss: 4.16909694672\n",
      "Epoch: 42 itrerations: 800 Training Loss: 4.28077316284\n",
      "Epoch: 42 itrerations: 900 Training Loss: 4.23844146729\n",
      "Epoch: 42 itrerations: 1000 Training Loss: 4.25511598587\n",
      "Epoch: 42 itrerations: 1100 Training Loss: 4.22544145584\n",
      "Epoch: 42 itrerations: 1200 Training Loss: 4.32745933533\n",
      "Epoch: 42 itrerations: 1300 Training Loss: 4.30732822418\n",
      "Epoch: 42 itrerations: 1400 Training Loss: 4.42437171936\n",
      "Epoch: 42 itrerations: 1500 Training Loss: 4.37960290909\n",
      "Epoch: 42 itrerations: 1600 Training Loss: 4.42648935318\n",
      "Epoch: 42 itrerations: 1700 Training Loss: 4.42804193497\n",
      "Epoch: 42 itrerations: 1800 Training Loss: 4.39606618881\n",
      "Epoch: 42 itrerations: 1900 Training Loss: 4.40832471848\n",
      "Epoch: 42 itrerations: 2000 Training Loss: 4.43131971359\n",
      "Epoch: 42 itrerations: 2100 Training Loss: 4.38789653778\n",
      "Epoch: 42 itrerations: 2200 Training Loss: 4.35266780853\n",
      "Epoch: 42 itrerations: 2300 Training Loss: 4.40338802338\n",
      "Epoch: 42 itrerations: 2400 Training Loss: 4.37848997116\n",
      "Epoch: 42 itrerations: 2500 Training Loss: 4.3520860672\n",
      "Epoch: 42 itrerations: 2600 Training Loss: 4.3440952301\n",
      "Epoch: 42 itrerations: 2700 Training Loss: 4.35886573792\n",
      "Epoch: 42 itrerations: 2800 Training Loss: 4.346991539\n",
      "Epoch: 42 itrerations: 2900 Training Loss: 4.30358314514\n",
      "Epoch: 42 itrerations: 3000 Training Loss: 4.3306221962\n",
      "Epoch: 42 itrerations: 3100 Training Loss: 4.32089328766\n",
      "Epoch: 42 itrerations: 3200 Training Loss: 4.30384588242\n",
      "Epoch: 42 itrerations: 3300 Training Loss: 4.28383207321\n",
      "Epoch: 42 itrerations: 3400 Training Loss: 4.30021524429\n",
      "Epoch: 42 itrerations: 3500 Training Loss: 4.31952238083\n",
      "Epoch: 42 itrerations: 3600 Training Loss: 4.33473396301\n",
      "Epoch: 42 itrerations: 3700 Training Loss: 4.32175445557\n",
      "Epoch: 42 itrerations: 3800 Training Loss: 4.33017206192\n",
      "Epoch: 42 itrerations: 3900 Training Loss: 4.32536840439\n",
      "Epoch: 42 itrerations: 4000 Training Loss: 4.3435716629\n",
      "Epoch: 42 itrerations: 4100 Training Loss: 4.34401226044\n",
      "Epoch: 42 itrerations: 4200 Training Loss: 4.3192691803\n",
      "Epoch: 42 itrerations: 4300 Training Loss: 4.32134342194\n",
      "Epoch: 42 itrerations: 4400 Training Loss: 4.34332323074\n",
      "Epoch: 42 itrerations: 4500 Training Loss: 4.32772922516\n",
      "Epoch: 42 itrerations: 4600 Training Loss: 4.33500576019\n",
      "Epoch: 42 itrerations: 4700 Training Loss: 4.33618688583\n",
      "Epoch: 42 itrerations: 4800 Training Loss: 4.33315896988\n",
      "Epoch: 42 itrerations: 4900 Training Loss: 4.31125259399\n",
      "Epoch: 42 itrerations: 5000 Training Loss: 4.29090118408\n",
      "Epoch: 42 itrerations: 5100 Training Loss: 4.26914739609\n",
      "Epoch: 42 itrerations: 5200 Training Loss: 4.27046966553\n",
      "Epoch: 42 itrerations: 5300 Training Loss: 4.25925540924\n",
      "Epoch: 42 itrerations: 5400 Training Loss: 4.29127931595\n",
      "Epoch: 42 itrerations: 5500 Training Loss: 4.28886365891\n",
      "Epoch: 42 itrerations: 5600 Training Loss: 4.29635953903\n",
      "Epoch: 42 itrerations: 5700 Training Loss: 4.28608512878\n",
      "Epoch: 42 itrerations: 5800 Training Loss: 4.28319692612\n",
      "Epoch: 42 itrerations: 5900 Training Loss: 4.2873468399\n",
      "Epoch: 42 itrerations: 6000 Training Loss: 4.28231143951\n",
      "Epoch: 42 itrerations: 6100 Training Loss: 4.275203228\n",
      "Epoch: 42 itrerations: 6200 Training Loss: 4.28389978409\n",
      "Epoch: 42 itrerations: 6300 Training Loss: 4.27770042419\n",
      "Epoch: 42 itrerations: 6400 Training Loss: 4.27636671066\n",
      "Epoch: 42 itrerations: 6500 Training Loss: 4.27355384827\n",
      "Epoch: 42 itrerations: 6600 Training Loss: 4.27257061005\n",
      "Epoch: 42 itrerations: 6700 Training Loss: 4.27940273285\n",
      "Epoch: 42 itrerations: 6800 Training Loss: 4.28354930878\n",
      "Epoch: 42 itrerations: 6900 Training Loss: 4.29056930542\n",
      "Epoch: 42 itrerations: 7000 Training Loss: 4.31607580185\n",
      "Epoch: 42 itrerations: 7100 Training Loss: 4.3332862854\n",
      "Epoch: 42 itrerations: 7200 Training Loss: 4.3186879158\n",
      "Epoch: 42 itrerations: 7300 Training Loss: 4.32537937164\n",
      "Epoch: 42 itrerations: 7400 Training Loss: 4.31408882141\n",
      "Epoch: 42 itrerations: 7500 Training Loss: 4.3223733902\n",
      "Epoch: 42 itrerations: 7600 Training Loss: 4.32286119461\n",
      "Epoch: 42 itrerations: 7700 Training Loss: 4.31206464767\n",
      "Epoch: 42 itrerations: 7800 Training Loss: 4.31521701813\n",
      "Epoch: 42 itrerations: 7900 Training Loss: 4.31934070587\n",
      "Epoch: 42 itrerations: 8000 Training Loss: 4.31907558441\n",
      "Epoch: 42 itrerations: 8100 Training Loss: 4.31488418579\n",
      "Epoch: 42 itrerations: 8200 Training Loss: 4.32394313812\n",
      "Epoch: 42 itrerations: 8300 Training Loss: 4.33531904221\n",
      "Epoch: 42 itrerations: 8400 Training Loss: 4.3335442543\n",
      "Epoch: 42 itrerations: 8500 Training Loss: 4.33474159241\n",
      "Epoch: 42 itrerations: 8600 Training Loss: 4.33045434952\n",
      "Epoch: 42 itrerations: 8700 Training Loss: 4.33406114578\n",
      "Epoch: 42 itrerations: 8800 Training Loss: 4.32573890686\n",
      "Epoch: 42 itrerations: 8900 Training Loss: 4.32501745224\n",
      "Epoch: 42 itrerations: 9000 Training Loss: 4.33406162262\n",
      "Epoch: 42 itrerations: 9100 Training Loss: 4.33041143417\n",
      "Epoch: 42 itrerations: 9200 Training Loss: 4.33896589279\n",
      "Epoch: 42 itrerations: 9300 Training Loss: 4.33853197098\n",
      "Epoch: 42 itrerations: 9400 Training Loss: 4.32777452469\n",
      "Epoch: 42 itrerations: 9500 Training Loss: 4.32831811905\n",
      "Epoch: 42 itrerations: 9600 Training Loss: 4.34521341324\n",
      "Epoch: 42 itrerations: 9700 Training Loss: 4.34601306915\n",
      "Epoch: 42 itrerations: 9800 Training Loss: 4.34493684769\n",
      "Epoch: 42 itrerations: 9900 Training Loss: 4.35055112839\n",
      "Epoch: 42 itrerations: 10000 Training Loss: 4.36301660538\n",
      "Epoch: 42 itrerations: 10100 Training Loss: 4.36338758469\n",
      "Epoch: 42 itrerations: 10200 Training Loss: 4.36321973801\n",
      "Epoch: 42 itrerations: 10300 Training Loss: 4.36618757248\n",
      "Epoch: 42 itrerations: 10400 Training Loss: 4.36233758926\n",
      "Epoch: 42 itrerations: 10500 Training Loss: 4.35630750656\n",
      "Epoch: 42 itrerations: 10600 Training Loss: 4.35914897919\n",
      "Epoch: 42 itrerations: 10700 Training Loss: 4.35998296738\n",
      "Epoch: 43 itrerations: 0 Training Loss: 4.12511825562\n",
      "Epoch: 43 itrerations: 100 Training Loss: 4.30307102203\n",
      "Epoch: 43 itrerations: 200 Training Loss: 3.82887148857\n",
      "Epoch: 43 itrerations: 300 Training Loss: 4.19209289551\n",
      "Epoch: 43 itrerations: 400 Training Loss: 4.21988534927\n",
      "Epoch: 43 itrerations: 500 Training Loss: 3.90604805946\n",
      "Epoch: 43 itrerations: 600 Training Loss: 4.14600658417\n",
      "Epoch: 43 itrerations: 700 Training Loss: 4.15397930145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 itrerations: 800 Training Loss: 4.40114879608\n",
      "Epoch: 43 itrerations: 900 Training Loss: 4.36347818375\n",
      "Epoch: 43 itrerations: 1000 Training Loss: 4.36140918732\n",
      "Epoch: 43 itrerations: 1100 Training Loss: 4.30891942978\n",
      "Epoch: 43 itrerations: 1200 Training Loss: 4.41904354095\n",
      "Epoch: 43 itrerations: 1300 Training Loss: 4.43586349487\n",
      "Epoch: 43 itrerations: 1400 Training Loss: 4.53952789307\n",
      "Epoch: 43 itrerations: 1500 Training Loss: 4.49365520477\n",
      "Epoch: 43 itrerations: 1600 Training Loss: 4.46461725235\n",
      "Epoch: 43 itrerations: 1700 Training Loss: 4.45736980438\n",
      "Epoch: 43 itrerations: 1800 Training Loss: 4.44346237183\n",
      "Epoch: 43 itrerations: 1900 Training Loss: 4.49513101578\n",
      "Epoch: 43 itrerations: 2000 Training Loss: 4.49054765701\n",
      "Epoch: 43 itrerations: 2100 Training Loss: 4.45853281021\n",
      "Epoch: 43 itrerations: 2200 Training Loss: 4.44249582291\n",
      "Epoch: 43 itrerations: 2300 Training Loss: 4.43541955948\n",
      "Epoch: 43 itrerations: 2400 Training Loss: 4.40699386597\n",
      "Epoch: 43 itrerations: 2500 Training Loss: 4.38621044159\n",
      "Epoch: 43 itrerations: 2600 Training Loss: 4.37710666656\n",
      "Epoch: 43 itrerations: 2700 Training Loss: 4.42012691498\n",
      "Epoch: 43 itrerations: 2800 Training Loss: 4.39000511169\n",
      "Epoch: 43 itrerations: 2900 Training Loss: 4.34881734848\n",
      "Epoch: 43 itrerations: 3000 Training Loss: 4.40206241608\n",
      "Epoch: 43 itrerations: 3100 Training Loss: 4.37035417557\n",
      "Epoch: 43 itrerations: 3200 Training Loss: 4.39076280594\n",
      "Epoch: 43 itrerations: 3300 Training Loss: 4.36743593216\n",
      "Epoch: 43 itrerations: 3400 Training Loss: 4.3782377243\n",
      "Epoch: 43 itrerations: 3500 Training Loss: 4.40737915039\n",
      "Epoch: 43 itrerations: 3600 Training Loss: 4.40739679337\n",
      "Epoch: 43 itrerations: 3700 Training Loss: 4.40446949005\n",
      "Epoch: 43 itrerations: 3800 Training Loss: 4.40616035461\n",
      "Epoch: 43 itrerations: 3900 Training Loss: 4.37706804276\n",
      "Epoch: 43 itrerations: 4000 Training Loss: 4.36911487579\n",
      "Epoch: 43 itrerations: 4100 Training Loss: 4.35579586029\n",
      "Epoch: 43 itrerations: 4200 Training Loss: 4.33026885986\n",
      "Epoch: 43 itrerations: 4300 Training Loss: 4.3370013237\n",
      "Epoch: 43 itrerations: 4400 Training Loss: 4.34759378433\n",
      "Epoch: 43 itrerations: 4500 Training Loss: 4.34917068481\n",
      "Epoch: 43 itrerations: 4600 Training Loss: 4.3476190567\n",
      "Epoch: 43 itrerations: 4700 Training Loss: 4.35111951828\n",
      "Epoch: 43 itrerations: 4800 Training Loss: 4.35335445404\n",
      "Epoch: 43 itrerations: 4900 Training Loss: 4.33950424194\n",
      "Epoch: 43 itrerations: 5000 Training Loss: 4.31982278824\n",
      "Epoch: 43 itrerations: 5100 Training Loss: 4.31108617783\n",
      "Epoch: 43 itrerations: 5200 Training Loss: 4.30121612549\n",
      "Epoch: 43 itrerations: 5300 Training Loss: 4.28638887405\n",
      "Epoch: 43 itrerations: 5400 Training Loss: 4.31525325775\n",
      "Epoch: 43 itrerations: 5500 Training Loss: 4.31241750717\n",
      "Epoch: 43 itrerations: 5600 Training Loss: 4.31986522675\n",
      "Epoch: 43 itrerations: 5700 Training Loss: 4.31515932083\n",
      "Epoch: 43 itrerations: 5800 Training Loss: 4.32014608383\n",
      "Epoch: 43 itrerations: 5900 Training Loss: 4.33712387085\n",
      "Epoch: 43 itrerations: 6000 Training Loss: 4.34881067276\n",
      "Epoch: 43 itrerations: 6100 Training Loss: 4.34625101089\n",
      "Epoch: 43 itrerations: 6200 Training Loss: 4.35457849503\n",
      "Epoch: 43 itrerations: 6300 Training Loss: 4.35752248764\n",
      "Epoch: 43 itrerations: 6400 Training Loss: 4.35259628296\n",
      "Epoch: 43 itrerations: 6500 Training Loss: 4.34986400604\n",
      "Epoch: 43 itrerations: 6600 Training Loss: 4.34895467758\n",
      "Epoch: 43 itrerations: 6700 Training Loss: 4.34974479675\n",
      "Epoch: 43 itrerations: 6800 Training Loss: 4.35547399521\n",
      "Epoch: 43 itrerations: 6900 Training Loss: 4.37153863907\n",
      "Epoch: 43 itrerations: 7000 Training Loss: 4.38478803635\n",
      "Epoch: 43 itrerations: 7100 Training Loss: 4.39272975922\n",
      "Epoch: 43 itrerations: 7200 Training Loss: 4.39188528061\n",
      "Epoch: 43 itrerations: 7300 Training Loss: 4.40012836456\n",
      "Epoch: 43 itrerations: 7400 Training Loss: 4.40101909637\n",
      "Epoch: 43 itrerations: 7500 Training Loss: 4.3976855278\n",
      "Epoch: 43 itrerations: 7600 Training Loss: 4.3881020546\n",
      "Epoch: 43 itrerations: 7700 Training Loss: 4.38166856766\n",
      "Epoch: 43 itrerations: 7800 Training Loss: 4.38007116318\n",
      "Epoch: 43 itrerations: 7900 Training Loss: 4.39427471161\n",
      "Epoch: 43 itrerations: 8000 Training Loss: 4.3878326416\n",
      "Epoch: 43 itrerations: 8100 Training Loss: 4.38129663467\n",
      "Epoch: 43 itrerations: 8200 Training Loss: 4.38006687164\n",
      "Epoch: 43 itrerations: 8300 Training Loss: 4.38198566437\n",
      "Epoch: 43 itrerations: 8400 Training Loss: 4.37999916077\n",
      "Epoch: 43 itrerations: 8500 Training Loss: 4.3852148056\n",
      "Epoch: 43 itrerations: 8600 Training Loss: 4.37786483765\n",
      "Epoch: 43 itrerations: 8700 Training Loss: 4.38257741928\n",
      "Epoch: 43 itrerations: 8800 Training Loss: 4.3815946579\n",
      "Epoch: 43 itrerations: 8900 Training Loss: 4.37688875198\n",
      "Epoch: 43 itrerations: 9000 Training Loss: 4.38608884811\n",
      "Epoch: 43 itrerations: 9100 Training Loss: 4.38203334808\n",
      "Epoch: 43 itrerations: 9200 Training Loss: 4.39208650589\n",
      "Epoch: 43 itrerations: 9300 Training Loss: 4.38947105408\n",
      "Epoch: 43 itrerations: 9400 Training Loss: 4.37974691391\n",
      "Epoch: 43 itrerations: 9500 Training Loss: 4.38117837906\n",
      "Epoch: 43 itrerations: 9600 Training Loss: 4.39259052277\n",
      "Epoch: 43 itrerations: 9700 Training Loss: 4.39515972137\n",
      "Epoch: 43 itrerations: 9800 Training Loss: 4.39781808853\n",
      "Epoch: 43 itrerations: 9900 Training Loss: 4.39545249939\n",
      "Epoch: 43 itrerations: 10000 Training Loss: 4.40821790695\n",
      "Epoch: 43 itrerations: 10100 Training Loss: 4.41499662399\n",
      "Epoch: 43 itrerations: 10200 Training Loss: 4.41408157349\n",
      "Epoch: 43 itrerations: 10300 Training Loss: 4.41689682007\n",
      "Epoch: 43 itrerations: 10400 Training Loss: 4.41007328033\n",
      "Epoch: 43 itrerations: 10500 Training Loss: 4.40469026566\n",
      "Epoch: 43 itrerations: 10600 Training Loss: 4.40253734589\n",
      "Epoch: 43 itrerations: 10700 Training Loss: 4.40129184723\n",
      "Epoch: 44 itrerations: 0 Training Loss: 2.06609988213\n",
      "Epoch: 44 itrerations: 100 Training Loss: 4.55462503433\n",
      "Epoch: 44 itrerations: 200 Training Loss: 4.08091831207\n",
      "Epoch: 44 itrerations: 300 Training Loss: 4.11493015289\n",
      "Epoch: 44 itrerations: 400 Training Loss: 4.09872579575\n",
      "Epoch: 44 itrerations: 500 Training Loss: 3.94707727432\n",
      "Epoch: 44 itrerations: 600 Training Loss: 4.12429904938\n",
      "Epoch: 44 itrerations: 700 Training Loss: 4.17988824844\n",
      "Epoch: 44 itrerations: 800 Training Loss: 4.2517156601\n",
      "Epoch: 44 itrerations: 900 Training Loss: 4.27879142761\n",
      "Epoch: 44 itrerations: 1000 Training Loss: 4.27686691284\n",
      "Epoch: 44 itrerations: 1100 Training Loss: 4.24953174591\n",
      "Epoch: 44 itrerations: 1200 Training Loss: 4.34568023682\n",
      "Epoch: 44 itrerations: 1300 Training Loss: 4.36730575562\n",
      "Epoch: 44 itrerations: 1400 Training Loss: 4.43876934052\n",
      "Epoch: 44 itrerations: 1500 Training Loss: 4.3695230484\n",
      "Epoch: 44 itrerations: 1600 Training Loss: 4.35841417313\n",
      "Epoch: 44 itrerations: 1700 Training Loss: 4.37128257751\n",
      "Epoch: 44 itrerations: 1800 Training Loss: 4.36239719391\n",
      "Epoch: 44 itrerations: 1900 Training Loss: 4.39999055862\n",
      "Epoch: 44 itrerations: 2000 Training Loss: 4.41487073898\n",
      "Epoch: 44 itrerations: 2100 Training Loss: 4.4010424614\n",
      "Epoch: 44 itrerations: 2200 Training Loss: 4.38604593277\n",
      "Epoch: 44 itrerations: 2300 Training Loss: 4.41749095917\n",
      "Epoch: 44 itrerations: 2400 Training Loss: 4.38415002823\n",
      "Epoch: 44 itrerations: 2500 Training Loss: 4.36948871613\n",
      "Epoch: 44 itrerations: 2600 Training Loss: 4.38623666763\n",
      "Epoch: 44 itrerations: 2700 Training Loss: 4.42533922195\n",
      "Epoch: 44 itrerations: 2800 Training Loss: 4.37941455841\n",
      "Epoch: 44 itrerations: 2900 Training Loss: 4.34814500809\n",
      "Epoch: 44 itrerations: 3000 Training Loss: 4.36763381958\n",
      "Epoch: 44 itrerations: 3100 Training Loss: 4.34161615372\n",
      "Epoch: 44 itrerations: 3200 Training Loss: 4.34873914719\n",
      "Epoch: 44 itrerations: 3300 Training Loss: 4.33814001083\n",
      "Epoch: 44 itrerations: 3400 Training Loss: 4.36022758484\n",
      "Epoch: 44 itrerations: 3500 Training Loss: 4.40169334412\n",
      "Epoch: 44 itrerations: 3600 Training Loss: 4.40854978561\n",
      "Epoch: 44 itrerations: 3700 Training Loss: 4.40187931061\n",
      "Epoch: 44 itrerations: 3800 Training Loss: 4.43117141724\n",
      "Epoch: 44 itrerations: 3900 Training Loss: 4.41419219971\n",
      "Epoch: 44 itrerations: 4000 Training Loss: 4.40613031387\n",
      "Epoch: 44 itrerations: 4100 Training Loss: 4.39660835266\n",
      "Epoch: 44 itrerations: 4200 Training Loss: 4.37885570526\n",
      "Epoch: 44 itrerations: 4300 Training Loss: 4.37001085281\n",
      "Epoch: 44 itrerations: 4400 Training Loss: 4.38744544983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 itrerations: 4500 Training Loss: 4.38214588165\n",
      "Epoch: 44 itrerations: 4600 Training Loss: 4.368019104\n",
      "Epoch: 44 itrerations: 4700 Training Loss: 4.36493444443\n",
      "Epoch: 44 itrerations: 4800 Training Loss: 4.34744262695\n",
      "Epoch: 44 itrerations: 4900 Training Loss: 4.34176206589\n",
      "Epoch: 44 itrerations: 5000 Training Loss: 4.32255935669\n",
      "Epoch: 44 itrerations: 5100 Training Loss: 4.31712627411\n",
      "Epoch: 44 itrerations: 5200 Training Loss: 4.31399965286\n",
      "Epoch: 44 itrerations: 5300 Training Loss: 4.3043680191\n",
      "Epoch: 44 itrerations: 5400 Training Loss: 4.32495117188\n",
      "Epoch: 44 itrerations: 5500 Training Loss: 4.31489229202\n",
      "Epoch: 44 itrerations: 5600 Training Loss: 4.310526371\n",
      "Epoch: 44 itrerations: 5700 Training Loss: 4.314702034\n",
      "Epoch: 44 itrerations: 5800 Training Loss: 4.31183338165\n",
      "Epoch: 44 itrerations: 5900 Training Loss: 4.31833934784\n",
      "Epoch: 44 itrerations: 6000 Training Loss: 4.31427431107\n",
      "Epoch: 44 itrerations: 6100 Training Loss: 4.30692577362\n",
      "Epoch: 44 itrerations: 6200 Training Loss: 4.31638908386\n",
      "Epoch: 44 itrerations: 6300 Training Loss: 4.31700706482\n",
      "Epoch: 44 itrerations: 6400 Training Loss: 4.30724191666\n",
      "Epoch: 44 itrerations: 6500 Training Loss: 4.3027176857\n",
      "Epoch: 44 itrerations: 6600 Training Loss: 4.29167222977\n",
      "Epoch: 44 itrerations: 6700 Training Loss: 4.30640554428\n",
      "Epoch: 44 itrerations: 6800 Training Loss: 4.31713294983\n",
      "Epoch: 44 itrerations: 6900 Training Loss: 4.33087062836\n",
      "Epoch: 44 itrerations: 7000 Training Loss: 4.35312318802\n",
      "Epoch: 44 itrerations: 7100 Training Loss: 4.34660196304\n",
      "Epoch: 44 itrerations: 7200 Training Loss: 4.32526922226\n",
      "Epoch: 44 itrerations: 7300 Training Loss: 4.32296228409\n",
      "Epoch: 44 itrerations: 7400 Training Loss: 4.3204369545\n",
      "Epoch: 44 itrerations: 7500 Training Loss: 4.32552528381\n",
      "Epoch: 44 itrerations: 7600 Training Loss: 4.32171535492\n",
      "Epoch: 44 itrerations: 7700 Training Loss: 4.3042383194\n",
      "Epoch: 44 itrerations: 7800 Training Loss: 4.30747890472\n",
      "Epoch: 44 itrerations: 7900 Training Loss: 4.31459999084\n",
      "Epoch: 44 itrerations: 8000 Training Loss: 4.30743312836\n",
      "Epoch: 44 itrerations: 8100 Training Loss: 4.30666208267\n",
      "Epoch: 44 itrerations: 8200 Training Loss: 4.30983066559\n",
      "Epoch: 44 itrerations: 8300 Training Loss: 4.31445932388\n",
      "Epoch: 44 itrerations: 8400 Training Loss: 4.31572341919\n",
      "Epoch: 44 itrerations: 8500 Training Loss: 4.3213429451\n",
      "Epoch: 44 itrerations: 8600 Training Loss: 4.32105588913\n",
      "Epoch: 44 itrerations: 8700 Training Loss: 4.31820583344\n",
      "Epoch: 44 itrerations: 8800 Training Loss: 4.3180065155\n",
      "Epoch: 44 itrerations: 8900 Training Loss: 4.31882715225\n",
      "Epoch: 44 itrerations: 9000 Training Loss: 4.32028245926\n",
      "Epoch: 44 itrerations: 9100 Training Loss: 4.31524276733\n",
      "Epoch: 44 itrerations: 9200 Training Loss: 4.31966161728\n",
      "Epoch: 44 itrerations: 9300 Training Loss: 4.31437063217\n",
      "Epoch: 44 itrerations: 9400 Training Loss: 4.30235338211\n",
      "Epoch: 44 itrerations: 9500 Training Loss: 4.30137968063\n",
      "Epoch: 44 itrerations: 9600 Training Loss: 4.32004308701\n",
      "Epoch: 44 itrerations: 9700 Training Loss: 4.32343006134\n",
      "Epoch: 44 itrerations: 9800 Training Loss: 4.32363653183\n",
      "Epoch: 44 itrerations: 9900 Training Loss: 4.32414007187\n",
      "Epoch: 44 itrerations: 10000 Training Loss: 4.3339881897\n",
      "Epoch: 44 itrerations: 10100 Training Loss: 4.34001588821\n",
      "Epoch: 44 itrerations: 10200 Training Loss: 4.33656311035\n",
      "Epoch: 44 itrerations: 10300 Training Loss: 4.34240627289\n",
      "Epoch: 44 itrerations: 10400 Training Loss: 4.34496831894\n",
      "Epoch: 44 itrerations: 10500 Training Loss: 4.33380460739\n",
      "Epoch: 44 itrerations: 10600 Training Loss: 4.33475542068\n",
      "Epoch: 44 itrerations: 10700 Training Loss: 4.33958053589\n",
      "Epoch: 45 itrerations: 0 Training Loss: 4.86460065842\n",
      "Epoch: 45 itrerations: 100 Training Loss: 3.78732299805\n",
      "Epoch: 45 itrerations: 200 Training Loss: 3.68582630157\n",
      "Epoch: 45 itrerations: 300 Training Loss: 3.66617226601\n",
      "Epoch: 45 itrerations: 400 Training Loss: 3.68005156517\n",
      "Epoch: 45 itrerations: 500 Training Loss: 3.4195997715\n",
      "Epoch: 45 itrerations: 600 Training Loss: 3.72489786148\n",
      "Epoch: 45 itrerations: 700 Training Loss: 3.85152697563\n",
      "Epoch: 45 itrerations: 800 Training Loss: 4.05655145645\n",
      "Epoch: 45 itrerations: 900 Training Loss: 4.04327344894\n",
      "Epoch: 45 itrerations: 1000 Training Loss: 4.11894369125\n",
      "Epoch: 45 itrerations: 1100 Training Loss: 4.09256219864\n",
      "Epoch: 45 itrerations: 1200 Training Loss: 4.19189405441\n",
      "Epoch: 45 itrerations: 1300 Training Loss: 4.1842083931\n",
      "Epoch: 45 itrerations: 1400 Training Loss: 4.25305652618\n",
      "Epoch: 45 itrerations: 1500 Training Loss: 4.24924707413\n",
      "Epoch: 45 itrerations: 1600 Training Loss: 4.29905176163\n",
      "Epoch: 45 itrerations: 1700 Training Loss: 4.30489587784\n",
      "Epoch: 45 itrerations: 1800 Training Loss: 4.30789709091\n",
      "Epoch: 45 itrerations: 1900 Training Loss: 4.35643768311\n",
      "Epoch: 45 itrerations: 2000 Training Loss: 4.35755443573\n",
      "Epoch: 45 itrerations: 2100 Training Loss: 4.31333208084\n",
      "Epoch: 45 itrerations: 2200 Training Loss: 4.27476215363\n",
      "Epoch: 45 itrerations: 2300 Training Loss: 4.29831981659\n",
      "Epoch: 45 itrerations: 2400 Training Loss: 4.26778030396\n",
      "Epoch: 45 itrerations: 2500 Training Loss: 4.22279119492\n",
      "Epoch: 45 itrerations: 2600 Training Loss: 4.22352266312\n",
      "Epoch: 45 itrerations: 2700 Training Loss: 4.26087474823\n",
      "Epoch: 45 itrerations: 2800 Training Loss: 4.25079631805\n",
      "Epoch: 45 itrerations: 2900 Training Loss: 4.22239923477\n",
      "Epoch: 45 itrerations: 3000 Training Loss: 4.281935215\n",
      "Epoch: 45 itrerations: 3100 Training Loss: 4.2555270195\n",
      "Epoch: 45 itrerations: 3200 Training Loss: 4.24892711639\n",
      "Epoch: 45 itrerations: 3300 Training Loss: 4.22674894333\n",
      "Epoch: 45 itrerations: 3400 Training Loss: 4.24207496643\n",
      "Epoch: 45 itrerations: 3500 Training Loss: 4.27046489716\n",
      "Epoch: 45 itrerations: 3600 Training Loss: 4.27258872986\n",
      "Epoch: 45 itrerations: 3700 Training Loss: 4.27239084244\n",
      "Epoch: 45 itrerations: 3800 Training Loss: 4.29718255997\n",
      "Epoch: 45 itrerations: 3900 Training Loss: 4.28060817719\n",
      "Epoch: 45 itrerations: 4000 Training Loss: 4.29318857193\n",
      "Epoch: 45 itrerations: 4100 Training Loss: 4.2953119278\n",
      "Epoch: 45 itrerations: 4200 Training Loss: 4.29486370087\n",
      "Epoch: 45 itrerations: 4300 Training Loss: 4.29241895676\n",
      "Epoch: 45 itrerations: 4400 Training Loss: 4.31279277802\n",
      "Epoch: 45 itrerations: 4500 Training Loss: 4.30900669098\n",
      "Epoch: 45 itrerations: 4600 Training Loss: 4.30649423599\n",
      "Epoch: 45 itrerations: 4700 Training Loss: 4.30284261703\n",
      "Epoch: 45 itrerations: 4800 Training Loss: 4.29560184479\n",
      "Epoch: 45 itrerations: 4900 Training Loss: 4.29589128494\n",
      "Epoch: 45 itrerations: 5000 Training Loss: 4.27400875092\n",
      "Epoch: 45 itrerations: 5100 Training Loss: 4.25935649872\n",
      "Epoch: 45 itrerations: 5200 Training Loss: 4.26008558273\n",
      "Epoch: 45 itrerations: 5300 Training Loss: 4.2668056488\n",
      "Epoch: 45 itrerations: 5400 Training Loss: 4.27239370346\n",
      "Epoch: 45 itrerations: 5500 Training Loss: 4.27050542831\n",
      "Epoch: 45 itrerations: 5600 Training Loss: 4.26802635193\n",
      "Epoch: 45 itrerations: 5700 Training Loss: 4.25702190399\n",
      "Epoch: 45 itrerations: 5800 Training Loss: 4.27995824814\n",
      "Epoch: 45 itrerations: 5900 Training Loss: 4.28503799438\n",
      "Epoch: 45 itrerations: 6000 Training Loss: 4.27682113647\n",
      "Epoch: 45 itrerations: 6100 Training Loss: 4.28045511246\n",
      "Epoch: 45 itrerations: 6200 Training Loss: 4.29151439667\n",
      "Epoch: 45 itrerations: 6300 Training Loss: 4.28919649124\n",
      "Epoch: 45 itrerations: 6400 Training Loss: 4.29620981216\n",
      "Epoch: 45 itrerations: 6500 Training Loss: 4.29045724869\n",
      "Epoch: 45 itrerations: 6600 Training Loss: 4.28000116348\n",
      "Epoch: 45 itrerations: 6700 Training Loss: 4.28489208221\n",
      "Epoch: 45 itrerations: 6800 Training Loss: 4.29244947433\n",
      "Epoch: 45 itrerations: 6900 Training Loss: 4.31036663055\n",
      "Epoch: 45 itrerations: 7000 Training Loss: 4.3324432373\n",
      "Epoch: 45 itrerations: 7100 Training Loss: 4.34223842621\n",
      "Epoch: 45 itrerations: 7200 Training Loss: 4.32917833328\n",
      "Epoch: 45 itrerations: 7300 Training Loss: 4.31870508194\n",
      "Epoch: 45 itrerations: 7400 Training Loss: 4.31043195724\n",
      "Epoch: 45 itrerations: 7500 Training Loss: 4.31247663498\n",
      "Epoch: 45 itrerations: 7600 Training Loss: 4.30416440964\n",
      "Epoch: 45 itrerations: 7700 Training Loss: 4.28634357452\n",
      "Epoch: 45 itrerations: 7800 Training Loss: 4.29745674133\n",
      "Epoch: 45 itrerations: 7900 Training Loss: 4.31229734421\n",
      "Epoch: 45 itrerations: 8000 Training Loss: 4.31119251251\n",
      "Epoch: 45 itrerations: 8100 Training Loss: 4.30588388443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 itrerations: 8200 Training Loss: 4.30957269669\n",
      "Epoch: 45 itrerations: 8300 Training Loss: 4.30896806717\n",
      "Epoch: 45 itrerations: 8400 Training Loss: 4.31337594986\n",
      "Epoch: 45 itrerations: 8500 Training Loss: 4.31849479675\n",
      "Epoch: 45 itrerations: 8600 Training Loss: 4.3117146492\n",
      "Epoch: 45 itrerations: 8700 Training Loss: 4.31057929993\n",
      "Epoch: 45 itrerations: 8800 Training Loss: 4.31127166748\n",
      "Epoch: 45 itrerations: 8900 Training Loss: 4.3107380867\n",
      "Epoch: 45 itrerations: 9000 Training Loss: 4.31261730194\n",
      "Epoch: 45 itrerations: 9100 Training Loss: 4.30514907837\n",
      "Epoch: 45 itrerations: 9200 Training Loss: 4.31019496918\n",
      "Epoch: 45 itrerations: 9300 Training Loss: 4.31150817871\n",
      "Epoch: 45 itrerations: 9400 Training Loss: 4.30557823181\n",
      "Epoch: 45 itrerations: 9500 Training Loss: 4.30525016785\n",
      "Epoch: 45 itrerations: 9600 Training Loss: 4.31839704514\n",
      "Epoch: 45 itrerations: 9700 Training Loss: 4.32386064529\n",
      "Epoch: 45 itrerations: 9800 Training Loss: 4.31919193268\n",
      "Epoch: 45 itrerations: 9900 Training Loss: 4.32730340958\n",
      "Epoch: 45 itrerations: 10000 Training Loss: 4.33617162704\n",
      "Epoch: 45 itrerations: 10100 Training Loss: 4.33650922775\n",
      "Epoch: 45 itrerations: 10200 Training Loss: 4.33954381943\n",
      "Epoch: 45 itrerations: 10300 Training Loss: 4.34096002579\n",
      "Epoch: 45 itrerations: 10400 Training Loss: 4.33639287949\n",
      "Epoch: 45 itrerations: 10500 Training Loss: 4.32676172256\n",
      "Epoch: 45 itrerations: 10600 Training Loss: 4.32826328278\n",
      "Epoch: 45 itrerations: 10700 Training Loss: 4.33062124252\n",
      "Epoch: 46 itrerations: 0 Training Loss: 3.67937350273\n",
      "Epoch: 46 itrerations: 100 Training Loss: 3.90520787239\n",
      "Epoch: 46 itrerations: 200 Training Loss: 3.48342299461\n",
      "Epoch: 46 itrerations: 300 Training Loss: 3.6945950985\n",
      "Epoch: 46 itrerations: 400 Training Loss: 3.9526245594\n",
      "Epoch: 46 itrerations: 500 Training Loss: 3.72790074348\n",
      "Epoch: 46 itrerations: 600 Training Loss: 3.91047143936\n",
      "Epoch: 46 itrerations: 700 Training Loss: 3.99785804749\n",
      "Epoch: 46 itrerations: 800 Training Loss: 4.23983192444\n",
      "Epoch: 46 itrerations: 900 Training Loss: 4.27205562592\n",
      "Epoch: 46 itrerations: 1000 Training Loss: 4.30663299561\n",
      "Epoch: 46 itrerations: 1100 Training Loss: 4.28624391556\n",
      "Epoch: 46 itrerations: 1200 Training Loss: 4.42257404327\n",
      "Epoch: 46 itrerations: 1300 Training Loss: 4.36734199524\n",
      "Epoch: 46 itrerations: 1400 Training Loss: 4.47488832474\n",
      "Epoch: 46 itrerations: 1500 Training Loss: 4.46138095856\n",
      "Epoch: 46 itrerations: 1600 Training Loss: 4.47987222672\n",
      "Epoch: 46 itrerations: 1700 Training Loss: 4.46345567703\n",
      "Epoch: 46 itrerations: 1800 Training Loss: 4.45984268188\n",
      "Epoch: 46 itrerations: 1900 Training Loss: 4.46667098999\n",
      "Epoch: 46 itrerations: 2000 Training Loss: 4.47664546967\n",
      "Epoch: 46 itrerations: 2100 Training Loss: 4.44264650345\n",
      "Epoch: 46 itrerations: 2200 Training Loss: 4.42266654968\n",
      "Epoch: 46 itrerations: 2300 Training Loss: 4.44355201721\n",
      "Epoch: 46 itrerations: 2400 Training Loss: 4.4074177742\n",
      "Epoch: 46 itrerations: 2500 Training Loss: 4.40593862534\n",
      "Epoch: 46 itrerations: 2600 Training Loss: 4.41395902634\n",
      "Epoch: 46 itrerations: 2700 Training Loss: 4.4500541687\n",
      "Epoch: 46 itrerations: 2800 Training Loss: 4.41150379181\n",
      "Epoch: 46 itrerations: 2900 Training Loss: 4.36251592636\n",
      "Epoch: 46 itrerations: 3000 Training Loss: 4.41310596466\n",
      "Epoch: 46 itrerations: 3100 Training Loss: 4.39438009262\n",
      "Epoch: 46 itrerations: 3200 Training Loss: 4.40421390533\n",
      "Epoch: 46 itrerations: 3300 Training Loss: 4.37636041641\n",
      "Epoch: 46 itrerations: 3400 Training Loss: 4.38605880737\n",
      "Epoch: 46 itrerations: 3500 Training Loss: 4.4138879776\n",
      "Epoch: 46 itrerations: 3600 Training Loss: 4.41799211502\n",
      "Epoch: 46 itrerations: 3700 Training Loss: 4.4076757431\n",
      "Epoch: 46 itrerations: 3800 Training Loss: 4.39994668961\n",
      "Epoch: 46 itrerations: 3900 Training Loss: 4.37305927277\n",
      "Epoch: 46 itrerations: 4000 Training Loss: 4.37660312653\n",
      "Epoch: 46 itrerations: 4100 Training Loss: 4.37424087524\n",
      "Epoch: 46 itrerations: 4200 Training Loss: 4.36676740646\n",
      "Epoch: 46 itrerations: 4300 Training Loss: 4.35925674438\n",
      "Epoch: 46 itrerations: 4400 Training Loss: 4.37016153336\n",
      "Epoch: 46 itrerations: 4500 Training Loss: 4.36922454834\n",
      "Epoch: 46 itrerations: 4600 Training Loss: 4.36029434204\n",
      "Epoch: 46 itrerations: 4700 Training Loss: 4.3640255928\n",
      "Epoch: 46 itrerations: 4800 Training Loss: 4.34163188934\n",
      "Epoch: 46 itrerations: 4900 Training Loss: 4.33248090744\n",
      "Epoch: 46 itrerations: 5000 Training Loss: 4.3116941452\n",
      "Epoch: 46 itrerations: 5100 Training Loss: 4.29822874069\n",
      "Epoch: 46 itrerations: 5200 Training Loss: 4.30376815796\n",
      "Epoch: 46 itrerations: 5300 Training Loss: 4.30466127396\n",
      "Epoch: 46 itrerations: 5400 Training Loss: 4.31262493134\n",
      "Epoch: 46 itrerations: 5500 Training Loss: 4.30953979492\n",
      "Epoch: 46 itrerations: 5600 Training Loss: 4.30390930176\n",
      "Epoch: 46 itrerations: 5700 Training Loss: 4.29480791092\n",
      "Epoch: 46 itrerations: 5800 Training Loss: 4.30522537231\n",
      "Epoch: 46 itrerations: 5900 Training Loss: 4.30965089798\n",
      "Epoch: 46 itrerations: 6000 Training Loss: 4.30419015884\n",
      "Epoch: 46 itrerations: 6100 Training Loss: 4.30266857147\n",
      "Epoch: 46 itrerations: 6200 Training Loss: 4.31108570099\n",
      "Epoch: 46 itrerations: 6300 Training Loss: 4.30577898026\n",
      "Epoch: 46 itrerations: 6400 Training Loss: 4.29339170456\n",
      "Epoch: 46 itrerations: 6500 Training Loss: 4.28417825699\n",
      "Epoch: 46 itrerations: 6600 Training Loss: 4.27026367188\n",
      "Epoch: 46 itrerations: 6700 Training Loss: 4.27707862854\n",
      "Epoch: 46 itrerations: 6800 Training Loss: 4.28298044205\n",
      "Epoch: 46 itrerations: 6900 Training Loss: 4.3071680069\n",
      "Epoch: 46 itrerations: 7000 Training Loss: 4.32556915283\n",
      "Epoch: 46 itrerations: 7100 Training Loss: 4.32784461975\n",
      "Epoch: 46 itrerations: 7200 Training Loss: 4.31762886047\n",
      "Epoch: 46 itrerations: 7300 Training Loss: 4.32298088074\n",
      "Epoch: 46 itrerations: 7400 Training Loss: 4.31418657303\n",
      "Epoch: 46 itrerations: 7500 Training Loss: 4.31465387344\n",
      "Epoch: 46 itrerations: 7600 Training Loss: 4.31163024902\n",
      "Epoch: 46 itrerations: 7700 Training Loss: 4.3003692627\n",
      "Epoch: 46 itrerations: 7800 Training Loss: 4.29307126999\n",
      "Epoch: 46 itrerations: 7900 Training Loss: 4.299243927\n",
      "Epoch: 46 itrerations: 8000 Training Loss: 4.2971906662\n",
      "Epoch: 46 itrerations: 8100 Training Loss: 4.29398107529\n",
      "Epoch: 46 itrerations: 8200 Training Loss: 4.29856681824\n",
      "Epoch: 46 itrerations: 8300 Training Loss: 4.29403495789\n",
      "Epoch: 46 itrerations: 8400 Training Loss: 4.29546165466\n",
      "Epoch: 46 itrerations: 8500 Training Loss: 4.30176353455\n",
      "Epoch: 46 itrerations: 8600 Training Loss: 4.30110025406\n",
      "Epoch: 46 itrerations: 8700 Training Loss: 4.29285478592\n",
      "Epoch: 46 itrerations: 8800 Training Loss: 4.28866147995\n",
      "Epoch: 46 itrerations: 8900 Training Loss: 4.28312349319\n",
      "Epoch: 46 itrerations: 9000 Training Loss: 4.2908654213\n",
      "Epoch: 46 itrerations: 9100 Training Loss: 4.2868270874\n",
      "Epoch: 46 itrerations: 9200 Training Loss: 4.29157924652\n",
      "Epoch: 46 itrerations: 9300 Training Loss: 4.28725385666\n",
      "Epoch: 46 itrerations: 9400 Training Loss: 4.28162145615\n",
      "Epoch: 46 itrerations: 9500 Training Loss: 4.28606796265\n",
      "Epoch: 46 itrerations: 9600 Training Loss: 4.30008935928\n",
      "Epoch: 46 itrerations: 9700 Training Loss: 4.30433320999\n",
      "Epoch: 46 itrerations: 9800 Training Loss: 4.30182886124\n",
      "Epoch: 46 itrerations: 9900 Training Loss: 4.30098628998\n",
      "Epoch: 46 itrerations: 10000 Training Loss: 4.32148122787\n",
      "Epoch: 46 itrerations: 10100 Training Loss: 4.32019472122\n",
      "Epoch: 46 itrerations: 10200 Training Loss: 4.32132434845\n",
      "Epoch: 46 itrerations: 10300 Training Loss: 4.32554292679\n",
      "Epoch: 46 itrerations: 10400 Training Loss: 4.32680654526\n",
      "Epoch: 46 itrerations: 10500 Training Loss: 4.32001018524\n",
      "Epoch: 46 itrerations: 10600 Training Loss: 4.32189416885\n",
      "Epoch: 46 itrerations: 10700 Training Loss: 4.32880020142\n",
      "Epoch: 47 itrerations: 0 Training Loss: 4.34607601166\n",
      "Epoch: 47 itrerations: 100 Training Loss: 4.1703414917\n",
      "Epoch: 47 itrerations: 200 Training Loss: 3.78690648079\n",
      "Epoch: 47 itrerations: 300 Training Loss: 3.87082362175\n",
      "Epoch: 47 itrerations: 400 Training Loss: 3.97244501114\n",
      "Epoch: 47 itrerations: 500 Training Loss: 3.76156806946\n",
      "Epoch: 47 itrerations: 600 Training Loss: 3.97144007683\n",
      "Epoch: 47 itrerations: 700 Training Loss: 4.02185201645\n",
      "Epoch: 47 itrerations: 800 Training Loss: 4.19582080841\n",
      "Epoch: 47 itrerations: 900 Training Loss: 4.27303743362\n",
      "Epoch: 47 itrerations: 1000 Training Loss: 4.28631544113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 itrerations: 1100 Training Loss: 4.27560043335\n",
      "Epoch: 47 itrerations: 1200 Training Loss: 4.37273550034\n",
      "Epoch: 47 itrerations: 1300 Training Loss: 4.3814330101\n",
      "Epoch: 47 itrerations: 1400 Training Loss: 4.45303916931\n",
      "Epoch: 47 itrerations: 1500 Training Loss: 4.44438886642\n",
      "Epoch: 47 itrerations: 1600 Training Loss: 4.47270774841\n",
      "Epoch: 47 itrerations: 1700 Training Loss: 4.49856901169\n",
      "Epoch: 47 itrerations: 1800 Training Loss: 4.50646781921\n",
      "Epoch: 47 itrerations: 1900 Training Loss: 4.49980974197\n",
      "Epoch: 47 itrerations: 2000 Training Loss: 4.49943399429\n",
      "Epoch: 47 itrerations: 2100 Training Loss: 4.48630094528\n",
      "Epoch: 47 itrerations: 2200 Training Loss: 4.45872163773\n",
      "Epoch: 47 itrerations: 2300 Training Loss: 4.4588136673\n",
      "Epoch: 47 itrerations: 2400 Training Loss: 4.42151117325\n",
      "Epoch: 47 itrerations: 2500 Training Loss: 4.37255907059\n",
      "Epoch: 47 itrerations: 2600 Training Loss: 4.36536884308\n",
      "Epoch: 47 itrerations: 2700 Training Loss: 4.38435935974\n",
      "Epoch: 47 itrerations: 2800 Training Loss: 4.3394536972\n",
      "Epoch: 47 itrerations: 2900 Training Loss: 4.3016037941\n",
      "Epoch: 47 itrerations: 3000 Training Loss: 4.34908342361\n",
      "Epoch: 47 itrerations: 3100 Training Loss: 4.32584667206\n",
      "Epoch: 47 itrerations: 3200 Training Loss: 4.34976577759\n",
      "Epoch: 47 itrerations: 3300 Training Loss: 4.34100818634\n",
      "Epoch: 47 itrerations: 3400 Training Loss: 4.33727455139\n",
      "Epoch: 47 itrerations: 3500 Training Loss: 4.37253761292\n",
      "Epoch: 47 itrerations: 3600 Training Loss: 4.39115810394\n",
      "Epoch: 47 itrerations: 3700 Training Loss: 4.37267589569\n",
      "Epoch: 47 itrerations: 3800 Training Loss: 4.38988780975\n",
      "Epoch: 47 itrerations: 3900 Training Loss: 4.35933637619\n",
      "Epoch: 47 itrerations: 4000 Training Loss: 4.35475826263\n",
      "Epoch: 47 itrerations: 4100 Training Loss: 4.36108732224\n",
      "Epoch: 47 itrerations: 4200 Training Loss: 4.35192346573\n",
      "Epoch: 47 itrerations: 4300 Training Loss: 4.34703397751\n",
      "Epoch: 47 itrerations: 4400 Training Loss: 4.35080194473\n",
      "Epoch: 47 itrerations: 4500 Training Loss: 4.35441684723\n",
      "Epoch: 47 itrerations: 4600 Training Loss: 4.35904312134\n",
      "Epoch: 47 itrerations: 4700 Training Loss: 4.35412502289\n",
      "Epoch: 47 itrerations: 4800 Training Loss: 4.34393167496\n",
      "Epoch: 47 itrerations: 4900 Training Loss: 4.32983064651\n",
      "Epoch: 47 itrerations: 5000 Training Loss: 4.30026960373\n",
      "Epoch: 47 itrerations: 5100 Training Loss: 4.28379678726\n",
      "Epoch: 47 itrerations: 5200 Training Loss: 4.29384803772\n",
      "Epoch: 47 itrerations: 5300 Training Loss: 4.29684925079\n",
      "Epoch: 47 itrerations: 5400 Training Loss: 4.32008886337\n",
      "Epoch: 47 itrerations: 5500 Training Loss: 4.3151807785\n",
      "Epoch: 47 itrerations: 5600 Training Loss: 4.31607866287\n",
      "Epoch: 47 itrerations: 5700 Training Loss: 4.32164812088\n",
      "Epoch: 47 itrerations: 5800 Training Loss: 4.3219461441\n",
      "Epoch: 47 itrerations: 5900 Training Loss: 4.32537460327\n",
      "Epoch: 47 itrerations: 6000 Training Loss: 4.32793664932\n",
      "Epoch: 47 itrerations: 6100 Training Loss: 4.3160867691\n",
      "Epoch: 47 itrerations: 6200 Training Loss: 4.32893371582\n",
      "Epoch: 47 itrerations: 6300 Training Loss: 4.32642364502\n",
      "Epoch: 47 itrerations: 6400 Training Loss: 4.32327651978\n",
      "Epoch: 47 itrerations: 6500 Training Loss: 4.31612157822\n",
      "Epoch: 47 itrerations: 6600 Training Loss: 4.30614185333\n",
      "Epoch: 47 itrerations: 6700 Training Loss: 4.31139755249\n",
      "Epoch: 47 itrerations: 6800 Training Loss: 4.3218755722\n",
      "Epoch: 47 itrerations: 6900 Training Loss: 4.32916212082\n",
      "Epoch: 47 itrerations: 7000 Training Loss: 4.34580469131\n",
      "Epoch: 47 itrerations: 7100 Training Loss: 4.35701704025\n",
      "Epoch: 47 itrerations: 7200 Training Loss: 4.33942508698\n",
      "Epoch: 47 itrerations: 7300 Training Loss: 4.33576202393\n",
      "Epoch: 47 itrerations: 7400 Training Loss: 4.32825803757\n",
      "Epoch: 47 itrerations: 7500 Training Loss: 4.32067489624\n",
      "Epoch: 47 itrerations: 7600 Training Loss: 4.31339931488\n",
      "Epoch: 47 itrerations: 7700 Training Loss: 4.30470371246\n",
      "Epoch: 47 itrerations: 7800 Training Loss: 4.30677890778\n",
      "Epoch: 47 itrerations: 7900 Training Loss: 4.31834697723\n",
      "Epoch: 47 itrerations: 8000 Training Loss: 4.31617259979\n",
      "Epoch: 47 itrerations: 8100 Training Loss: 4.31096553802\n",
      "Epoch: 47 itrerations: 8200 Training Loss: 4.32102108002\n",
      "Epoch: 47 itrerations: 8300 Training Loss: 4.32904624939\n",
      "Epoch: 47 itrerations: 8400 Training Loss: 4.33680105209\n",
      "Epoch: 47 itrerations: 8500 Training Loss: 4.33342456818\n",
      "Epoch: 47 itrerations: 8600 Training Loss: 4.32999706268\n",
      "Epoch: 47 itrerations: 8700 Training Loss: 4.33401966095\n",
      "Epoch: 47 itrerations: 8800 Training Loss: 4.33001661301\n",
      "Epoch: 47 itrerations: 8900 Training Loss: 4.32851028442\n",
      "Epoch: 47 itrerations: 9000 Training Loss: 4.33838653564\n",
      "Epoch: 47 itrerations: 9100 Training Loss: 4.340675354\n",
      "Epoch: 47 itrerations: 9200 Training Loss: 4.34256505966\n",
      "Epoch: 47 itrerations: 9300 Training Loss: 4.33582162857\n",
      "Epoch: 47 itrerations: 9400 Training Loss: 4.32791996002\n",
      "Epoch: 47 itrerations: 9500 Training Loss: 4.32741785049\n",
      "Epoch: 47 itrerations: 9600 Training Loss: 4.33920145035\n",
      "Epoch: 47 itrerations: 9700 Training Loss: 4.34215497971\n",
      "Epoch: 47 itrerations: 9800 Training Loss: 4.33782815933\n",
      "Epoch: 47 itrerations: 9900 Training Loss: 4.33934593201\n",
      "Epoch: 47 itrerations: 10000 Training Loss: 4.34727525711\n",
      "Epoch: 47 itrerations: 10100 Training Loss: 4.34543323517\n",
      "Epoch: 47 itrerations: 10200 Training Loss: 4.34183311462\n",
      "Epoch: 47 itrerations: 10300 Training Loss: 4.34663581848\n",
      "Epoch: 47 itrerations: 10400 Training Loss: 4.3425989151\n",
      "Epoch: 47 itrerations: 10500 Training Loss: 4.33491659164\n",
      "Epoch: 47 itrerations: 10600 Training Loss: 4.33062553406\n",
      "Epoch: 47 itrerations: 10700 Training Loss: 4.3288731575\n",
      "Epoch: 48 itrerations: 0 Training Loss: 1.77519488335\n",
      "Epoch: 48 itrerations: 100 Training Loss: 4.24495506287\n",
      "Epoch: 48 itrerations: 200 Training Loss: 3.95777463913\n",
      "Epoch: 48 itrerations: 300 Training Loss: 4.01176023483\n",
      "Epoch: 48 itrerations: 400 Training Loss: 4.11542367935\n",
      "Epoch: 48 itrerations: 500 Training Loss: 3.81315016747\n",
      "Epoch: 48 itrerations: 600 Training Loss: 4.00525999069\n",
      "Epoch: 48 itrerations: 700 Training Loss: 4.11728239059\n",
      "Epoch: 48 itrerations: 800 Training Loss: 4.23981142044\n",
      "Epoch: 48 itrerations: 900 Training Loss: 4.20323896408\n",
      "Epoch: 48 itrerations: 1000 Training Loss: 4.31006383896\n",
      "Epoch: 48 itrerations: 1100 Training Loss: 4.24972486496\n",
      "Epoch: 48 itrerations: 1200 Training Loss: 4.45292758942\n",
      "Epoch: 48 itrerations: 1300 Training Loss: 4.40948390961\n",
      "Epoch: 48 itrerations: 1400 Training Loss: 4.48648166656\n",
      "Epoch: 48 itrerations: 1500 Training Loss: 4.45208501816\n",
      "Epoch: 48 itrerations: 1600 Training Loss: 4.45418024063\n",
      "Epoch: 48 itrerations: 1700 Training Loss: 4.46255350113\n",
      "Epoch: 48 itrerations: 1800 Training Loss: 4.44893407822\n",
      "Epoch: 48 itrerations: 1900 Training Loss: 4.50129938126\n",
      "Epoch: 48 itrerations: 2000 Training Loss: 4.50849246979\n",
      "Epoch: 48 itrerations: 2100 Training Loss: 4.4667468071\n",
      "Epoch: 48 itrerations: 2200 Training Loss: 4.42988204956\n",
      "Epoch: 48 itrerations: 2300 Training Loss: 4.44638538361\n",
      "Epoch: 48 itrerations: 2400 Training Loss: 4.42879343033\n",
      "Epoch: 48 itrerations: 2500 Training Loss: 4.39388847351\n",
      "Epoch: 48 itrerations: 2600 Training Loss: 4.37085437775\n",
      "Epoch: 48 itrerations: 2700 Training Loss: 4.39686965942\n",
      "Epoch: 48 itrerations: 2800 Training Loss: 4.34539365768\n",
      "Epoch: 48 itrerations: 2900 Training Loss: 4.32447433472\n",
      "Epoch: 48 itrerations: 3000 Training Loss: 4.36443424225\n",
      "Epoch: 48 itrerations: 3100 Training Loss: 4.35100889206\n",
      "Epoch: 48 itrerations: 3200 Training Loss: 4.36320638657\n",
      "Epoch: 48 itrerations: 3300 Training Loss: 4.32854652405\n",
      "Epoch: 48 itrerations: 3400 Training Loss: 4.32990503311\n",
      "Epoch: 48 itrerations: 3500 Training Loss: 4.36429929733\n",
      "Epoch: 48 itrerations: 3600 Training Loss: 4.38222026825\n",
      "Epoch: 48 itrerations: 3700 Training Loss: 4.38227462769\n",
      "Epoch: 48 itrerations: 3800 Training Loss: 4.39802408218\n",
      "Epoch: 48 itrerations: 3900 Training Loss: 4.39107275009\n",
      "Epoch: 48 itrerations: 4000 Training Loss: 4.39981174469\n",
      "Epoch: 48 itrerations: 4100 Training Loss: 4.39691257477\n",
      "Epoch: 48 itrerations: 4200 Training Loss: 4.37550592422\n",
      "Epoch: 48 itrerations: 4300 Training Loss: 4.36656999588\n",
      "Epoch: 48 itrerations: 4400 Training Loss: 4.38500595093\n",
      "Epoch: 48 itrerations: 4500 Training Loss: 4.36410474777\n",
      "Epoch: 48 itrerations: 4600 Training Loss: 4.36687231064\n",
      "Epoch: 48 itrerations: 4700 Training Loss: 4.36766862869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 itrerations: 4800 Training Loss: 4.36036682129\n",
      "Epoch: 48 itrerations: 4900 Training Loss: 4.34064054489\n",
      "Epoch: 48 itrerations: 5000 Training Loss: 4.31964111328\n",
      "Epoch: 48 itrerations: 5100 Training Loss: 4.30995798111\n",
      "Epoch: 48 itrerations: 5200 Training Loss: 4.30075979233\n",
      "Epoch: 48 itrerations: 5300 Training Loss: 4.30391836166\n",
      "Epoch: 48 itrerations: 5400 Training Loss: 4.3105134964\n",
      "Epoch: 48 itrerations: 5500 Training Loss: 4.30820989609\n",
      "Epoch: 48 itrerations: 5600 Training Loss: 4.3239274025\n",
      "Epoch: 48 itrerations: 5700 Training Loss: 4.32197713852\n",
      "Epoch: 48 itrerations: 5800 Training Loss: 4.33116006851\n",
      "Epoch: 48 itrerations: 5900 Training Loss: 4.33965158463\n",
      "Epoch: 48 itrerations: 6000 Training Loss: 4.34210634232\n",
      "Epoch: 48 itrerations: 6100 Training Loss: 4.32422971725\n",
      "Epoch: 48 itrerations: 6200 Training Loss: 4.33617687225\n",
      "Epoch: 48 itrerations: 6300 Training Loss: 4.33964824677\n",
      "Epoch: 48 itrerations: 6400 Training Loss: 4.3369641304\n",
      "Epoch: 48 itrerations: 6500 Training Loss: 4.33229732513\n",
      "Epoch: 48 itrerations: 6600 Training Loss: 4.33163166046\n",
      "Epoch: 48 itrerations: 6700 Training Loss: 4.32634305954\n",
      "Epoch: 48 itrerations: 6800 Training Loss: 4.33228969574\n",
      "Epoch: 48 itrerations: 6900 Training Loss: 4.33517980576\n",
      "Epoch: 48 itrerations: 7000 Training Loss: 4.36132669449\n",
      "Epoch: 48 itrerations: 7100 Training Loss: 4.36257266998\n",
      "Epoch: 48 itrerations: 7200 Training Loss: 4.35285615921\n",
      "Epoch: 48 itrerations: 7300 Training Loss: 4.34769296646\n",
      "Epoch: 48 itrerations: 7400 Training Loss: 4.34101295471\n",
      "Epoch: 48 itrerations: 7500 Training Loss: 4.34297800064\n",
      "Epoch: 48 itrerations: 7600 Training Loss: 4.33884477615\n",
      "Epoch: 48 itrerations: 7700 Training Loss: 4.3293132782\n",
      "Epoch: 48 itrerations: 7800 Training Loss: 4.33328819275\n",
      "Epoch: 48 itrerations: 7900 Training Loss: 4.3417634964\n",
      "Epoch: 48 itrerations: 8000 Training Loss: 4.33428668976\n",
      "Epoch: 48 itrerations: 8100 Training Loss: 4.33213329315\n",
      "Epoch: 48 itrerations: 8200 Training Loss: 4.34456729889\n",
      "Epoch: 48 itrerations: 8300 Training Loss: 4.34866905212\n",
      "Epoch: 48 itrerations: 8400 Training Loss: 4.34339189529\n",
      "Epoch: 48 itrerations: 8500 Training Loss: 4.34195184708\n",
      "Epoch: 48 itrerations: 8600 Training Loss: 4.33441209793\n",
      "Epoch: 48 itrerations: 8700 Training Loss: 4.32350254059\n",
      "Epoch: 48 itrerations: 8800 Training Loss: 4.32194423676\n",
      "Epoch: 48 itrerations: 8900 Training Loss: 4.32124376297\n",
      "Epoch: 48 itrerations: 9000 Training Loss: 4.33050441742\n",
      "Epoch: 48 itrerations: 9100 Training Loss: 4.32964229584\n",
      "Epoch: 48 itrerations: 9200 Training Loss: 4.33564901352\n",
      "Epoch: 48 itrerations: 9300 Training Loss: 4.3294467926\n",
      "Epoch: 48 itrerations: 9400 Training Loss: 4.3185338974\n",
      "Epoch: 48 itrerations: 9500 Training Loss: 4.32205104828\n",
      "Epoch: 48 itrerations: 9600 Training Loss: 4.3385014534\n",
      "Epoch: 48 itrerations: 9700 Training Loss: 4.33971500397\n",
      "Epoch: 48 itrerations: 9800 Training Loss: 4.34095859528\n",
      "Epoch: 48 itrerations: 9900 Training Loss: 4.34290313721\n",
      "Epoch: 48 itrerations: 10000 Training Loss: 4.35332918167\n",
      "Epoch: 48 itrerations: 10100 Training Loss: 4.34596681595\n",
      "Epoch: 48 itrerations: 10200 Training Loss: 4.34559488297\n",
      "Epoch: 48 itrerations: 10300 Training Loss: 4.34071159363\n",
      "Epoch: 48 itrerations: 10400 Training Loss: 4.34060955048\n",
      "Epoch: 48 itrerations: 10500 Training Loss: 4.33054161072\n",
      "Epoch: 48 itrerations: 10600 Training Loss: 4.33585071564\n",
      "Epoch: 48 itrerations: 10700 Training Loss: 4.33914422989\n",
      "Epoch: 49 itrerations: 0 Training Loss: 1.99496030807\n",
      "Epoch: 49 itrerations: 100 Training Loss: 4.09913253784\n",
      "Epoch: 49 itrerations: 200 Training Loss: 3.96204972267\n",
      "Epoch: 49 itrerations: 300 Training Loss: 3.98023343086\n",
      "Epoch: 49 itrerations: 400 Training Loss: 4.00131559372\n",
      "Epoch: 49 itrerations: 500 Training Loss: 3.70548057556\n",
      "Epoch: 49 itrerations: 600 Training Loss: 3.95355081558\n",
      "Epoch: 49 itrerations: 700 Training Loss: 4.01199436188\n",
      "Epoch: 49 itrerations: 800 Training Loss: 4.30326223373\n",
      "Epoch: 49 itrerations: 900 Training Loss: 4.33443260193\n",
      "Epoch: 49 itrerations: 1000 Training Loss: 4.34016561508\n",
      "Epoch: 49 itrerations: 1100 Training Loss: 4.25386333466\n",
      "Epoch: 49 itrerations: 1200 Training Loss: 4.38302087784\n",
      "Epoch: 49 itrerations: 1300 Training Loss: 4.39697170258\n",
      "Epoch: 49 itrerations: 1400 Training Loss: 4.46134185791\n",
      "Epoch: 49 itrerations: 1500 Training Loss: 4.45155620575\n",
      "Epoch: 49 itrerations: 1600 Training Loss: 4.48575162888\n",
      "Epoch: 49 itrerations: 1700 Training Loss: 4.5279955864\n",
      "Epoch: 49 itrerations: 1800 Training Loss: 4.53502607346\n",
      "Epoch: 49 itrerations: 1900 Training Loss: 4.56380224228\n",
      "Epoch: 49 itrerations: 2000 Training Loss: 4.56529808044\n",
      "Epoch: 49 itrerations: 2100 Training Loss: 4.53582286835\n",
      "Epoch: 49 itrerations: 2200 Training Loss: 4.49070739746\n",
      "Epoch: 49 itrerations: 2300 Training Loss: 4.4880604744\n",
      "Epoch: 49 itrerations: 2400 Training Loss: 4.45785999298\n",
      "Epoch: 49 itrerations: 2500 Training Loss: 4.43307352066\n",
      "Epoch: 49 itrerations: 2600 Training Loss: 4.40573692322\n",
      "Epoch: 49 itrerations: 2700 Training Loss: 4.43484163284\n",
      "Epoch: 49 itrerations: 2800 Training Loss: 4.38721990585\n",
      "Epoch: 49 itrerations: 2900 Training Loss: 4.35257434845\n",
      "Epoch: 49 itrerations: 3000 Training Loss: 4.40311717987\n",
      "Epoch: 49 itrerations: 3100 Training Loss: 4.38098287582\n",
      "Epoch: 49 itrerations: 3200 Training Loss: 4.37731456757\n",
      "Epoch: 49 itrerations: 3300 Training Loss: 4.3534526825\n",
      "Epoch: 49 itrerations: 3400 Training Loss: 4.35924243927\n",
      "Epoch: 49 itrerations: 3500 Training Loss: 4.37341451645\n",
      "Epoch: 49 itrerations: 3600 Training Loss: 4.40757322311\n",
      "Epoch: 49 itrerations: 3700 Training Loss: 4.39248085022\n",
      "Epoch: 49 itrerations: 3800 Training Loss: 4.41585874557\n",
      "Epoch: 49 itrerations: 3900 Training Loss: 4.40852212906\n",
      "Epoch: 49 itrerations: 4000 Training Loss: 4.41728925705\n",
      "Epoch: 49 itrerations: 4100 Training Loss: 4.41062402725\n",
      "Epoch: 49 itrerations: 4200 Training Loss: 4.39224100113\n",
      "Epoch: 49 itrerations: 4300 Training Loss: 4.38789319992\n",
      "Epoch: 49 itrerations: 4400 Training Loss: 4.40838956833\n",
      "Epoch: 49 itrerations: 4500 Training Loss: 4.4085855484\n",
      "Epoch: 49 itrerations: 4600 Training Loss: 4.4086856842\n",
      "Epoch: 49 itrerations: 4700 Training Loss: 4.4080324173\n",
      "Epoch: 49 itrerations: 4800 Training Loss: 4.39698457718\n",
      "Epoch: 49 itrerations: 4900 Training Loss: 4.39341402054\n",
      "Epoch: 49 itrerations: 5000 Training Loss: 4.36382818222\n",
      "Epoch: 49 itrerations: 5100 Training Loss: 4.35620641708\n",
      "Epoch: 49 itrerations: 5200 Training Loss: 4.35769319534\n",
      "Epoch: 49 itrerations: 5300 Training Loss: 4.34950017929\n",
      "Epoch: 49 itrerations: 5400 Training Loss: 4.37276172638\n",
      "Epoch: 49 itrerations: 5500 Training Loss: 4.35665798187\n",
      "Epoch: 49 itrerations: 5600 Training Loss: 4.35228013992\n",
      "Epoch: 49 itrerations: 5700 Training Loss: 4.35397386551\n",
      "Epoch: 49 itrerations: 5800 Training Loss: 4.35945224762\n",
      "Epoch: 49 itrerations: 5900 Training Loss: 4.37020206451\n",
      "Epoch: 49 itrerations: 6000 Training Loss: 4.36022472382\n",
      "Epoch: 49 itrerations: 6100 Training Loss: 4.34653568268\n",
      "Epoch: 49 itrerations: 6200 Training Loss: 4.35187673569\n",
      "Epoch: 49 itrerations: 6300 Training Loss: 4.34536743164\n",
      "Epoch: 49 itrerations: 6400 Training Loss: 4.34293889999\n",
      "Epoch: 49 itrerations: 6500 Training Loss: 4.33530759811\n",
      "Epoch: 49 itrerations: 6600 Training Loss: 4.32694101334\n",
      "Epoch: 49 itrerations: 6700 Training Loss: 4.33588027954\n",
      "Epoch: 49 itrerations: 6800 Training Loss: 4.33413362503\n",
      "Epoch: 49 itrerations: 6900 Training Loss: 4.34863138199\n",
      "Epoch: 49 itrerations: 7000 Training Loss: 4.37315893173\n",
      "Epoch: 49 itrerations: 7100 Training Loss: 4.38334226608\n",
      "Epoch: 49 itrerations: 7200 Training Loss: 4.37018585205\n",
      "Epoch: 49 itrerations: 7300 Training Loss: 4.36349153519\n",
      "Epoch: 49 itrerations: 7400 Training Loss: 4.35488414764\n",
      "Epoch: 49 itrerations: 7500 Training Loss: 4.35564994812\n",
      "Epoch: 49 itrerations: 7600 Training Loss: 4.34943056107\n",
      "Epoch: 49 itrerations: 7700 Training Loss: 4.34001398087\n",
      "Epoch: 49 itrerations: 7800 Training Loss: 4.34669017792\n",
      "Epoch: 49 itrerations: 7900 Training Loss: 4.35415554047\n",
      "Epoch: 49 itrerations: 8000 Training Loss: 4.35366725922\n",
      "Epoch: 49 itrerations: 8100 Training Loss: 4.34689950943\n",
      "Epoch: 49 itrerations: 8200 Training Loss: 4.35661649704\n",
      "Epoch: 49 itrerations: 8300 Training Loss: 4.3531255722\n",
      "Epoch: 49 itrerations: 8400 Training Loss: 4.35526657104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 itrerations: 8500 Training Loss: 4.36025428772\n",
      "Epoch: 49 itrerations: 8600 Training Loss: 4.35394668579\n",
      "Epoch: 49 itrerations: 8700 Training Loss: 4.35061264038\n",
      "Epoch: 49 itrerations: 8800 Training Loss: 4.35182857513\n",
      "Epoch: 49 itrerations: 8900 Training Loss: 4.35116291046\n",
      "Epoch: 49 itrerations: 9000 Training Loss: 4.35654020309\n",
      "Epoch: 49 itrerations: 9100 Training Loss: 4.3563914299\n",
      "Epoch: 49 itrerations: 9200 Training Loss: 4.35610151291\n",
      "Epoch: 49 itrerations: 9300 Training Loss: 4.3469991684\n",
      "Epoch: 49 itrerations: 9400 Training Loss: 4.33788490295\n",
      "Epoch: 49 itrerations: 9500 Training Loss: 4.3369550705\n",
      "Epoch: 49 itrerations: 9600 Training Loss: 4.349568367\n",
      "Epoch: 49 itrerations: 9700 Training Loss: 4.35305404663\n",
      "Epoch: 49 itrerations: 9800 Training Loss: 4.34988451004\n",
      "Epoch: 49 itrerations: 9900 Training Loss: 4.34784412384\n",
      "Epoch: 49 itrerations: 10000 Training Loss: 4.36517810822\n",
      "Epoch: 49 itrerations: 10100 Training Loss: 4.36897802353\n",
      "Epoch: 49 itrerations: 10200 Training Loss: 4.3638176918\n",
      "Epoch: 49 itrerations: 10300 Training Loss: 4.36634635925\n",
      "Epoch: 49 itrerations: 10400 Training Loss: 4.36625814438\n",
      "Epoch: 49 itrerations: 10500 Training Loss: 4.35826683044\n",
      "Epoch: 49 itrerations: 10600 Training Loss: 4.36232709885\n",
      "Epoch: 49 itrerations: 10700 Training Loss: 4.36420869827\n",
      "Epoch: 50 itrerations: 0 Training Loss: 3.6596364975\n",
      "Epoch: 50 itrerations: 100 Training Loss: 4.15583181381\n",
      "Epoch: 50 itrerations: 200 Training Loss: 3.74430346489\n",
      "Epoch: 50 itrerations: 300 Training Loss: 3.79599452019\n",
      "Epoch: 50 itrerations: 400 Training Loss: 3.91329669952\n",
      "Epoch: 50 itrerations: 500 Training Loss: 3.56761860847\n",
      "Epoch: 50 itrerations: 600 Training Loss: 3.86962604523\n",
      "Epoch: 50 itrerations: 700 Training Loss: 3.98179602623\n",
      "Epoch: 50 itrerations: 800 Training Loss: 4.20873308182\n",
      "Epoch: 50 itrerations: 900 Training Loss: 4.26540994644\n",
      "Epoch: 50 itrerations: 1000 Training Loss: 4.31278848648\n",
      "Epoch: 50 itrerations: 1100 Training Loss: 4.25660085678\n",
      "Epoch: 50 itrerations: 1200 Training Loss: 4.37775468826\n",
      "Epoch: 50 itrerations: 1300 Training Loss: 4.34588718414\n",
      "Epoch: 50 itrerations: 1400 Training Loss: 4.49890756607\n",
      "Epoch: 50 itrerations: 1500 Training Loss: 4.44553089142\n",
      "Epoch: 50 itrerations: 1600 Training Loss: 4.4803314209\n",
      "Epoch: 50 itrerations: 1700 Training Loss: 4.46713829041\n",
      "Epoch: 50 itrerations: 1800 Training Loss: 4.46178722382\n",
      "Epoch: 50 itrerations: 1900 Training Loss: 4.51196527481\n",
      "Epoch: 50 itrerations: 2000 Training Loss: 4.50541734695\n",
      "Epoch: 50 itrerations: 2100 Training Loss: 4.46969556808\n",
      "Epoch: 50 itrerations: 2200 Training Loss: 4.45124387741\n",
      "Epoch: 50 itrerations: 2300 Training Loss: 4.43802404404\n",
      "Epoch: 50 itrerations: 2400 Training Loss: 4.40479469299\n",
      "Epoch: 50 itrerations: 2500 Training Loss: 4.350938797\n",
      "Epoch: 50 itrerations: 2600 Training Loss: 4.35288333893\n",
      "Epoch: 50 itrerations: 2700 Training Loss: 4.37637662888\n",
      "Epoch: 50 itrerations: 2800 Training Loss: 4.35282516479\n",
      "Epoch: 50 itrerations: 2900 Training Loss: 4.3111577034\n",
      "Epoch: 50 itrerations: 3000 Training Loss: 4.35843706131\n",
      "Epoch: 50 itrerations: 3100 Training Loss: 4.35523080826\n",
      "Epoch: 50 itrerations: 3200 Training Loss: 4.36174869537\n",
      "Epoch: 50 itrerations: 3300 Training Loss: 4.32687330246\n",
      "Epoch: 50 itrerations: 3400 Training Loss: 4.33514499664\n",
      "Epoch: 50 itrerations: 3500 Training Loss: 4.37134027481\n",
      "Epoch: 50 itrerations: 3600 Training Loss: 4.38387823105\n",
      "Epoch: 50 itrerations: 3700 Training Loss: 4.38646268845\n",
      "Epoch: 50 itrerations: 3800 Training Loss: 4.39804315567\n",
      "Epoch: 50 itrerations: 3900 Training Loss: 4.39370536804\n",
      "Epoch: 50 itrerations: 4000 Training Loss: 4.39264249802\n",
      "Epoch: 50 itrerations: 4100 Training Loss: 4.39594888687\n",
      "Epoch: 50 itrerations: 4200 Training Loss: 4.37672948837\n",
      "Epoch: 50 itrerations: 4300 Training Loss: 4.36900138855\n",
      "Epoch: 50 itrerations: 4400 Training Loss: 4.39264965057\n",
      "Epoch: 50 itrerations: 4500 Training Loss: 4.39470911026\n",
      "Epoch: 50 itrerations: 4600 Training Loss: 4.38655138016\n",
      "Epoch: 50 itrerations: 4700 Training Loss: 4.38792228699\n",
      "Epoch: 50 itrerations: 4800 Training Loss: 4.38769292831\n",
      "Epoch: 50 itrerations: 4900 Training Loss: 4.37469005585\n",
      "Epoch: 50 itrerations: 5000 Training Loss: 4.35401964188\n",
      "Epoch: 50 itrerations: 5100 Training Loss: 4.35175848007\n",
      "Epoch: 50 itrerations: 5200 Training Loss: 4.3436665535\n",
      "Epoch: 50 itrerations: 5300 Training Loss: 4.34378242493\n",
      "Epoch: 50 itrerations: 5400 Training Loss: 4.3630566597\n",
      "Epoch: 50 itrerations: 5500 Training Loss: 4.34886312485\n",
      "Epoch: 50 itrerations: 5600 Training Loss: 4.34320831299\n",
      "Epoch: 50 itrerations: 5700 Training Loss: 4.32907676697\n",
      "Epoch: 50 itrerations: 5800 Training Loss: 4.33816766739\n",
      "Epoch: 50 itrerations: 5900 Training Loss: 4.3487496376\n",
      "Epoch: 50 itrerations: 6000 Training Loss: 4.34577655792\n",
      "Epoch: 50 itrerations: 6100 Training Loss: 4.32912778854\n",
      "Epoch: 50 itrerations: 6200 Training Loss: 4.34028482437\n",
      "Epoch: 50 itrerations: 6300 Training Loss: 4.33561658859\n",
      "Epoch: 50 itrerations: 6400 Training Loss: 4.33667850494\n",
      "Epoch: 50 itrerations: 6500 Training Loss: 4.33451509476\n",
      "Epoch: 50 itrerations: 6600 Training Loss: 4.3347864151\n",
      "Epoch: 50 itrerations: 6700 Training Loss: 4.34060764313\n",
      "Epoch: 50 itrerations: 6800 Training Loss: 4.34531211853\n",
      "Epoch: 50 itrerations: 6900 Training Loss: 4.35746097565\n",
      "Epoch: 50 itrerations: 7000 Training Loss: 4.37688779831\n",
      "Epoch: 50 itrerations: 7100 Training Loss: 4.38546180725\n",
      "Epoch: 50 itrerations: 7200 Training Loss: 4.37708377838\n",
      "Epoch: 50 itrerations: 7300 Training Loss: 4.37138748169\n",
      "Epoch: 50 itrerations: 7400 Training Loss: 4.36811733246\n",
      "Epoch: 50 itrerations: 7500 Training Loss: 4.36404371262\n",
      "Epoch: 50 itrerations: 7600 Training Loss: 4.35988950729\n",
      "Epoch: 50 itrerations: 7700 Training Loss: 4.35836458206\n",
      "Epoch: 50 itrerations: 7800 Training Loss: 4.36540699005\n",
      "Epoch: 50 itrerations: 7900 Training Loss: 4.37386322021\n",
      "Epoch: 50 itrerations: 8000 Training Loss: 4.37232589722\n",
      "Epoch: 50 itrerations: 8100 Training Loss: 4.36780691147\n",
      "Epoch: 50 itrerations: 8200 Training Loss: 4.38174295425\n",
      "Epoch: 50 itrerations: 8300 Training Loss: 4.38587236404\n",
      "Epoch: 50 itrerations: 8400 Training Loss: 4.38855600357\n",
      "Epoch: 50 itrerations: 8500 Training Loss: 4.39533424377\n",
      "Epoch: 50 itrerations: 8600 Training Loss: 4.38735055923\n",
      "Epoch: 50 itrerations: 8700 Training Loss: 4.38622379303\n",
      "Epoch: 50 itrerations: 8800 Training Loss: 4.38362455368\n",
      "Epoch: 50 itrerations: 8900 Training Loss: 4.3786687851\n",
      "Epoch: 50 itrerations: 9000 Training Loss: 4.38632822037\n",
      "Epoch: 50 itrerations: 9100 Training Loss: 4.38862037659\n",
      "Epoch: 50 itrerations: 9200 Training Loss: 4.39209985733\n",
      "Epoch: 50 itrerations: 9300 Training Loss: 4.38898515701\n",
      "Epoch: 50 itrerations: 9400 Training Loss: 4.38049411774\n",
      "Epoch: 50 itrerations: 9500 Training Loss: 4.38599634171\n",
      "Epoch: 50 itrerations: 9600 Training Loss: 4.39647340775\n",
      "Epoch: 50 itrerations: 9700 Training Loss: 4.39861965179\n",
      "Epoch: 50 itrerations: 9800 Training Loss: 4.39532899857\n",
      "Epoch: 50 itrerations: 9900 Training Loss: 4.39642286301\n",
      "Epoch: 50 itrerations: 10000 Training Loss: 4.41137838364\n",
      "Epoch: 50 itrerations: 10100 Training Loss: 4.41139411926\n",
      "Epoch: 50 itrerations: 10200 Training Loss: 4.41159677505\n",
      "Epoch: 50 itrerations: 10300 Training Loss: 4.41058111191\n",
      "Epoch: 50 itrerations: 10400 Training Loss: 4.41465377808\n",
      "Epoch: 50 itrerations: 10500 Training Loss: 4.40872001648\n",
      "Epoch: 50 itrerations: 10600 Training Loss: 4.40714168549\n",
      "Epoch: 50 itrerations: 10700 Training Loss: 4.4070687294\n",
      "Epoch: 51 itrerations: 0 Training Loss: 3.20244908333\n",
      "Epoch: 51 itrerations: 100 Training Loss: 5.22918081284\n",
      "Epoch: 51 itrerations: 200 Training Loss: 4.60341358185\n",
      "Epoch: 51 itrerations: 300 Training Loss: 4.27307271957\n",
      "Epoch: 51 itrerations: 400 Training Loss: 4.31840419769\n",
      "Epoch: 51 itrerations: 500 Training Loss: 4.00673389435\n",
      "Epoch: 51 itrerations: 600 Training Loss: 4.07792043686\n",
      "Epoch: 51 itrerations: 700 Training Loss: 4.12454843521\n",
      "Epoch: 51 itrerations: 800 Training Loss: 4.34704351425\n",
      "Epoch: 51 itrerations: 900 Training Loss: 4.35973930359\n",
      "Epoch: 51 itrerations: 1000 Training Loss: 4.35138607025\n",
      "Epoch: 51 itrerations: 1100 Training Loss: 4.37316846848\n",
      "Epoch: 51 itrerations: 1200 Training Loss: 4.43152618408\n",
      "Epoch: 51 itrerations: 1300 Training Loss: 4.41751909256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 itrerations: 1400 Training Loss: 4.49801492691\n",
      "Epoch: 51 itrerations: 1500 Training Loss: 4.45982170105\n",
      "Epoch: 51 itrerations: 1600 Training Loss: 4.50043535233\n",
      "Epoch: 51 itrerations: 1700 Training Loss: 4.48444747925\n",
      "Epoch: 51 itrerations: 1800 Training Loss: 4.48810720444\n",
      "Epoch: 51 itrerations: 1900 Training Loss: 4.52190732956\n",
      "Epoch: 51 itrerations: 2000 Training Loss: 4.52839851379\n",
      "Epoch: 51 itrerations: 2100 Training Loss: 4.46641111374\n",
      "Epoch: 51 itrerations: 2200 Training Loss: 4.44138050079\n",
      "Epoch: 51 itrerations: 2300 Training Loss: 4.42792701721\n",
      "Epoch: 51 itrerations: 2400 Training Loss: 4.38891983032\n",
      "Epoch: 51 itrerations: 2500 Training Loss: 4.39550828934\n",
      "Epoch: 51 itrerations: 2600 Training Loss: 4.37844657898\n",
      "Epoch: 51 itrerations: 2700 Training Loss: 4.41119289398\n",
      "Epoch: 51 itrerations: 2800 Training Loss: 4.39333248138\n",
      "Epoch: 51 itrerations: 2900 Training Loss: 4.35811328888\n",
      "Epoch: 51 itrerations: 3000 Training Loss: 4.40549564362\n",
      "Epoch: 51 itrerations: 3100 Training Loss: 4.3875670433\n",
      "Epoch: 51 itrerations: 3200 Training Loss: 4.37471437454\n",
      "Epoch: 51 itrerations: 3300 Training Loss: 4.35399532318\n",
      "Epoch: 51 itrerations: 3400 Training Loss: 4.3581776619\n",
      "Epoch: 51 itrerations: 3500 Training Loss: 4.37303447723\n",
      "Epoch: 51 itrerations: 3600 Training Loss: 4.37861251831\n",
      "Epoch: 51 itrerations: 3700 Training Loss: 4.36590194702\n",
      "Epoch: 51 itrerations: 3800 Training Loss: 4.38544130325\n",
      "Epoch: 51 itrerations: 3900 Training Loss: 4.374563694\n",
      "Epoch: 51 itrerations: 4000 Training Loss: 4.37211465836\n",
      "Epoch: 51 itrerations: 4100 Training Loss: 4.37069797516\n",
      "Epoch: 51 itrerations: 4200 Training Loss: 4.34786128998\n",
      "Epoch: 51 itrerations: 4300 Training Loss: 4.3395781517\n",
      "Epoch: 51 itrerations: 4400 Training Loss: 4.34017944336\n",
      "Epoch: 51 itrerations: 4500 Training Loss: 4.33306694031\n",
      "Epoch: 51 itrerations: 4600 Training Loss: 4.33425712585\n",
      "Epoch: 51 itrerations: 4700 Training Loss: 4.33123874664\n",
      "Epoch: 51 itrerations: 4800 Training Loss: 4.32309961319\n",
      "Epoch: 51 itrerations: 4900 Training Loss: 4.32002353668\n",
      "Epoch: 51 itrerations: 5000 Training Loss: 4.29179096222\n",
      "Epoch: 51 itrerations: 5100 Training Loss: 4.27336454391\n",
      "Epoch: 51 itrerations: 5200 Training Loss: 4.26305770874\n",
      "Epoch: 51 itrerations: 5300 Training Loss: 4.25576686859\n",
      "Epoch: 51 itrerations: 5400 Training Loss: 4.27181720734\n",
      "Epoch: 51 itrerations: 5500 Training Loss: 4.2693939209\n",
      "Epoch: 51 itrerations: 5600 Training Loss: 4.2644701004\n",
      "Epoch: 51 itrerations: 5700 Training Loss: 4.25832986832\n",
      "Epoch: 51 itrerations: 5800 Training Loss: 4.27039194107\n",
      "Epoch: 51 itrerations: 5900 Training Loss: 4.2793841362\n",
      "Epoch: 51 itrerations: 6000 Training Loss: 4.27070379257\n",
      "Epoch: 51 itrerations: 6100 Training Loss: 4.26045656204\n",
      "Epoch: 51 itrerations: 6200 Training Loss: 4.27204704285\n",
      "Epoch: 51 itrerations: 6300 Training Loss: 4.26662111282\n",
      "Epoch: 51 itrerations: 6400 Training Loss: 4.26919746399\n",
      "Epoch: 51 itrerations: 6500 Training Loss: 4.27030706406\n",
      "Epoch: 51 itrerations: 6600 Training Loss: 4.26004219055\n",
      "Epoch: 51 itrerations: 6700 Training Loss: 4.26968812943\n",
      "Epoch: 51 itrerations: 6800 Training Loss: 4.27156543732\n",
      "Epoch: 51 itrerations: 6900 Training Loss: 4.29689979553\n",
      "Epoch: 51 itrerations: 7000 Training Loss: 4.31873273849\n",
      "Epoch: 51 itrerations: 7100 Training Loss: 4.33732175827\n",
      "Epoch: 51 itrerations: 7200 Training Loss: 4.32439088821\n",
      "Epoch: 51 itrerations: 7300 Training Loss: 4.32201766968\n",
      "Epoch: 51 itrerations: 7400 Training Loss: 4.30949687958\n",
      "Epoch: 51 itrerations: 7500 Training Loss: 4.31383371353\n",
      "Epoch: 51 itrerations: 7600 Training Loss: 4.30454683304\n",
      "Epoch: 51 itrerations: 7700 Training Loss: 4.29491949081\n",
      "Epoch: 51 itrerations: 7800 Training Loss: 4.30457544327\n",
      "Epoch: 51 itrerations: 7900 Training Loss: 4.31461620331\n",
      "Epoch: 51 itrerations: 8000 Training Loss: 4.30387592316\n",
      "Epoch: 51 itrerations: 8100 Training Loss: 4.29599952698\n",
      "Epoch: 51 itrerations: 8200 Training Loss: 4.30805778503\n",
      "Epoch: 51 itrerations: 8300 Training Loss: 4.30911016464\n",
      "Epoch: 51 itrerations: 8400 Training Loss: 4.31026887894\n",
      "Epoch: 51 itrerations: 8500 Training Loss: 4.3063750267\n",
      "Epoch: 51 itrerations: 8600 Training Loss: 4.30989408493\n",
      "Epoch: 51 itrerations: 8700 Training Loss: 4.3095407486\n",
      "Epoch: 51 itrerations: 8800 Training Loss: 4.31593465805\n",
      "Epoch: 51 itrerations: 8900 Training Loss: 4.30409669876\n",
      "Epoch: 51 itrerations: 9000 Training Loss: 4.31243228912\n",
      "Epoch: 51 itrerations: 9100 Training Loss: 4.31397771835\n",
      "Epoch: 51 itrerations: 9200 Training Loss: 4.31976699829\n",
      "Epoch: 51 itrerations: 9300 Training Loss: 4.31101322174\n",
      "Epoch: 51 itrerations: 9400 Training Loss: 4.30012083054\n",
      "Epoch: 51 itrerations: 9500 Training Loss: 4.29759407043\n",
      "Epoch: 51 itrerations: 9600 Training Loss: 4.3137550354\n",
      "Epoch: 51 itrerations: 9700 Training Loss: 4.31485700607\n",
      "Epoch: 51 itrerations: 9800 Training Loss: 4.31765651703\n",
      "Epoch: 51 itrerations: 9900 Training Loss: 4.31755924225\n",
      "Epoch: 51 itrerations: 10000 Training Loss: 4.32897043228\n",
      "Epoch: 51 itrerations: 10100 Training Loss: 4.33392333984\n",
      "Epoch: 51 itrerations: 10200 Training Loss: 4.33401870728\n",
      "Epoch: 51 itrerations: 10300 Training Loss: 4.33672952652\n",
      "Epoch: 51 itrerations: 10400 Training Loss: 4.33680438995\n",
      "Epoch: 51 itrerations: 10500 Training Loss: 4.33215236664\n",
      "Epoch: 51 itrerations: 10600 Training Loss: 4.33785581589\n",
      "Epoch: 51 itrerations: 10700 Training Loss: 4.33966016769\n",
      "Epoch: 52 itrerations: 0 Training Loss: 5.17950820923\n",
      "Epoch: 52 itrerations: 100 Training Loss: 3.93562507629\n",
      "Epoch: 52 itrerations: 200 Training Loss: 3.69190192223\n",
      "Epoch: 52 itrerations: 300 Training Loss: 3.83117556572\n",
      "Epoch: 52 itrerations: 400 Training Loss: 4.07054710388\n",
      "Epoch: 52 itrerations: 500 Training Loss: 3.77667713165\n",
      "Epoch: 52 itrerations: 600 Training Loss: 3.97973680496\n",
      "Epoch: 52 itrerations: 700 Training Loss: 4.04825973511\n",
      "Epoch: 52 itrerations: 800 Training Loss: 4.20720386505\n",
      "Epoch: 52 itrerations: 900 Training Loss: 4.24738454819\n",
      "Epoch: 52 itrerations: 1000 Training Loss: 4.22910785675\n",
      "Epoch: 52 itrerations: 1100 Training Loss: 4.1876745224\n",
      "Epoch: 52 itrerations: 1200 Training Loss: 4.31021118164\n",
      "Epoch: 52 itrerations: 1300 Training Loss: 4.32033205032\n",
      "Epoch: 52 itrerations: 1400 Training Loss: 4.40342950821\n",
      "Epoch: 52 itrerations: 1500 Training Loss: 4.39042139053\n",
      "Epoch: 52 itrerations: 1600 Training Loss: 4.36256837845\n",
      "Epoch: 52 itrerations: 1700 Training Loss: 4.37848043442\n",
      "Epoch: 52 itrerations: 1800 Training Loss: 4.38926029205\n",
      "Epoch: 52 itrerations: 1900 Training Loss: 4.40349054337\n",
      "Epoch: 52 itrerations: 2000 Training Loss: 4.43019294739\n",
      "Epoch: 52 itrerations: 2100 Training Loss: 4.39404296875\n",
      "Epoch: 52 itrerations: 2200 Training Loss: 4.35840320587\n",
      "Epoch: 52 itrerations: 2300 Training Loss: 4.37464857101\n",
      "Epoch: 52 itrerations: 2400 Training Loss: 4.32075166702\n",
      "Epoch: 52 itrerations: 2500 Training Loss: 4.27004671097\n",
      "Epoch: 52 itrerations: 2600 Training Loss: 4.2673034668\n",
      "Epoch: 52 itrerations: 2700 Training Loss: 4.30424499512\n",
      "Epoch: 52 itrerations: 2800 Training Loss: 4.26239109039\n",
      "Epoch: 52 itrerations: 2900 Training Loss: 4.2219171524\n",
      "Epoch: 52 itrerations: 3000 Training Loss: 4.28243923187\n",
      "Epoch: 52 itrerations: 3100 Training Loss: 4.26613664627\n",
      "Epoch: 52 itrerations: 3200 Training Loss: 4.28589868546\n",
      "Epoch: 52 itrerations: 3300 Training Loss: 4.26779127121\n",
      "Epoch: 52 itrerations: 3400 Training Loss: 4.27755403519\n",
      "Epoch: 52 itrerations: 3500 Training Loss: 4.30558633804\n",
      "Epoch: 52 itrerations: 3600 Training Loss: 4.31616306305\n",
      "Epoch: 52 itrerations: 3700 Training Loss: 4.30390739441\n",
      "Epoch: 52 itrerations: 3800 Training Loss: 4.32529973984\n",
      "Epoch: 52 itrerations: 3900 Training Loss: 4.31499195099\n",
      "Epoch: 52 itrerations: 4000 Training Loss: 4.30366802216\n",
      "Epoch: 52 itrerations: 4100 Training Loss: 4.29300546646\n",
      "Epoch: 52 itrerations: 4200 Training Loss: 4.28693675995\n",
      "Epoch: 52 itrerations: 4300 Training Loss: 4.27389621735\n",
      "Epoch: 52 itrerations: 4400 Training Loss: 4.27545404434\n",
      "Epoch: 52 itrerations: 4500 Training Loss: 4.25908708572\n",
      "Epoch: 52 itrerations: 4600 Training Loss: 4.25330066681\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "nb_epoch = 3000\n",
    "\n",
    "#model = create_model()\n",
    "# model.load_weights('5_EXPERIMNTS_FOR_PAPER/exp8_FUSION/HouseDetection_densenet_3fusion_208_Epochs.hdf5')\n",
    "lines = [line.rstrip('\\n') for line in open(root_pth +'sh_training_jitter_10755_housecounting_data_tagged.txt')]\n",
    "\n",
    "indices = np.arange(len(lines))\n",
    "train_indices = indices[:]    #split training\n",
    "print(train_indices.shape)\n",
    "iterations_t = np.int(len(train_indices)/batch_size)\n",
    "t_loss_ep = []\n",
    "\n",
    "for epoch in range(nb_epoch):\n",
    "    ## training\n",
    "    t_loss = []\n",
    "    start = time.time()\n",
    "    for i in range(iterations_t):\n",
    "        \n",
    "        X_train, X_pmap, X_prod, y_train = generator(train_indices, batch_size, i, lines, image_size=336)  \n",
    "        history = model.fit([X_train, X_pmap, X_prod], y_train, verbose=0)\n",
    "        t_loss.append(history.history['loss'])\n",
    "        if(i % 100 == 0):\n",
    "            it_loss = np.mean(np.array(t_loss, dtype=np.float32))\n",
    "            print(\"Epoch: {} itrerations: {} Training Loss: {}\".format(epoch,i, it_loss) )\n",
    "    stop = time.time()\n",
    "    duration = stop - start\n",
    "    t_loss_ep.append(np.mean(np.array(t_loss, dtype=np.float32)))\n",
    "#     print(\"################################MAIN LOSS#############################\")\n",
    "#     print(\"Epoch: {}  Training Loss: {}  Time: {} sec\".format(epoch+209, t_loss_ep[epoch], duration) )\n",
    "    \n",
    "            \n",
    "    model_name = model_weight_path  + '/weights_' + str(epoch) + \"_Epochs.hdf5\"\n",
    "    model.save_weights(model_name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531\n",
      "[0, 0]\n",
      "[5, array([[4.196158]], dtype=float32)]\n",
      "[1, array([[0.24875116]], dtype=float32)]\n",
      "[40, array([[40.92314]], dtype=float32)]\n",
      "[30, array([[33.747246]], dtype=float32)]\n",
      "[4, array([[7.877047]], dtype=float32)]\n",
      "[0, array([[0.22600794]], dtype=float32)]\n",
      "[0, array([[0.18292284]], dtype=float32)]\n",
      "[47, array([[36.541603]], dtype=float32)]\n",
      "[16, array([[17.61943]], dtype=float32)]\n",
      "[15, array([[14.413607]], dtype=float32)]\n",
      "[12, array([[10.24546]], dtype=float32)]\n",
      "[1, array([[1.08038]], dtype=float32)]\n",
      "[0, array([[0.44815826]], dtype=float32)]\n",
      "[15, array([[10.805791]], dtype=float32)]\n",
      "[22, array([[27.896841]], dtype=float32)]\n",
      "[36, array([[34.75606]], dtype=float32)]\n",
      "[1, array([[5.4089055]], dtype=float32)]\n",
      "[0, array([[0.9697712]], dtype=float32)]\n",
      "[0, array([[0.3055945]], dtype=float32)]\n",
      "[15, array([[12.452367]], dtype=float32)]\n",
      "[0, array([[0.16928798]], dtype=float32)]\n",
      "[0, array([[0.20835727]], dtype=float32)]\n",
      "[0, 0]\n",
      "[0, array([[0.19815922]], dtype=float32)]\n",
      "[49, array([[36.859303]], dtype=float32)]\n",
      "[3, array([[3.7058759]], dtype=float32)]\n",
      "[3, array([[1.1366606]], dtype=float32)]\n",
      "[2, array([[1.1633272]], dtype=float32)]\n",
      "[17, array([[9.549474]], dtype=float32)]\n",
      "[33, array([[24.466877]], dtype=float32)]\n",
      "[0, array([[0.10057849]], dtype=float32)]\n",
      "[0, array([[0.9605498]], dtype=float32)]\n",
      "[98, array([[70.039986]], dtype=float32)]\n",
      "[12, array([[12.342797]], dtype=float32)]\n",
      "[63, array([[51.38354]], dtype=float32)]\n",
      "[2, array([[3.3675122]], dtype=float32)]\n",
      "[4, array([[1.3929391]], dtype=float32)]\n",
      "[22, array([[21.029665]], dtype=float32)]\n",
      "[48, array([[38.57648]], dtype=float32)]\n",
      "[1, array([[0.93594486]], dtype=float32)]\n",
      "[26, array([[25.20719]], dtype=float32)]\n",
      "[5, array([[3.1509943]], dtype=float32)]\n",
      "[2, array([[10.004643]], dtype=float32)]\n",
      "[0, array([[0.04306912]], dtype=float32)]\n",
      "[46, array([[30.065166]], dtype=float32)]\n",
      "[2, array([[6.9534774]], dtype=float32)]\n",
      "[0, array([[0.8094251]], dtype=float32)]\n",
      "[65, array([[67.77707]], dtype=float32)]\n",
      "[0, array([[0.16665828]], dtype=float32)]\n",
      "[36, array([[36.880543]], dtype=float32)]\n",
      "[0, 0]\n",
      "[19, array([[25.168768]], dtype=float32)]\n",
      "[3, array([[2.833942]], dtype=float32)]\n",
      "[2, array([[0.536409]], dtype=float32)]\n",
      "[0, array([[0.4407233]], dtype=float32)]\n",
      "[30, array([[20.435589]], dtype=float32)]\n",
      "[0, array([[0.1617282]], dtype=float32)]\n",
      "[2, array([[3.004261]], dtype=float32)]\n",
      "[12, array([[13.8971815]], dtype=float32)]\n",
      "[0, array([[0.08095706]], dtype=float32)]\n",
      "[38, array([[21.741852]], dtype=float32)]\n",
      "[0, array([[0.46636897]], dtype=float32)]\n",
      "[0, array([[0.6146744]], dtype=float32)]\n",
      "[0, array([[0.17702675]], dtype=float32)]\n",
      "[1, array([[1.2693566]], dtype=float32)]\n",
      "[1, array([[0.73616683]], dtype=float32)]\n",
      "[16, array([[13.021784]], dtype=float32)]\n",
      "[2, array([[4.0471377]], dtype=float32)]\n",
      "[32, array([[20.00747]], dtype=float32)]\n",
      "[0, array([[0.33566856]], dtype=float32)]\n",
      "[14, array([[14.184433]], dtype=float32)]\n",
      "[32, array([[28.288698]], dtype=float32)]\n",
      "[25, array([[26.561535]], dtype=float32)]\n",
      "[23, array([[17.040462]], dtype=float32)]\n",
      "[0, array([[0.3389352]], dtype=float32)]\n",
      "[4, array([[4.0291243]], dtype=float32)]\n",
      "[5, array([[2.2452679]], dtype=float32)]\n",
      "[0, array([[0.4558906]], dtype=float32)]\n",
      "[20, array([[17.329992]], dtype=float32)]\n",
      "[32, array([[36.63401]], dtype=float32)]\n",
      "[15, array([[13.465727]], dtype=float32)]\n",
      "[0, array([[0.09247833]], dtype=float32)]\n",
      "[0, array([[0.33954924]], dtype=float32)]\n",
      "[16, array([[16.93207]], dtype=float32)]\n",
      "[48, array([[53.31687]], dtype=float32)]\n",
      "[0, array([[0.5573119]], dtype=float32)]\n",
      "[18, array([[13.827446]], dtype=float32)]\n",
      "[17, array([[16.039665]], dtype=float32)]\n",
      "[16, array([[15.357623]], dtype=float32)]\n",
      "[0, array([[0.21399087]], dtype=float32)]\n",
      "[0, array([[0.10875642]], dtype=float32)]\n",
      "[0, array([[0.45218068]], dtype=float32)]\n",
      "[45, array([[45.6793]], dtype=float32)]\n",
      "[9, array([[6.846995]], dtype=float32)]\n",
      "[0, array([[0.75810087]], dtype=float32)]\n",
      "[0, array([[0.30348384]], dtype=float32)]\n",
      "[36, array([[38.70372]], dtype=float32)]\n",
      "[24, array([[23.87927]], dtype=float32)]\n",
      "[0, array([[0.37581682]], dtype=float32)]\n",
      "[8, array([[10.820574]], dtype=float32)]\n",
      "[0, array([[0.4768051]], dtype=float32)]\n",
      "[17, array([[23.362373]], dtype=float32)]\n",
      "[0, array([[0.22946286]], dtype=float32)]\n",
      "[17, array([[16.22459]], dtype=float32)]\n",
      "[16, array([[11.622366]], dtype=float32)]\n",
      "[19, array([[27.286901]], dtype=float32)]\n",
      "[22, array([[17.490648]], dtype=float32)]\n",
      "[0, array([[0.41320783]], dtype=float32)]\n",
      "[33, array([[29.52988]], dtype=float32)]\n",
      "[37, array([[29.31457]], dtype=float32)]\n",
      "[20, array([[16.117928]], dtype=float32)]\n",
      "[5, array([[7.6065784]], dtype=float32)]\n",
      "[26, array([[26.06998]], dtype=float32)]\n",
      "[19, array([[22.766022]], dtype=float32)]\n",
      "[0, array([[0.23486638]], dtype=float32)]\n",
      "[25, array([[23.330816]], dtype=float32)]\n",
      "[9, array([[15.07261]], dtype=float32)]\n",
      "[1, array([[1.3892128]], dtype=float32)]\n",
      "[46, array([[46.590836]], dtype=float32)]\n",
      "[0, array([[0.4094112]], dtype=float32)]\n",
      "[25, array([[36.771336]], dtype=float32)]\n",
      "[32, array([[27.762646]], dtype=float32)]\n",
      "[36, array([[29.09372]], dtype=float32)]\n",
      "[0, array([[0.30798072]], dtype=float32)]\n",
      "[7, array([[5.840628]], dtype=float32)]\n",
      "[34, array([[32.40682]], dtype=float32)]\n",
      "[13, array([[13.886264]], dtype=float32)]\n",
      "[5, array([[9.129881]], dtype=float32)]\n",
      "[0, array([[0.64042354]], dtype=float32)]\n",
      "[7, array([[4.743204]], dtype=float32)]\n",
      "[36, array([[17.630436]], dtype=float32)]\n",
      "[0, array([[0.20445907]], dtype=float32)]\n",
      "[11, array([[18.805498]], dtype=float32)]\n",
      "[16, array([[19.757181]], dtype=float32)]\n",
      "[24, array([[37.503242]], dtype=float32)]\n",
      "[0, array([[0.35721314]], dtype=float32)]\n",
      "[12, array([[12.158401]], dtype=float32)]\n",
      "[18, array([[16.108065]], dtype=float32)]\n",
      "[25, array([[27.889431]], dtype=float32)]\n",
      "[15, array([[9.6563425]], dtype=float32)]\n",
      "[0, array([[0.51719487]], dtype=float32)]\n",
      "[0, 0]\n",
      "[0, array([[1.9539826]], dtype=float32)]\n",
      "[0, array([[0.27068675]], dtype=float32)]\n",
      "[27, array([[29.467756]], dtype=float32)]\n",
      "[0, array([[1.4782892]], dtype=float32)]\n",
      "[37, array([[42.10362]], dtype=float32)]\n",
      "[21, array([[28.740906]], dtype=float32)]\n",
      "[30, array([[29.007448]], dtype=float32)]\n",
      "[8, array([[5.971176]], dtype=float32)]\n",
      "[0, array([[0.4166149]], dtype=float32)]\n",
      "[2, array([[2.5956895]], dtype=float32)]\n",
      "[0, array([[0.13257676]], dtype=float32)]\n",
      "[16, array([[12.655903]], dtype=float32)]\n",
      "[22, array([[34.12711]], dtype=float32)]\n",
      "[49, array([[33.9782]], dtype=float32)]\n",
      "[33, array([[31.907675]], dtype=float32)]\n",
      "[73, array([[60.276104]], dtype=float32)]\n",
      "[13, array([[9.800635]], dtype=float32)]\n",
      "[40, array([[22.752413]], dtype=float32)]\n",
      "[8, array([[8.855643]], dtype=float32)]\n",
      "[9, array([[3.7839956]], dtype=float32)]\n",
      "[4, array([[9.703316]], dtype=float32)]\n",
      "[6, array([[5.415196]], dtype=float32)]\n",
      "[0, array([[0.7066986]], dtype=float32)]\n",
      "[7, array([[7.083982]], dtype=float32)]\n",
      "[8, array([[10.220792]], dtype=float32)]\n",
      "[22, array([[19.634703]], dtype=float32)]\n",
      "[16, array([[19.903145]], dtype=float32)]\n",
      "[23, array([[25.169355]], dtype=float32)]\n",
      "[12, array([[23.923908]], dtype=float32)]\n",
      "[30, array([[35.24426]], dtype=float32)]\n",
      "[0, array([[0.4177481]], dtype=float32)]\n",
      "[0, array([[0.5828185]], dtype=float32)]\n",
      "[25, array([[17.852606]], dtype=float32)]\n",
      "[6, array([[12.752783]], dtype=float32)]\n",
      "[0, array([[0.4113993]], dtype=float32)]\n",
      "[68, array([[33.69995]], dtype=float32)]\n",
      "[0, array([[0.2364726]], dtype=float32)]\n",
      "[29, array([[28.660429]], dtype=float32)]\n",
      "[9, array([[7.0588946]], dtype=float32)]\n",
      "[17, array([[12.478024]], dtype=float32)]\n",
      "[23, array([[21.219206]], dtype=float32)]\n",
      "[12, array([[11.569014]], dtype=float32)]\n",
      "[0, array([[0.13899243]], dtype=float32)]\n",
      "[32, array([[22.95079]], dtype=float32)]\n",
      "[0, 0]\n",
      "[2, array([[6.0062094]], dtype=float32)]\n",
      "[13, array([[3.0252724]], dtype=float32)]\n",
      "[18, array([[24.06284]], dtype=float32)]\n",
      "[2, array([[3.8603258]], dtype=float32)]\n",
      "[35, array([[22.311922]], dtype=float32)]\n",
      "[14, array([[13.018929]], dtype=float32)]\n",
      "[0, array([[0.14124131]], dtype=float32)]\n",
      "[8, array([[11.078424]], dtype=float32)]\n",
      "[25, array([[29.293386]], dtype=float32)]\n",
      "[38, array([[12.389376]], dtype=float32)]\n",
      "[0, 0]\n",
      "[15, array([[23.293818]], dtype=float32)]\n",
      "[30, array([[24.84968]], dtype=float32)]\n",
      "[0, array([[0.3613357]], dtype=float32)]\n",
      "[12, array([[13.016014]], dtype=float32)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, array([[0.185565]], dtype=float32)]\n",
      "[24, array([[25.50395]], dtype=float32)]\n",
      "[9, array([[9.305759]], dtype=float32)]\n",
      "[31, array([[31.455713]], dtype=float32)]\n",
      "[21, array([[23.389883]], dtype=float32)]\n",
      "[22, array([[19.179007]], dtype=float32)]\n",
      "[0, array([[0.38897687]], dtype=float32)]\n",
      "[0, array([[0.5891069]], dtype=float32)]\n",
      "[26, array([[20.337294]], dtype=float32)]\n",
      "[44, array([[45.36332]], dtype=float32)]\n",
      "[7, array([[8.320624]], dtype=float32)]\n",
      "[11, array([[7.1933765]], dtype=float32)]\n",
      "[31, array([[24.627613]], dtype=float32)]\n",
      "[6, array([[7.2368193]], dtype=float32)]\n",
      "[33, array([[32.44287]], dtype=float32)]\n",
      "[9, array([[11.084945]], dtype=float32)]\n",
      "[30, array([[28.87394]], dtype=float32)]\n",
      "[4, array([[8.205449]], dtype=float32)]\n",
      "[17, array([[18.195955]], dtype=float32)]\n",
      "[0, array([[0.9292843]], dtype=float32)]\n",
      "[20, array([[18.069508]], dtype=float32)]\n",
      "[0, array([[0.10639477]], dtype=float32)]\n",
      "[0, array([[0.37319398]], dtype=float32)]\n",
      "[60, array([[34.8203]], dtype=float32)]\n",
      "[0, array([[0.35050452]], dtype=float32)]\n",
      "[1, array([[1.7539414]], dtype=float32)]\n",
      "[0, array([[0.919262]], dtype=float32)]\n",
      "[0, array([[0.5984279]], dtype=float32)]\n",
      "[54, array([[23.28297]], dtype=float32)]\n",
      "[16, array([[13.593225]], dtype=float32)]\n",
      "[21, array([[21.931217]], dtype=float32)]\n",
      "[15, array([[18.36926]], dtype=float32)]\n",
      "[12, array([[13.298389]], dtype=float32)]\n",
      "[34, array([[18.994223]], dtype=float32)]\n",
      "[7, array([[3.7242007]], dtype=float32)]\n",
      "[35, array([[42.88299]], dtype=float32)]\n",
      "[45, array([[31.819283]], dtype=float32)]\n",
      "[35, array([[64.05272]], dtype=float32)]\n",
      "[88, array([[62.49738]], dtype=float32)]\n",
      "[10, array([[18.254612]], dtype=float32)]\n",
      "[1, array([[1.1815522]], dtype=float32)]\n",
      "[11, array([[13.811138]], dtype=float32)]\n",
      "[77, array([[62.48806]], dtype=float32)]\n",
      "[0, array([[0.27220607]], dtype=float32)]\n",
      "[0, array([[0.58756]], dtype=float32)]\n",
      "[44, array([[39.20762]], dtype=float32)]\n",
      "[0, array([[0.10580182]], dtype=float32)]\n",
      "[33, array([[26.597002]], dtype=float32)]\n",
      "[11, array([[10.584573]], dtype=float32)]\n",
      "[11, array([[14.974798]], dtype=float32)]\n",
      "[14, array([[12.712541]], dtype=float32)]\n",
      "[0, array([[0.40682995]], dtype=float32)]\n",
      "[0, array([[0.47429675]], dtype=float32)]\n",
      "[4, array([[1.5909913]], dtype=float32)]\n",
      "[9, array([[6.0443344]], dtype=float32)]\n",
      "[6, array([[5.8440146]], dtype=float32)]\n",
      "[14, array([[14.079505]], dtype=float32)]\n",
      "[41, array([[37.578938]], dtype=float32)]\n",
      "[0, array([[0.38667065]], dtype=float32)]\n",
      "[34, array([[40.25153]], dtype=float32)]\n",
      "[56, array([[66.165924]], dtype=float32)]\n",
      "[0, array([[0.5138659]], dtype=float32)]\n",
      "[15, array([[11.686246]], dtype=float32)]\n",
      "[38, array([[36.450176]], dtype=float32)]\n",
      "[12, array([[12.477212]], dtype=float32)]\n",
      "[13, array([[19.368979]], dtype=float32)]\n",
      "[5, array([[4.8920007]], dtype=float32)]\n",
      "[71, array([[72.857475]], dtype=float32)]\n",
      "[0, array([[0.3876338]], dtype=float32)]\n",
      "[0, 0]\n",
      "[35, array([[32.109135]], dtype=float32)]\n",
      "[32, array([[28.225615]], dtype=float32)]\n",
      "[7, array([[7.810919]], dtype=float32)]\n",
      "[21, array([[12.249351]], dtype=float32)]\n",
      "[9, array([[11.743167]], dtype=float32)]\n",
      "[22, array([[33.487843]], dtype=float32)]\n",
      "[25, array([[21.341696]], dtype=float32)]\n",
      "[37, array([[30.908676]], dtype=float32)]\n",
      "[22, array([[19.948061]], dtype=float32)]\n",
      "[16, array([[22.353876]], dtype=float32)]\n",
      "[2, array([[2.4745302]], dtype=float32)]\n",
      "[1, array([[0.23414248]], dtype=float32)]\n",
      "[28, array([[27.354904]], dtype=float32)]\n",
      "[38, array([[30.762903]], dtype=float32)]\n",
      "[0, array([[0.25866854]], dtype=float32)]\n",
      "[3, array([[2.0336921]], dtype=float32)]\n",
      "[17, array([[25.385738]], dtype=float32)]\n",
      "[32, array([[39.59845]], dtype=float32)]\n",
      "[9, array([[8.732422]], dtype=float32)]\n",
      "[11, array([[12.849119]], dtype=float32)]\n",
      "[0, array([[0.4510489]], dtype=float32)]\n",
      "[67, array([[57.79537]], dtype=float32)]\n",
      "[0, array([[0.7200581]], dtype=float32)]\n",
      "[39, array([[35.214977]], dtype=float32)]\n",
      "[9, array([[4.779325]], dtype=float32)]\n",
      "[9, array([[6.737843]], dtype=float32)]\n",
      "[40, array([[41.980816]], dtype=float32)]\n",
      "[4, array([[7.966104]], dtype=float32)]\n",
      "[20, array([[18.883488]], dtype=float32)]\n",
      "[56, array([[61.126366]], dtype=float32)]\n",
      "[0, array([[0.38731688]], dtype=float32)]\n",
      "[16, array([[30.977446]], dtype=float32)]\n",
      "[0, array([[0.37952924]], dtype=float32)]\n",
      "[13, array([[15.709787]], dtype=float32)]\n",
      "[13, array([[19.1804]], dtype=float32)]\n",
      "[0, array([[0.4784637]], dtype=float32)]\n",
      "[0, array([[0.01893449]], dtype=float32)]\n",
      "[3, array([[0.5974911]], dtype=float32)]\n",
      "[40, array([[34.86255]], dtype=float32)]\n",
      "[12, array([[9.368838]], dtype=float32)]\n",
      "[7, array([[5.3774548]], dtype=float32)]\n",
      "[32, array([[20.15859]], dtype=float32)]\n",
      "[19, array([[16.223703]], dtype=float32)]\n",
      "[0, array([[0.7988888]], dtype=float32)]\n",
      "[39, array([[33.363087]], dtype=float32)]\n",
      "[47, array([[38.103672]], dtype=float32)]\n",
      "[11, array([[12.6607685]], dtype=float32)]\n",
      "[29, array([[21.457102]], dtype=float32)]\n",
      "[22, array([[25.316132]], dtype=float32)]\n",
      "[9, array([[13.03587]], dtype=float32)]\n",
      "[40, array([[28.265926]], dtype=float32)]\n",
      "[0, array([[0.01299858]], dtype=float32)]\n",
      "[10, array([[14.153535]], dtype=float32)]\n",
      "[42, array([[33.22818]], dtype=float32)]\n",
      "[72, array([[64.636086]], dtype=float32)]\n",
      "[0, 0]\n",
      "[20, array([[15.212157]], dtype=float32)]\n",
      "[6, array([[2.9897308]], dtype=float32)]\n",
      "[26, array([[22.299948]], dtype=float32)]\n",
      "[0, array([[0.547016]], dtype=float32)]\n",
      "[29, array([[41.5893]], dtype=float32)]\n",
      "[32, array([[23.64112]], dtype=float32)]\n",
      "[0, 0]\n",
      "[48, array([[68.968605]], dtype=float32)]\n",
      "[28, array([[19.301235]], dtype=float32)]\n",
      "[24, array([[28.327827]], dtype=float32)]\n",
      "[51, array([[42.04429]], dtype=float32)]\n",
      "[43, array([[44.63973]], dtype=float32)]\n",
      "[25, array([[21.195953]], dtype=float32)]\n",
      "[21, array([[19.575953]], dtype=float32)]\n",
      "[0, array([[0.3523323]], dtype=float32)]\n",
      "[35, array([[41.706177]], dtype=float32)]\n",
      "[11, array([[19.626276]], dtype=float32)]\n",
      "[24, array([[31.086582]], dtype=float32)]\n",
      "[50, array([[34.45104]], dtype=float32)]\n",
      "[7, array([[4.874164]], dtype=float32)]\n",
      "[4, array([[8.913351]], dtype=float32)]\n",
      "[9, array([[5.550712]], dtype=float32)]\n",
      "[7, array([[10.1813345]], dtype=float32)]\n",
      "[0, array([[0.57740825]], dtype=float32)]\n",
      "[13, array([[14.368034]], dtype=float32)]\n",
      "[14, array([[17.558243]], dtype=float32)]\n",
      "[5, array([[8.89801]], dtype=float32)]\n",
      "[37, array([[27.60038]], dtype=float32)]\n",
      "[0, array([[0.3357545]], dtype=float32)]\n",
      "[24, array([[30.374334]], dtype=float32)]\n",
      "[11, array([[16.694294]], dtype=float32)]\n",
      "[3, array([[2.3303232]], dtype=float32)]\n",
      "[0, array([[0.42605233]], dtype=float32)]\n",
      "[33, array([[20.890244]], dtype=float32)]\n",
      "[4, array([[2.610496]], dtype=float32)]\n",
      "[41, array([[34.241543]], dtype=float32)]\n",
      "[32, array([[35.218426]], dtype=float32)]\n",
      "[4, array([[2.8430662]], dtype=float32)]\n",
      "[50, array([[43.975403]], dtype=float32)]\n",
      "[0, array([[0.21017402]], dtype=float32)]\n",
      "[35, array([[30.216269]], dtype=float32)]\n",
      "[0, array([[0.3480395]], dtype=float32)]\n",
      "[6, array([[10.034239]], dtype=float32)]\n",
      "[0, array([[0.45532966]], dtype=float32)]\n",
      "[36, array([[33.442432]], dtype=float32)]\n",
      "[34, array([[32.99227]], dtype=float32)]\n",
      "[0, array([[0.6278447]], dtype=float32)]\n",
      "[3, array([[0.44572395]], dtype=float32)]\n",
      "[15, array([[15.422797]], dtype=float32)]\n",
      "[22, array([[25.11293]], dtype=float32)]\n",
      "[26, array([[19.369432]], dtype=float32)]\n",
      "[37, array([[38.048306]], dtype=float32)]\n",
      "[15, array([[10.585892]], dtype=float32)]\n",
      "[24, array([[19.004786]], dtype=float32)]\n",
      "[58, array([[38.456787]], dtype=float32)]\n",
      "[25, array([[18.956034]], dtype=float32)]\n",
      "[0, array([[0.0386728]], dtype=float32)]\n",
      "[0, array([[0.32193154]], dtype=float32)]\n",
      "[0, array([[0.6757286]], dtype=float32)]\n",
      "[7, array([[5.128601]], dtype=float32)]\n",
      "[11, array([[15.237507]], dtype=float32)]\n",
      "[55, array([[48.013348]], dtype=float32)]\n",
      "[31, array([[40.374466]], dtype=float32)]\n",
      "[38, array([[46.480324]], dtype=float32)]\n",
      "[31, array([[23.509405]], dtype=float32)]\n",
      "[12, array([[12.1678705]], dtype=float32)]\n",
      "[27, array([[17.895298]], dtype=float32)]\n",
      "[17, array([[16.935999]], dtype=float32)]\n",
      "[18, array([[9.96857]], dtype=float32)]\n",
      "[82, array([[64.403435]], dtype=float32)]\n",
      "[0, array([[0.16134328]], dtype=float32)]\n",
      "[0, array([[0.16977197]], dtype=float32)]\n",
      "[22, array([[24.865612]], dtype=float32)]\n",
      "[23, array([[15.325581]], dtype=float32)]\n",
      "[38, array([[37.50018]], dtype=float32)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70, array([[72.24823]], dtype=float32)]\n",
      "[14, array([[13.961662]], dtype=float32)]\n",
      "[0, array([[0.54059976]], dtype=float32)]\n",
      "[16, array([[20.421652]], dtype=float32)]\n",
      "[0, array([[0.37325704]], dtype=float32)]\n",
      "[16, array([[9.085379]], dtype=float32)]\n",
      "[0, array([[0.39915645]], dtype=float32)]\n",
      "[0, array([[0.05248296]], dtype=float32)]\n",
      "[21, array([[18.738104]], dtype=float32)]\n",
      "[32, array([[34.220856]], dtype=float32)]\n",
      "[5, array([[2.9084468]], dtype=float32)]\n",
      "[7, array([[6.279581]], dtype=float32)]\n",
      "[0, array([[0.32987416]], dtype=float32)]\n",
      "[0, array([[1.3330953]], dtype=float32)]\n",
      "[0, array([[0.06690645]], dtype=float32)]\n",
      "[12, array([[13.01031]], dtype=float32)]\n",
      "[0, 0]\n",
      "[16, array([[13.269768]], dtype=float32)]\n",
      "[31, array([[26.292076]], dtype=float32)]\n",
      "[10, array([[6.495773]], dtype=float32)]\n",
      "[0, array([[0.40487373]], dtype=float32)]\n",
      "[0, array([[0.13923007]], dtype=float32)]\n",
      "[0, array([[0.7688144]], dtype=float32)]\n",
      "[1, array([[2.3446767]], dtype=float32)]\n",
      "[0, array([[0.07812965]], dtype=float32)]\n",
      "[30, array([[22.816772]], dtype=float32)]\n",
      "[60, array([[102.88914]], dtype=float32)]\n",
      "[0, array([[0.53357244]], dtype=float32)]\n",
      "[0, array([[0.07223475]], dtype=float32)]\n",
      "[21, array([[27.74406]], dtype=float32)]\n",
      "[0, array([[0.8726475]], dtype=float32)]\n",
      "[41, array([[24.917118]], dtype=float32)]\n",
      "[10, array([[27.495775]], dtype=float32)]\n",
      "[17, array([[18.810713]], dtype=float32)]\n",
      "[31, array([[31.056463]], dtype=float32)]\n",
      "[15, array([[20.05709]], dtype=float32)]\n",
      "[27, array([[30.078993]], dtype=float32)]\n",
      "[0, array([[0.7033468]], dtype=float32)]\n",
      "[14, array([[25.353226]], dtype=float32)]\n",
      "[2, array([[2.4229388]], dtype=float32)]\n",
      "[0, array([[0.66147166]], dtype=float32)]\n",
      "[36, array([[36.29886]], dtype=float32)]\n",
      "[17, array([[14.132234]], dtype=float32)]\n",
      "[21, array([[18.823164]], dtype=float32)]\n",
      "[27, array([[22.19492]], dtype=float32)]\n",
      "[3, array([[3.6030025]], dtype=float32)]\n",
      "[29, array([[29.74838]], dtype=float32)]\n",
      "[15, array([[9.3436775]], dtype=float32)]\n",
      "[56, array([[58.84369]], dtype=float32)]\n",
      "[20, array([[19.734089]], dtype=float32)]\n",
      "[31, array([[37.45504]], dtype=float32)]\n",
      "[7, array([[2.0781927]], dtype=float32)]\n",
      "[29, array([[16.320747]], dtype=float32)]\n",
      "[17, array([[10.749384]], dtype=float32)]\n",
      "[8, array([[3.686078]], dtype=float32)]\n",
      "[5, array([[4.38395]], dtype=float32)]\n",
      "[1, array([[0.67012763]], dtype=float32)]\n",
      "[16, array([[30.01961]], dtype=float32)]\n",
      "[10, array([[13.5492525]], dtype=float32)]\n",
      "[0, array([[0.48183674]], dtype=float32)]\n",
      "[29, array([[29.014887]], dtype=float32)]\n",
      "[2, array([[3.3495655]], dtype=float32)]\n",
      "[2, array([[1.9047945]], dtype=float32)]\n",
      "[0, array([[0.42571366]], dtype=float32)]\n",
      "[11, array([[14.942141]], dtype=float32)]\n",
      "[13, array([[13.11908]], dtype=float32)]\n",
      "[5, array([[0.09272778]], dtype=float32)]\n",
      "[38, array([[38.688465]], dtype=float32)]\n",
      "[0, array([[0.13671535]], dtype=float32)]\n",
      "[0, array([[0.38063025]], dtype=float32)]\n",
      "[0, array([[0.09117228]], dtype=float32)]\n",
      "[0, array([[0.09381258]], dtype=float32)]\n",
      "[4, array([[4.5238376]], dtype=float32)]\n",
      "[51, array([[52.769115]], dtype=float32)]\n",
      "[3, array([[2.7082005]], dtype=float32)]\n",
      "[42, array([[40.317223]], dtype=float32)]\n",
      "[0, array([[0.49233955]], dtype=float32)]\n",
      "[10, array([[9.230456]], dtype=float32)]\n",
      "[11, array([[8.880944]], dtype=float32)]\n",
      "[21, array([[24.542046]], dtype=float32)]\n",
      "[5, array([[6.677958]], dtype=float32)]\n",
      "[32, array([[34.429745]], dtype=float32)]\n",
      "[3, array([[6.66412]], dtype=float32)]\n",
      "[7, array([[7.4867134]], dtype=float32)]\n",
      "[0, array([[0.28727168]], dtype=float32)]\n",
      "[30, array([[32.41363]], dtype=float32)]\n",
      "[26, array([[21.822914]], dtype=float32)]\n",
      "[30, array([[27.382198]], dtype=float32)]\n",
      "[50, array([[51.33097]], dtype=float32)]\n",
      "[22, array([[26.532825]], dtype=float32)]\n",
      "[16, array([[24.884693]], dtype=float32)]\n",
      "[3, array([[1.4347208]], dtype=float32)]\n",
      "[32, array([[23.947784]], dtype=float32)]\n",
      "[76, array([[63.097225]], dtype=float32)]\n",
      "[24, array([[12.540378]], dtype=float32)]\n",
      "[33, array([[26.558773]], dtype=float32)]\n",
      "[1, array([[4.422155]], dtype=float32)]\n",
      "[31, array([[28.536295]], dtype=float32)]\n",
      "[0, array([[0.26480722]], dtype=float32)]\n",
      "[21, array([[24.059275]], dtype=float32)]\n",
      "[17, array([[13.2726345]], dtype=float32)]\n",
      "[18, array([[21.733438]], dtype=float32)]\n",
      "[36, array([[26.051476]], dtype=float32)]\n",
      "[15, array([[13.818652]], dtype=float32)]\n",
      "[6, array([[8.401848]], dtype=float32)]\n",
      "[11, array([[20.73472]], dtype=float32)]\n",
      "[0, array([[0.16056544]], dtype=float32)]\n",
      "[0, array([[0.03014612]], dtype=float32)]\n",
      "[51, array([[56.87862]], dtype=float32)]\n",
      "[20, array([[14.963238]], dtype=float32)]\n",
      "[28, array([[23.028648]], dtype=float32)]\n",
      "[93, array([[131.54353]], dtype=float32)]\n",
      "[25, array([[30.988045]], dtype=float32)]\n",
      "[31, array([[37.78501]], dtype=float32)]\n",
      "[34, array([[28.466404]], dtype=float32)]\n",
      "[29, array([[40.26949]], dtype=float32)]\n",
      "[1, array([[1.0492098]], dtype=float32)]\n",
      "[65, array([[51.801643]], dtype=float32)]\n",
      "[3, array([[2.465772]], dtype=float32)]\n",
      "[0, array([[0.02864528]], dtype=float32)]\n",
      "[0, array([[0.3006873]], dtype=float32)]\n",
      "[0, array([[0.07810903]], dtype=float32)]\n",
      "[21, array([[20.872372]], dtype=float32)]\n",
      "[9, array([[12.231759]], dtype=float32)]\n",
      "[14, array([[8.790547]], dtype=float32)]\n",
      "[9, array([[14.277654]], dtype=float32)]\n",
      "[13, array([[5.7741976]], dtype=float32)]\n",
      "##################################### ERROR VALUES ###########################################\n",
      "Mean Squared Error: [[43.221115]] \n",
      "Mean Absolute Error: [[3.8406677]] \n",
      "Number of Instances: 531 \n",
      "##############################################################################################\n",
      "##################################### ERROR VALUES-LOW ###########################################\n",
      "Mean Squared Error: [[14.934297]] \n",
      "Mean Absolute Error: [[2.4776738]] \n",
      "Number of Instances: 416 \n",
      "##############################################################################################\n",
      "##################################### ERROR VALUES-MEDIUM ###########################################\n",
      "Mean Squared Error: [[113.30563]] \n",
      "Mean Absolute Error: [[7.7637377]] \n",
      "Number of Instances: 100 \n",
      "##############################################################################################\n",
      "##################################### ERROR VALUES-HIGH ###########################################\n",
      "Mean Squared Error: [[360.47775]] \n",
      "Mean Absolute Error: [[15.487168]] \n",
      "Number of Instances: 15 \n",
      "##############################################################################################\n"
     ]
    }
   ],
   "source": [
    "lines = [line.rstrip('\\n') for line in open(root_pth +'sh_testing_531_housecounting_data_tagged.txt')]\n",
    "\n",
    "\n",
    "model.load_weights(root_pth+'FusionNet_weights/weights_45_Epochs.hdf5')\n",
    "\n",
    "data = \"\"\n",
    "data_low = \"\"\n",
    "data_low_gt = \"\"\n",
    "data_high = \"\"\n",
    "data_high_gt = \"\"\n",
    "data_medium = \"\"\n",
    "data_medium_gt = \"\"\n",
    "\n",
    "mean_abs_error = 0\n",
    "mean_sqrd_error = 0\n",
    "\n",
    "mean_abs_error_low = 0\n",
    "mean_sqrd_error_low = 0\n",
    "\n",
    "mean_abs_error_high = 0\n",
    "mean_sqrd_error_high = 0\n",
    "\n",
    "mean_abs_error_medium = 0\n",
    "mean_sqrd_error_medium = 0\n",
    "\n",
    "total_data = len(lines)\n",
    "count = 0\n",
    "count_low = 0\n",
    "count_high = 0\n",
    "count_medium = 0\n",
    "fmap_size = 42\n",
    "p_mapsize = 32\n",
    "\n",
    "print(len(lines))\n",
    "for i in range(len(lines)):\n",
    "    line = lines[i].split(' ')\n",
    "    gt_count = np.int(line[1])\n",
    "    #print(lines[i])\n",
    "    \n",
    "#     image_path = os.path.join('/media/itu/CV_Lab2/Dr_waqas', 'DenseNetFeatures_pool5', line[0] + '.mat')\n",
    "    image_path = os.path.join(feature_map_save_path + 'pool5', line[0] + '.mat')\n",
    "    #print(image_path)\n",
    "    fmap1 = sio.loadmat(image_path)\n",
    "    fmap1 = fmap1['fmap']\n",
    "    fmap = fmap1.ravel()\n",
    "    test_image = np.zeros((1,1024))\n",
    "    test_image[0,:] = fmap\n",
    "\n",
    "#     image_path1 = os.path.join('/media/itu/CV_Lab2/Dr_waqas', 'DenseNetFeatures_Pmaps1only_GAP', line[0] + '.mat')\n",
    "    image_path1 = os.path.join(feature_map_save_path + 'gap_maps', line[0] + '.mat')\n",
    "    #print(image_path)\n",
    "    fmap11 = sio.loadmat(image_path1)\n",
    "    fmap11 = fmap11['pgap']\n",
    "    fmap1 = fmap11.ravel()\n",
    "    test_pmap = np.zeros((1,1024))\n",
    "    test_pmap[0, :] = fmap1\n",
    "    \n",
    "#     image_path2 = os.path.join('/media/itu/CV_Lab2/Dr_waqas', 'DenseNetFeatures_product', line[0] + '.mat')\n",
    "    image_path2 = os.path.join(feature_map_save_path + 'pf_maps', line[0] + '.mat')\n",
    "    #print(image_path)\n",
    "    fmap22 = sio.loadmat(image_path2)\n",
    "#     print(fmap22)\n",
    "    fmap22 = fmap22['pfmap']\n",
    "    fmap2 = np.zeros((fmap_size,fmap_size,1024))\n",
    "    fmap2[:,:,:] = fmap22[:,:,:]\n",
    "    test_product = np.zeros((1,42,42,1024))\n",
    "    test_product[0, :, :, :] = fmap2\n",
    "         \n",
    "    \n",
    "    output = model.predict([test_image, test_pmap, test_product])\n",
    "    if output < 0:\n",
    "        output = 0\n",
    "    print([gt_count, output])\n",
    "    \n",
    "    abs_error = np.abs(gt_count - output)\n",
    "    mean_abs_error = mean_abs_error + abs_error\n",
    "    \n",
    "    sqrd_error = (gt_count - output) * (gt_count - output)\n",
    "    mean_sqrd_error = mean_sqrd_error + sqrd_error  \n",
    "    \n",
    "    if gt_count >= 0 and gt_count <= 30:\n",
    "            count_low = count_low + 1\n",
    "            mean_abs_error_low = mean_abs_error_low + abs_error\n",
    "            mean_sqrd_error_low = mean_sqrd_error_low + sqrd_error\n",
    "            data_low = data_low + line[0] + \" \" + str(float(output)) + \"\\n\"\n",
    "            data_low_gt = data_low_gt + line[0] + \" \" + str(float(gt_count)) + \"\\n\"\n",
    "        \n",
    "    if gt_count >= 61 and gt_count <= 100:\n",
    "            count_high = count_high + 1\n",
    "            mean_abs_error_high = mean_abs_error_high + abs_error\n",
    "            mean_sqrd_error_high = mean_sqrd_error_high + sqrd_error\n",
    "            data_high = data_high + line[0] + \" \" + str(float(output)) + \"\\n\"\n",
    "            data_high_gt = data_high_gt + line[0] + \" \" + str(float(gt_count)) + \"\\n\"\n",
    "            \n",
    "    if gt_count >= 31 and gt_count <= 60:\n",
    "            count_medium = count_medium + 1\n",
    "            mean_abs_error_medium = mean_abs_error_medium + abs_error\n",
    "            mean_sqrd_error_medium = mean_sqrd_error_medium + sqrd_error\n",
    "            data_medium = data_medium + line[0] + \" \" + str(float(output)) + \"\\n\"\n",
    "            data_medium_gt = data_medium_gt + line[0] + \" \" + str(float(gt_count)) + \"\\n\"\n",
    "    \n",
    "    \n",
    "    data = data + line[0] + \" \" + str(float(output)) + \"\\n\"\n",
    "f = open(root_pth + 'HouseDetection_densenet_3fusion_total.txt','w')\n",
    "f.write(data)\n",
    "\n",
    "f_mean_abs_error = mean_abs_error / total_data\n",
    "f_mean_sqrd_error = mean_sqrd_error / total_data\n",
    "\n",
    "f_mean_abs_error_low = mean_abs_error_low / count_low\n",
    "f_mean_sqrd_error_low = mean_sqrd_error_low / count_low\n",
    "\n",
    "f_mean_abs_error_high = mean_abs_error_high / count_high\n",
    "f_mean_sqrd_error_high = mean_sqrd_error_high / count_high\n",
    "\n",
    "f_mean_abs_error_medium = mean_abs_error_medium / count_medium\n",
    "f_mean_sqrd_error_medium = mean_sqrd_error_medium / count_medium\n",
    "\n",
    "print('##################################### ERROR VALUES ###########################################')\n",
    "print(\"Mean Squared Error: {} \".format(f_mean_sqrd_error))\n",
    "print(\"Mean Absolute Error: {} \".format(f_mean_abs_error))\n",
    "print(\"Number of Instances: {} \".format(total_data))\n",
    "print('##############################################################################################')\n",
    "\n",
    "print('##################################### ERROR VALUES-LOW ###########################################')\n",
    "print(\"Mean Squared Error: {} \".format(f_mean_sqrd_error_low))\n",
    "print(\"Mean Absolute Error: {} \".format(f_mean_abs_error_low))\n",
    "print(\"Number of Instances: {} \".format(count_low))\n",
    "print('##############################################################################################')\n",
    "\n",
    "print('##################################### ERROR VALUES-MEDIUM ###########################################')\n",
    "print(\"Mean Squared Error: {} \".format(f_mean_sqrd_error_medium))\n",
    "print(\"Mean Absolute Error: {} \".format(f_mean_abs_error_medium))\n",
    "print(\"Number of Instances: {} \".format(count_medium))\n",
    "print('##############################################################################################')\n",
    "\n",
    "print('##################################### ERROR VALUES-HIGH ###########################################')\n",
    "print(\"Mean Squared Error: {} \".format(f_mean_sqrd_error_high))\n",
    "print(\"Mean Absolute Error: {} \".format(f_mean_abs_error_high))\n",
    "print(\"Number of Instances: {} \".format(count_high))\n",
    "print('##############################################################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
